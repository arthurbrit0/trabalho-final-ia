"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"VV62U58C","preprint","2025","Ji, Zimo; Wang, Xunguang; Li, Zongjie; Ma, Pingchuan; Gao, Yudong; Wu, Daoyuan; Yan, Xincheng; Tian, Tian; Wang, Shuai","Taxonomy, Evaluation and Exploitation of IPI-Centric LLM Agent Defense Frameworks","","","","10.48550/arXiv.2511.15203","http://arxiv.org/abs/2511.15203","Large Language Model (LLM)-based agents with function-calling capabilities are increasingly deployed, but remain vulnerable to Indirect Prompt Injection (IPI) attacks that hijack their tool calls. In response, numerous IPI-centric defense frameworks have emerged. However, these defenses are fragmented, lacking a unified taxonomy and comprehensive evaluation. In this Systematization of Knowledge (SoK), we present the first comprehensive analysis of IPI-centric defense frameworks. We introduce a comprehensive taxonomy of these defenses, classifying them along five dimensions. We then thoroughly assess the security and usability of representative defense frameworks. Through analysis of defensive failures in the assessment, we identify six root causes of defense circumvention. Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses. Our paper provides a foundation and critical insights for the future development of more secure and usable IPI-centric agent defense frameworks.","2025-11-19","2026-01-17 20:41:14","2026-01-17 20:41:14","2026-01-17 20:41:14","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2511.15203 [cs]","","/home/arthurbrito/Zotero/storage/RI6TTHPV/Ji et al. - 2025 - Taxonomy, Evaluation and Exploitation of IPI-Centric LLM Agent Defense Frameworks.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2511.15203","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XBM98BU9","preprint","2025","Ikbarieh, Seif; Aryal, Kshitiz; Gupta, Maanak","RAG-targeted Adversarial Attack on LLM-based Threat Detection and Mitigation Framework","","","","10.48550/arXiv.2511.06212","http://arxiv.org/abs/2511.06212","The rapid expansion of the Internet of Things (IoT) is reshaping communication and operational practices across industries, but it also broadens the attack surface and increases susceptibility to security breaches. Artificial Intelligence has become a valuable solution in securing IoT networks, with Large Language Models (LLMs) enabling automated attack behavior analysis and mitigation suggestion in Network Intrusion Detection Systems (NIDS). Despite advancements, the use of LLMs in such systems further expands the attack surface, putting entire networks at risk by introducing vulnerabilities such as prompt injection and data poisoning. In this work, we attack an LLM-based IoT attack analysis and mitigation framework to test its adversarial robustness. We construct an attack description dataset and use it in a targeted data poisoning attack that applies word-level, meaning-preserving perturbations to corrupt the Retrieval-Augmented Generation (RAG) knowledge base of the framework. We then compare pre-attack and post-attack mitigation responses from the target model, ChatGPT-5 Thinking, to measure the impact of the attack on model performance, using an established evaluation rubric designed for human experts and judge LLMs. Our results show that small perturbations degrade LLM performance by weakening the linkage between observed network traffic features and attack behavior, and by reducing the specificity and practicality of recommended mitigations for resource-constrained devices.","2025-11-09","2026-01-17 20:41:14","2026-01-17 20:41:14","2026-01-17 20:41:14","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2511.06212 [cs]","","/home/arthurbrito/Zotero/storage/S8H7HSGF/Ikbarieh et al. - 2025 - RAG-targeted Adversarial Attack on LLM-based Threat Detection and Mitigation Framework.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2511.06212","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SBEAMWXJ","preprint","2025","Jayathilaka, Hasini","Privacy-Preserving Prompt Injection Detection for LLMs Using Federated Learning and Embedding-Based NLP Classification","","","","10.48550/arXiv.2511.12295","http://arxiv.org/abs/2511.12295","Prompt injection attacks are an emerging threat to large language models (LLMs), enabling malicious users to manipulate outputs through carefully designed inputs. Existing detection approaches often require centralizing prompt data, creating significant privacy risks. This paper proposes a privacy-preserving prompt injection detection framework based on federated learning and embedding-based classification. A curated dataset of benign and adversarial prompts was encoded with sentence embedding and used to train both centralized and federated logistic regression models. The federated approach preserved privacy by sharing only model parameters across clients, while achieving detection performance comparable to centralized training. Results demonstrate that effective prompt injection detection is feasible without exposing raw data, making this one of the first explorations of federated security for LLMs. Although the dataset is limited in scale, the findings establish a strong proof-of-concept and highlight new directions for building secure and privacy-aware LLM systems.","2025-11-15","2026-01-17 20:41:14","2026-01-17 20:41:14","2026-01-17 20:41:14","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2511.12295 [cs]","","/home/arthurbrito/Zotero/storage/U6ZQR723/Jayathilaka - 2025 - Privacy-Preserving Prompt Injection Detection for LLMs Using Federated Learning and Embedding-Based.pdf","","","Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2511.12295","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XNDUUP4G","preprint","2025","Peh, Steven","Prompt Fencing: A Cryptographic Approach to Establishing Security Boundaries in Large Language Model Prompts","","","","10.48550/arXiv.2511.19727","http://arxiv.org/abs/2511.19727","Large Language Models (LLMs) remain vulnerable to prompt injection attacks, representing the most significant security threat in production deployments. We present Prompt Fencing, a novel architectural approach that applies cryptographic authentication and data architecture principles to establish explicit security boundaries within LLM prompts. Our approach decorates prompt segments with cryptographically signed metadata including trust ratings and content types, enabling LLMs to distinguish between trusted instructions and untrusted content. While current LLMs lack native fence awareness, we demonstrate that simulated awareness through prompt instructions achieved complete prevention of injection attacks in our experiments, reducing success rates from 86.7% (260/300 successful attacks) to 0% (0/300 successful attacks) across 300 test cases with two leading LLM providers. We implement a proof-of-concept fence generation and verification pipeline with a total overhead of 0.224 seconds (0.130s for fence generation, 0.094s for validation) across 100 samples. Our approach is platform-agnostic and can be incrementally deployed as a security layer above existing LLM infrastructure, with the expectation that future models will be trained with native fence awareness for optimal security.","2025-11-24","2026-01-17 20:41:14","2026-01-17 20:41:14","2026-01-17 20:41:14","","","","","","","Prompt Fencing","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2511.19727 [cs]","Comment: 44 pages, 1 figure","/home/arthurbrito/Zotero/storage/2TPIJZ3K/Peh - 2025 - Prompt Fencing A Cryptographic Approach to Establishing Security Boundaries in Large Language Model.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2511.19727","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EYRNRFKS","preprint","2025","Jamshidi, Saeid; Nafi, Kawser Wazed; Dakhel, Arghavan Moradi; Shahabi, Negar; Khomh, Foutse; Ezzati-Jivan, Naser","Securing the Model Context Protocol: Defending LLMs Against Tool Poisoning and Adversarial Attacks","","","","10.48550/arXiv.2512.06556","http://arxiv.org/abs/2512.06556","The Model Context Protocol (MCP) enables Large Language Models to integrate external tools through structured descriptors, increasing autonomy in decision-making, task execution, and multi-agent workflows. However, this autonomy creates a largely overlooked security gap. Existing defenses focus on prompt-injection attacks and fail to address threats embedded in tool metadata, leaving MCP-based systems exposed to semantic manipulation. This work analyzes three classes of semantic attacks on MCP-integrated systems: (1) Tool Poisoning, where adversarial instructions are hidden in tool descriptors; (2) Shadowing, where trusted tools are indirectly compromised through contaminated shared context; and (3) Rug Pulls, where descriptors are altered after approval to subvert behavior. To counter these threats, we introduce a layered security framework with three components: RSA-based manifest signing to enforce descriptor integrity, LLM-on-LLM semantic vetting to detect suspicious tool definitions, and lightweight heuristic guardrails that block anomalous tool behavior at runtime. Through evaluation of GPT-4, DeepSeek, and Llama-3.5 across eight prompting strategies, we find that security performance varies widely by model architecture and reasoning method. GPT-4 blocks about 71 percent of unsafe tool calls, balancing latency and safety. DeepSeek shows the highest resilience to Shadowing attacks but with greater latency, while Llama-3.5 is fastest but least robust. Our results show that the proposed framework reduces unsafe tool invocation rates without model fine-tuning or internal modification.","2025-12-06","2026-01-17 20:41:14","2026-01-17 20:41:14","2026-01-17 20:41:14","","","","","","","Securing the Model Context Protocol","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2512.06556 [cs]","","/home/arthurbrito/Zotero/storage/NN8K7RI4/Jamshidi et al. - 2025 - Securing the Model Context Protocol Defending LLMs Against Tool Poisoning and Adversarial Attacks.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2512.06556","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZCIYVZJG","preprint","2025","Yadav, Neemesh; Ortu, Francesco; Liu, Jiarui; Yook, Joeun; Schölkopf, Bernhard; Mihalcea, Rada; Cazzaniga, Alberto; Jin, Zhijing","Are LLMs Good Safety Agents or a Propaganda Engine?","","","","10.48550/arXiv.2511.23174","http://arxiv.org/abs/2511.23174","Large Language Models (LLMs) are trained to refuse to respond to harmful content. However, systematic analyses of whether this behavior is truly a reflection of its safety policies or an indication of political censorship, that is practiced globally by countries, is lacking. Differentiating between safety influenced refusals or politically motivated censorship is hard and unclear. For this purpose we introduce PSP, a dataset built specifically to probe the refusal behaviors in LLMs from an explicitly political context. PSP is built by formatting existing censored content from two data sources, openly available on the internet: sensitive prompts in China generalized to multiple countries, and tweets that have been censored in various countries. We study: 1) impact of political sensitivity in seven LLMs through data-driven (making PSP implicit) and representation-level approaches (erasing the concept of politics); and, 2) vulnerability of models on PSP through prompt injection attacks (PIAs). Associating censorship with refusals on content with masked implicit intent, we find that most LLMs perform some form of censorship. We conclude with summarizing major attributes that can cause a shift in refusal distributions across models and contexts of different countries.","2025-11-28","2026-01-17 20:41:14","2026-01-17 20:41:14","2026-01-17 20:41:14","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2511.23174 [cs]","Comment: 15 pages, 7 tables, 4 figures","/home/arthurbrito/Zotero/storage/3KZBTCHL/Yadav et al. - 2025 - Are LLMs Good Safety Agents or a Propaganda Engine.pdf","","","Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2511.23174","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M8X59Y48","preprint","2025","Bandara, Eranga; Hass, Amin; Gore, Ross; Shetty, Sachin; Mukkamala, Ravi; Bouk, Safdar H.; Liang, Xueping; Keong, Ng Wee; Zoysa, Kasun De; Withanage, Aruna; Loganathan, Nilaan","ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications","","","","10.48550/arXiv.2512.04785","http://arxiv.org/abs/2512.04785","AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.","2025-12-04","2026-01-17 20:41:14","2026-01-17 20:41:14","2026-01-17 20:41:14","","","","","","","ASTRIDE","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2512.04785 [cs]","","/home/arthurbrito/Zotero/storage/Z8ZJKG59/Bandara et al. - 2025 - ASTRIDE A Security Threat Modeling Platform for Agentic-AI Applications.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2512.04785","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TKHF6LC7","preprint","2025","Chang, Amy; Conley, Nicholas; Ganesan, Harish Santhanalakshmi; Swanda, Adam","Death by a Thousand Prompts: Open Model Vulnerability Analysis","","","","10.48550/arXiv.2511.03247","http://arxiv.org/abs/2511.03247","Open-weight models provide researchers and developers with accessible foundations for diverse downstream applications. We tested the safety and security postures of eight open-weight large language models (LLMs) to identify vulnerabilities that may impact subsequent fine-tuning and deployment. Using automated adversarial testing, we measured each model's resilience against single-turn and multi-turn prompt injection and jailbreak attacks. Our findings reveal pervasive vulnerabilities across all tested models, with multi-turn attacks achieving success rates between 25.86\% and 92.78\% -- representing a $2\times$ to $10\times$ increase over single-turn baselines. These results underscore a systemic inability of current open-weight models to maintain safety guardrails across extended interactions. We assess that alignment strategies and lab priorities significantly influence resilience: capability-focused models such as Llama 3.3 and Qwen 3 demonstrate higher multi-turn susceptibility, whereas safety-oriented designs such as Google Gemma 3 exhibit more balanced performance. The analysis concludes that open-weight models, while crucial for innovation, pose tangible operational and ethical risks when deployed without layered security controls. These findings are intended to inform practitioners and developers of the potential risks and the value of professional AI security solutions to mitigate exposure. Addressing multi-turn vulnerabilities is essential to ensure the safe, reliable, and responsible deployment of open-weight LLMs in enterprise and public domains. We recommend adopting a security-first design philosophy and layered protections to ensure resilient deployments of open-weight models.","2025-11-05","2026-01-17 20:41:14","2026-01-17 20:41:14","2026-01-17 20:41:14","","","","","","","Death by a Thousand Prompts","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2511.03247 [cs]","","/home/arthurbrito/Zotero/storage/59YFPUQF/Chang et al. - 2025 - Death by a Thousand Prompts Open Model Vulnerability Analysis.pdf","","","Computer Science - Cryptography and Security; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2511.03247","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XW3NH8SK","preprint","2025","Lu, Weikai; Zeng, Ziqian; Zhang, Kehua; Li, Haoran; Zhuang, Huiping; Wang, Ruidong; Chen, Cen; Peng, Hao","ARGUS: Defending Against Multimodal Indirect Prompt Injection via Steering Instruction-Following Behavior","","","","10.48550/arXiv.2512.05745","http://arxiv.org/abs/2512.05745","Multimodal Large Language Models (MLLMs) are increasingly vulnerable to multimodal Indirect Prompt Injection (IPI) attacks, which embed malicious instructions in images, videos, or audio to hijack model behavior. Existing defenses, designed primarily for text-only LLMs, are unsuitable for countering these multimodal threats, as they are easily bypassed, modality-dependent, or generalize poorly. Inspired by activation steering researches, we hypothesize that a robust, general defense independent of modality can be achieved by steering the model's behavior in the representation space. Through extensive experiments, we discover that the instruction-following behavior of MLLMs is encoded in a subspace. Steering along directions within this subspace can enforce adherence to user instructions, forming the basis of a defense. However, we also found that a naive defense direction could be coupled with a utility-degrading direction, and excessive intervention strength harms model performance. To address this, we propose ARGUS, which searches for an optimal defense direction within the safety subspace that decouples from the utility degradation direction, further combining adaptive strength steering to achieve a better safety-utility trade-off. ARGUS also introduces lightweight injection detection stage to activate the defense on-demand, and a post-filtering stage to verify defense success. Experimental results show that ARGUS can achieve robust defense against multimodal IPI while maximally preserving the MLLM's utility.","2025-12-05","2026-01-17 20:41:14","2026-01-17 20:41:14","2026-01-17 20:41:14","","","","","","","ARGUS","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2512.05745 [cs]","","/home/arthurbrito/Zotero/storage/9CFGXY9D/Lu et al. - 2025 - ARGUS Defending Against Multimodal Indirect Prompt Injection via Steering Instruction-Following Beh.pdf","","","Computer Science - Cryptography and Security; Computer Science - Multimedia","","","","","","","","","","","","","","","","","","","arXiv:2512.05745","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XSJZP4UB","preprint","2025","Wang, Yihan; Yang, Huanqi; Pal, Shantanu; Xu, Weitao","AegisAgent: An Autonomous Defense Agent Against Prompt Injection Attacks in LLM-HARs","","","","10.48550/arXiv.2512.20986","http://arxiv.org/abs/2512.20986","The integration of Large Language Models (LLMs) into wearable sensing is creating a new class of mobile applications capable of nuanced human activity understanding. However, the reliability of these systems is critically undermined by their vulnerability to prompt injection attacks, where attackers deliberately input deceptive instructions into LLMs. Traditional defenses, based on static filters and rigid rules, are insufficient to address the semantic complexity of these new attacks. We argue that a paradigm shift is needed -- from passive filtering to active protection and autonomous reasoning. We introduce AegisAgent, an autonomous agent system designed to ensure the security of LLM-driven HAR systems. Instead of merely blocking threats, AegisAgent functions as a cognitive guardian. It autonomously perceives potential semantic inconsistencies, reasons about the user's true intent by consulting a dynamic memory of past interactions, and acts by generating and executing a multi-step verification and repair plan. We implement AegisAgent as a lightweight, full-stack prototype and conduct a systematic evaluation on 15 common attacks with five state-of-the-art LLM-based HAR systems on three public datasets. Results show it reduces attack success rate by 30\% on average while incurring only 78.6 ms of latency overhead on a GPU workstation. Our work makes the first step towards building secure and trustworthy LLM-driven HAR systems.","2025-12-24","2026-01-17 20:41:14","2026-01-17 20:41:14","2026-01-17 20:41:14","","","","","","","AegisAgent","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2512.20986 [cs]","","/home/arthurbrito/Zotero/storage/UD597SE2/Wang et al. - 2025 - AegisAgent An Autonomous Defense Agent Against Prompt Injection Attacks in LLM-HARs.pdf","","","Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2512.20986","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DEYCLFJQ","preprint","2025","Zhu, Changjia; Xiong, Junjie; Ma, Renkai; Lu, Zhicong; Liu, Yao; Li, Lingyao","When Your Reviewer is an LLM: Biases, Divergence, and Prompt Injection Risks in Peer Review","","","","10.48550/arXiv.2509.09912","http://arxiv.org/abs/2509.09912","Peer review is the cornerstone of academic publishing, yet the process is increasingly strained by rising submission volumes, reviewer overload, and expertise mismatches. Large language models (LLMs) are now being used as ""reviewer aids,"" raising concerns about their fairness, consistency, and robustness against indirect prompt injection attacks. This paper presents a systematic evaluation of LLMs as academic reviewers. Using a curated dataset of 1,441 papers from ICLR 2023 and NeurIPS 2022, we evaluate GPT-5-mini against human reviewers across ratings, strengths, and weaknesses. The evaluation employs structured prompting with reference paper calibration, topic modeling, and similarity analysis to compare review content. We further embed covert instructions into PDF submissions to assess LLMs' susceptibility to prompt injection. Our findings show that LLMs consistently inflate ratings for weaker papers while aligning more closely with human judgments on stronger contributions. Moreover, while overarching malicious prompts induce only minor shifts in topical focus, explicitly field-specific instructions successfully manipulate specific aspects of LLM-generated reviews. This study underscores both the promises and perils of integrating LLMs into peer review and points to the importance of designing safeguards that ensure integrity and trust in future review processes.","2025-09-12","2026-01-17 20:43:31","2026-01-17 21:19:13","2026-01-17 20:43:30","","","","","","","When Your Reviewer is an LLM","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2509.09912 [cs]","","/home/arthurbrito/Zotero/storage/NCABMWT2/Zhu et al. - 2025 - When Your Reviewer is an LLM Biases, Divergence, and Prompt Injection Risks in Peer Review.pdf","","","Computer Science - Computers and Society; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2509.09912","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2MG8PTEY","preprint","2025","Wang, Thomas; Li, Haowen","OpenGuardrails: A Configurable, Unified, and Scalable Guardrails Platform for Large Language Models","","","","10.48550/arXiv.2510.19169","http://arxiv.org/abs/2510.19169","As large language models (LLMs) are increasingly integrated into real-world applications, ensuring their safety, robustness, and privacy compliance has become critical. We present OpenGuardrails, the first fully open-source platform that unifies large-model-based safety detection, manipulation defense, and deployable guardrail infrastructure. OpenGuardrails protects against three major classes of risks: (1) content-safety violations such as harmful or explicit text generation, (2) model-manipulation attacks including prompt injection, jailbreaks, and code-interpreter abuse, and (3) data leakage involving sensitive or private information. Unlike prior modular or rule-based frameworks, OpenGuardrails introduces three core innovations: (1) a Configurable Policy Adaptation mechanism that allows per-request customization of unsafe categories and sensitivity thresholds; (2) a Unified LLM-based Guard Architecture that performs both content-safety and manipulation detection within a single model; and (3) a Quantized, Scalable Model Design that compresses a 14B dense base model to 3.3B via GPTQ while preserving over 98 of benchmark accuracy. The system supports 119 languages, achieves state-of-the-art performance across multilingual safety benchmarks, and can be deployed as a secure gateway or API-based service for enterprise use. All models, datasets, and deployment scripts are released under the Apache 2.0 license.","2025-10-29","2026-01-17 20:43:31","2026-01-17 21:18:52","2026-01-17 20:43:30","","","","","","","OpenGuardrails","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2510.19169 [cs]","","/home/arthurbrito/Zotero/storage/BCICV824/Wang e Li - 2025 - OpenGuardrails A Configurable, Unified, and Scalable Guardrails Platform for Large Language Models.pdf","","","Computer Science - Computation and Language; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2510.19169","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3FRZKQYP","preprint","2025","Peer, David; Stabinger, Sebastian","ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents","","","","10.48550/arXiv.2510.16381","http://arxiv.org/abs/2510.16381","Large Language Models (LLMs) have demonstrated impressive capabilities, yet their deployment in high-stakes domains is hindered by inherent limitations in trustworthiness, including hallucinations, instability, and a lack of transparency. To address these challenges, we introduce a generic neuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The core of our approach lies in decoupling tasks into two distinct phases: Offline knowledge ingestion and online task processing. During knowledge ingestion, an LLM translates an informal problem specification into a formal, symbolic knowledge base. This formal representation is crucial as it can be verified and refined by human experts, ensuring its correctness and alignment with domain requirements. In the subsequent task processing phase, each incoming input is encoded into the same formal language. A symbolic decision engine then utilizes this encoded input in conjunction with the formal knowledge base to derive a reliable result. Through an extensive evaluation on a complex reasoning task, we demonstrate that a concrete implementation of ATA is competitive with state-of-the-art end-to-end reasoning models in a fully automated setup while maintaining trustworthiness. Crucially, with a human-verified and corrected knowledge base, our approach significantly outperforms even larger models, while exhibiting perfect determinism, enhanced stability against input perturbations, and inherent immunity to prompt injection attacks. By generating decisions grounded in symbolic reasoning, ATA offers a practical and controllable architecture for building the next generation of transparent, auditable, and reliable autonomous agents.","2025-10-18","2026-01-17 20:43:31","2026-01-17 21:17:29","2026-01-17 20:43:30","","","","","","","ATA","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2510.16381 [cs]","","/home/arthurbrito/Zotero/storage/J8EPIJ3P/Peer e Stabinger - 2025 - ATA A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2510.16381","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PIYC8LKR","preprint","2025","Zhang, Dongsen; Li, Zekun; Luo, Xu; Liu, Xuannan; Li, Peipei; Xu, Wenjun","MCP Security Bench (MSB): Benchmarking Attacks Against Model Context Protocol in LLM Agents","","","","10.48550/arXiv.2510.15994","http://arxiv.org/abs/2510.15994","The Model Context Protocol (MCP) standardizes how large language model (LLM) agents discover, describe, and call external tools. While MCP unlocks broad interoperability, it also enlarges the attack surface by making tools first-class, composable objects with natural-language metadata, and standardized I/O. We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP). We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. Results reveal the effectiveness of attacks against each stage of MCP. Models with stronger performance are more vulnerable to attacks due to their outstanding tool calling and instruction following capabilities. MSB provides a practical baseline for researchers and practitioners to study, compare, and harden MCP agents.","2025-10-14","2026-01-17 20:43:31","2026-01-17 21:18:43","2026-01-17 20:43:30","","","","","","","MCP Security Bench (MSB)","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2510.15994 [cs]","","/home/arthurbrito/Zotero/storage/D75SMAFP/Zhang et al. - 2025 - MCP Security Bench (MSB) Benchmarking Attacks Against Model Context Protocol in LLM Agents.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2510.15994","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQ2N64VA","preprint","2025","Lilienthal, Derek; Hong, Sanghyun","Mind the Gap: Time-of-Check to Time-of-Use Vulnerabilities in LLM-Enabled Agents","","","","10.48550/arXiv.2508.17155","http://arxiv.org/abs/2508.17155","Large Language Model (LLM)-enabled agents are rapidly emerging across a wide range of applications, but their deployment introduces vulnerabilities with security implications. While prior work has examined prompt-based attacks (e.g., prompt injection) and data-oriented threats (e.g., data exfiltration), time-of-check to time-of-use (TOCTOU) remain largely unexplored in this context. TOCTOU arises when an agent validates external state (e.g., a file or API response) that is later modified before use, enabling practical attacks such as malicious configuration swaps or payload injection. In this work, we present the first study of TOCTOU vulnerabilities in LLM-enabled agents. We introduce TOCTOU-Bench, a benchmark with 66 realistic user tasks designed to evaluate this class of vulnerabilities. As countermeasures, we adapt detection and mitigation techniques from systems security to this setting and propose prompt rewriting, state integrity monitoring, and tool-fusing. Our study highlights challenges unique to agentic workflows, where we achieve up to 25% detection accuracy using automated detection methods, a 3% decrease in vulnerable plan generation, and a 95% reduction in the attack window. When combining all three approaches, we reduce the TOCTOU vulnerabilities from an executed trajectory from 12% to 8%. Our findings open a new research direction at the intersection of AI safety and systems security.","2025-08-23","2026-01-17 20:43:31","2026-01-17 21:18:47","2026-01-17 20:43:30","","","","","","","Mind the Gap","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2508.17155 [cs]","Comment: Pre-print; Comment: Pre-print","/home/arthurbrito/Zotero/storage/I88EUAW7/Lilienthal e Hong - 2025 - Mind the Gap Time-of-Check to Time-of-Use Vulnerabilities in LLM-Enabled Agents.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2508.17155","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MLLE45MM","preprint","2025","Verma, Ishaan; Yadav, Arsheya","Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization","","","","10.48550/arXiv.2509.05831","http://arxiv.org/abs/2509.05831","Large Language Models (LLMs) are increasingly integrated into web-based systems for content summarization, yet their susceptibility to prompt injection attacks remains a pressing concern. In this study, we explore how non-visible HTML elements such as <meta>, aria-label, and alt attributes can be exploited to embed adversarial instructions without altering the visible content of a webpage. We introduce a novel dataset comprising 280 static web pages, evenly divided between clean and adversarial injected versions, crafted using diverse HTML-based strategies. These pages are processed through a browser automation pipeline to extract both raw HTML and rendered text, closely mimicking real-world LLM deployment scenarios. We evaluate two state-of-the-art open-source models, Llama 4 Scout (Meta) and Gemma 9B IT (Google), on their ability to summarize this content. Using both lexical (ROUGE-L) and semantic (SBERT cosine similarity) metrics, along with manual annotations, we assess the impact of these covert injections. Our findings reveal that over 29% of injected samples led to noticeable changes in the Llama 4 Scout summaries, while Gemma 9B IT showed a lower, yet non-trivial, success rate of 15%. These results highlight a critical and largely overlooked vulnerability in LLM driven web pipelines, where hidden adversarial content can subtly manipulate model outputs. Our work offers a reproducible framework and benchmark for evaluating HTML-based prompt injection and underscores the urgent need for robust mitigation strategies in LLM applications involving web content.","2025-11-11","2026-01-17 20:43:31","2026-01-17 21:17:59","2026-01-17 20:43:30","","","","","","","Decoding Latent Attack Surfaces in LLMs","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2509.05831 [cs]","","/home/arthurbrito/Zotero/storage/QK2ANR6I/Verma e Yadav - 2025 - Decoding Latent Attack Surfaces in LLMs Prompt Injection via HTML in Web Summarization.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2509.05831","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KVUCYHHT","preprint","2025","Hossain, S. M. Asif; Shayoni, Ruksat Khan; Ameen, Mohd Ruhul; Islam, Akif; Mridha, M. F.; Shin, Jungpil","A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks","","","","10.48550/arXiv.2509.14285","http://arxiv.org/abs/2509.14285","Prompt injection attacks represent a major vulnerability in Large Language Model (LLM) deployments, where malicious instructions embedded in user inputs can override system prompts and induce unintended behaviors. This paper presents a novel multi-agent defense framework that employs specialized LLM agents in coordinated pipelines to detect and neutralize prompt injection attacks in real-time. We evaluate our approach using two distinct architectures: a sequential chain-of-agents pipeline and a hierarchical coordinator-based system. Our comprehensive evaluation on 55 unique prompt injection attacks, grouped into 8 categories and totaling 400 attack instances across two LLM platforms (ChatGLM and Llama2), demonstrates significant security improvements. Without defense mechanisms, baseline Attack Success Rates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent pipeline achieved 100% mitigation, reducing ASR to 0% across all tested scenarios. The framework demonstrates robustness across multiple attack categories including direct overrides, code execution attempts, data exfiltration, and obfuscation techniques, while maintaining system functionality for legitimate queries.","2025-12-17","2026-01-17 20:43:31","2026-01-17 21:17:23","2026-01-17 20:43:30","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2509.14285 [cs]","Comment: Accepted at the 11th IEEE WIECON-ECE 2025; Comment: Accepted at the 11th IEEE WIECON-ECE 2025","/home/arthurbrito/Zotero/storage/9VSCUILU/Hossain et al. - 2025 - A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks.pdf","","","Computer Science - Cryptography and Security; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2509.14285","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5EXBDRPU","preprint","2025","An, Hengyu; Zhang, Jinghuai; Du, Tianyu; Zhou, Chunyi; Li, Qingming; Lin, Tao; Ji, Shouling","IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents","","","","10.48550/arXiv.2508.15310","http://arxiv.org/abs/2508.15310","Large language model (LLM) agents are widely deployed in real-world applications, where they leverage tools to retrieve and manipulate external data for complex tasks. However, when interacting with untrusted data sources (e.g., fetching information from public websites), tool responses may contain injected instructions that covertly influence agent behaviors and lead to malicious outcomes, a threat referred to as Indirect Prompt Injection (IPI). Existing defenses typically rely on advanced prompting strategies or auxiliary detection models. While these methods have demonstrated some effectiveness, they fundamentally rely on assumptions about the model's inherent security, which lacks structural constraints on agent behaviors. As a result, agents still retain unrestricted access to tool invocations, leaving them vulnerable to stronger attack vectors that can bypass the security guardrails of the model. To prevent malicious tool invocations at the source, we propose a novel defensive task execution paradigm, called IPIGuard, which models the agents' task execution process as a traversal over a planned Tool Dependency Graph (TDG). By explicitly decoupling action planning from interaction with external data, IPIGuard significantly reduces unintended tool invocations triggered by injected instructions, thereby enhancing robustness against IPI attacks. Experiments on the AgentDojo benchmark show that IPIGuard achieves a superior balance between effectiveness and robustness, paving the way for the development of safer agentic systems in dynamic environments.","2025-08-21","2026-01-17 20:43:31","2026-01-17 21:18:23","2026-01-17 20:43:30","","","","","","","IPIGuard","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2508.15310 [cs]","Comment: EMNLP 2025; Comment: EMNLP 2025","/home/arthurbrito/Zotero/storage/5GZ7ZL6I/An et al. - 2025 - IPIGuard A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agent.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2508.15310","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8CD6HQTL","preprint","2025","Oliveira, Matheus Vinicius da Silva de; Silva, Jonathan de Andrade; Fontao, Awdren de Lima","Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations Reveal Bias in Small Language Models","","","","10.48550/arXiv.2509.26584","http://arxiv.org/abs/2509.26584","Large Language Models (LLMs) are widely used across multiple domains but continue to raise concerns regarding security and fairness. Beyond known attack vectors such as data poisoning and prompt injection, LLMs are also vulnerable to fairness bugs. These refer to unintended behaviors influenced by sensitive demographic cues (e.g., race or sexual orientation) that should not affect outcomes. Another key issue is hallucination, where models generate plausible yet false information. Retrieval-Augmented Generation (RAG) has emerged as a strategy to mitigate hallucinations by combining external retrieval with text generation. However, its adoption raises new fairness concerns, as the retrieved content itself may surface or amplify bias. This study conducts fairness testing through metamorphic testing (MT), introducing controlled demographic perturbations in prompts to assess fairness in sentiment analysis performed by three Small Language Models (SLMs) hosted on HuggingFace (Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3.1-Nemotron-8B), each integrated into a RAG pipeline. Results show that minor demographic variations can break up to one third of metamorphic relations (MRs). A detailed analysis of these failures reveals a consistent bias hierarchy, with perturbations involving racial cues being the predominant cause of the violations. In addition to offering a comparative evaluation, this work reinforces that the retrieval component in RAG must be carefully curated to prevent bias amplification. The findings serve as a practical alert for developers, testers and small organizations aiming to adopt accessible SLMs without compromising fairness or reliability.","2025-09-30","2026-01-17 20:43:31","2026-01-17 21:18:09","2026-01-17 20:43:30","","","","","","","Fairness Testing in Retrieval-Augmented Generation","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2509.26584 [cs]","","/home/arthurbrito/Zotero/storage/WLYR2HMQ/Oliveira et al. - 2025 - Fairness Testing in Retrieval-Augmented Generation How Small Perturbations Reveal Bias in Small Lan.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Information Retrieval; Computer Science - Machine Learning; Computer Science - Software Engineering","","","","","","","","","","","","","","","","","","","arXiv:2509.26584","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZXEGLGRM","preprint","2025","Schwarz, Dominik","Countermind: A Multi-Layered Security Architecture for Large Language Models","","","","10.36227/techrxiv.175994550.08962082/v1","http://arxiv.org/abs/2510.11837","The security of Large Language Model (LLM) applications is fundamentally challenged by ""form-first"" attacks like prompt injection and jailbreaking, where malicious instructions are embedded within user inputs. Conventional defenses, which rely on post hoc output filtering, are often brittle and fail to address the root cause: the model's inability to distinguish trusted instructions from untrusted data. This paper proposes Countermind, a multi-layered security architecture intended to shift defenses from a reactive, post hoc posture to a proactive, pre-inference, and intra-inference enforcement model. The architecture proposes a fortified perimeter designed to structurally validate and transform all inputs, and an internal governance mechanism intended to constrain the model's semantic processing pathways before an output is generated. The primary contributions of this work are conceptual designs for: (1) A Semantic Boundary Logic (SBL) with a mandatory, time-coupled Text Crypter intended to reduce the plaintext prompt injection attack surface, provided all ingestion paths are enforced. (2) A Parameter-Space Restriction (PSR) mechanism, leveraging principles from representation engineering, to dynamically control the LLM's access to internal semantic clusters, with the goal of mitigating semantic drift and dangerous emergent behaviors. (3) A Secure, Self-Regulating Core that uses an OODA loop and a learning security module to adapt its defenses based on an immutable audit log. (4) A Multimodal Input Sandbox and Context-Defense mechanisms to address threats from non-textual data and long-term semantic poisoning. This paper outlines an evaluation plan designed to quantify the proposed architecture's effectiveness in reducing the Attack Success Rate (ASR) for form-first attacks and to measure its potential latency overhead.","2025-10-08","2026-01-17 20:43:31","2026-01-17 21:17:56","2026-01-17 20:43:31","","","","","","","Countermind","","","","","","","","","","","","arXiv.org","","arXiv:2510.11837 [cs]","Comment: 33 pages, 3 figures, 6 tables. Keywords: LLM security; defense-in-depth; prompt injection; activation steering; multimodal sandbox; threat modeling; Comment: 33 pages, 3 figures, 6 tables. Keywords: LLM security; defense-in-depth; prompt injection; activation steering; multimodal sandbox; threat modeling","/home/arthurbrito/Zotero/storage/QLW8QEY4/Schwarz - 2025 - Countermind A Multi-Layered Security Architecture for Large Language Models.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MCIX9M7J","preprint","2025","Pham, Dzung; Kairouz, Peter; Mireshghallah, Niloofar; Bagdasarian, Eugene; Pham, Chau Minh; Houmansadr, Amir","Can Large Language Models Really Recognize Your Name?","","","","10.48550/arXiv.2505.14549","http://arxiv.org/abs/2505.14549","Large language models (LLMs) are increasingly being used to protect sensitive user data. However, current LLM-based privacy solutions assume that these models can reliably detect personally identifiable information (PII), particularly named entities. In this paper, we challenge that assumption by revealing systematic failures in LLM-based privacy tasks. Specifically, we show that modern LLMs regularly overlook human names even in short text snippets due to ambiguous contexts, which cause the names to be misinterpreted or mishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous human names, leveraging the name regularity bias phenomenon, embedded within concise text snippets along with benign prompt injections. Our experiments on modern LLMs tasked to detect PII as well as specialized tools show that recall of ambiguous names drops by 20--40% compared to more recognizable names. Furthermore, ambiguous human names are four times more likely to be ignored in supposedly privacy-preserving summaries generated by LLMs when benign prompt injections are present. These findings highlight the underexplored risks of relying solely on LLMs to safeguard user privacy and underscore the need for a more systematic investigation into their privacy failure modes.","2025-05-20","2026-01-17 20:45:03","2026-01-17 20:45:03","2026-01-17 20:44:48","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2505.14549 [cs]","","/home/arthurbrito/Zotero/storage/6HPNW4ZY/Pham et al. - 2025 - Can Large Language Models Really Recognize Your Name.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2505.14549","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UC2JXSMZ","preprint","2025","Ji, Yi; Li, Runzhi; Mao, Baolei","Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering","","","","10.48550/arXiv.2506.06384","http://arxiv.org/abs/2506.06384","With the widespread adoption of Large Language Models (LLMs), prompt injection attacks have emerged as a significant security threat. Existing defense mechanisms often face critical trade-offs between effectiveness and generalizability. This highlights the urgent need for efficient prompt injection detection methods that are applicable across a wide range of LLMs. To address this challenge, we propose DMPI-PMHFE, a dual-channel feature fusion detection framework. It integrates a pretrained language model with heuristic feature engineering to detect prompt injection attacks. Specifically, the framework employs DeBERTa-v3-base as a feature extractor to transform input text into semantic vectors enriched with contextual information. In parallel, we design heuristic rules based on known attack patterns to extract explicit structural features commonly observed in attacks. Features from both channels are subsequently fused and passed through a fully connected neural network to produce the final prediction. This dual-channel approach mitigates the limitations of relying only on DeBERTa to extract features. Experimental results on diverse benchmark datasets demonstrate that DMPI-PMHFE outperforms existing methods in terms of accuracy, recall, and F1-score. Furthermore, when deployed actually, it significantly reduces attack success rates across mainstream LLMs, including GLM-4, LLaMA 3, Qwen 2.5, and GPT-4o.","2025-06-05","2026-01-17 20:45:03","2026-01-17 20:45:03","2026-01-17 20:44:48","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2506.06384 [cs]","Comment: Accepted by KSEM2025 AI & Sec Workshop","/home/arthurbrito/Zotero/storage/J3J89TT7/Ji et al. - 2025 - Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Enginee.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2506.06384","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X8WU572Z","preprint","2025","Patlan, Atharv Singh; Hebbar, Ashwin; Viswanath, Pramod; Mittal, Prateek","Context manipulation attacks : Web agents are susceptible to corrupted memory","","","","10.48550/arXiv.2506.17318","http://arxiv.org/abs/2506.17318","Autonomous web navigation agents, which translate natural language instructions into sequences of browser actions, are increasingly deployed for complex tasks across e-commerce, information retrieval, and content discovery. Due to the stateless nature of large language models (LLMs), these agents rely heavily on external memory systems to maintain context across interactions. Unlike centralized systems where context is securely stored server-side, agent memory is often managed client-side or by third-party applications, creating significant security vulnerabilities. This was recently exploited to attack production systems. We introduce and formalize ""plan injection,"" a novel context manipulation attack that corrupts these agents' internal task representations by targeting this vulnerable context. Through systematic evaluation of two popular web agents, Browser-use and Agent-E, we show that plan injections bypass robust prompt injection defenses, achieving up to 3x higher attack success rates than comparable prompt-based attacks. Furthermore, ""context-chained injections,"" which craft logical bridges between legitimate user goals and attacker objectives, lead to a 17.7% increase in success rate for privacy exfiltration tasks. Our findings highlight that secure memory handling must be a first-class concern in agentic systems.","2025-06-18","2026-01-17 20:45:03","2026-01-17 20:45:03","2026-01-17 20:44:48","","","","","","","Context manipulation attacks","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2506.17318 [cs]","Comment: 10 pages, 6 figures","/home/arthurbrito/Zotero/storage/UGAJVGVQ/Patlan et al. - 2025 - Context manipulation attacks  Web agents are susceptible to corrupted memory.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2506.17318","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UGQRS4GN","preprint","2025","Almheiri, Saeed; Kongrat, Yerulan; Santosh, Adrian; Tasmukhanov, Ruslan; Vera, Josemaria Loza; Kautsar, Muhammad Dehan Al; Koto, Fajri","Role-Aware Language Models for Secure and Contextualized Access Control in Organizations","","","","10.48550/arXiv.2507.23465","http://arxiv.org/abs/2507.23465","As large language models (LLMs) are increasingly deployed in enterprise settings, controlling model behavior based on user roles becomes an essential requirement. Existing safety methods typically assume uniform access and focus on preventing harmful or toxic outputs, without addressing role-specific access constraints. In this work, we investigate whether LLMs can be fine-tuned to generate responses that reflect the access privileges associated with different organizational roles. We explore three modeling strategies: a BERT-based classifier, an LLM-based classifier, and role-conditioned generation. To evaluate these approaches, we construct two complementary datasets. The first is adapted from existing instruction-tuning corpora through clustering and role labeling, while the second is synthetically generated to reflect realistic, role-sensitive enterprise scenarios. We assess model performance across varying organizational structures and analyze robustness to prompt injection, role mismatch, and jailbreak attempts.","2025-08-12","2026-01-17 20:45:03","2026-01-17 20:45:03","2026-01-17 20:44:48","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2507.23465 [cs]","","/home/arthurbrito/Zotero/storage/NYECGV6M/Almheiri et al. - 2025 - Role-Aware Language Models for Secure and Contextualized Access Control in Organizations.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2507.23465","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6YTITLT6","preprint","2025","Alizadeh, Meysam; Samei, Zeynab; Stetsenko, Daria; Gilardi, Fabrizio","Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution","","","","10.48550/arXiv.2506.01055","http://arxiv.org/abs/2506.01055","Previous benchmarks on prompt injection in large language models (LLMs) have primarily focused on generic tasks and attacks, offering limited insights into more complex threats like data exfiltration. This paper examines how prompt injection can cause tool-calling agents to leak personal data observed during task execution. Using a fictitious banking agent, we develop data flow-based attacks and integrate them into AgentDojo, a recent benchmark for agentic security. To enhance its scope, we also create a richer synthetic dataset of human-AI banking conversations. In 16 user tasks from AgentDojo, LLMs show a 15-50 percentage point drop in utility under attack, with average attack success rates (ASR) around 20 percent; some defenses reduce ASR to zero. Most LLMs, even when successfully tricked by the attack, avoid leaking highly sensitive data like passwords, likely due to safety alignments, but they remain vulnerable to disclosing other personal data. The likelihood of password leakage increases when a password is requested along with one or two additional personal details. In an extended evaluation across 48 tasks, the average ASR is around 15 percent, with no built-in AgentDojo defense fully preventing leakage. Tasks involving data extraction or authorization workflows, which closely resemble the structure of exfiltration attacks, exhibit the highest ASRs, highlighting the interaction between task type, agent performance, and defense efficacy.","2025-06-01","2026-01-17 20:45:03","2026-01-17 21:19:06","2026-01-17 20:44:48","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2506.01055 [cs]","Comment: 25 pages, 18 figures, NeurIPS formatting style; Comment: 25 pages, 18 figures, NeurIPS formatting style","/home/arthurbrito/Zotero/storage/PP86C6HK/Alizadeh et al. - 2025 - Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution.pdf","","","Computer Science - Computation and Language; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2506.01055","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JBEFAKAH","preprint","2025","Gandhi, Parth Atulbhai; Shukla, Akansha; Tayouri, David; Ifland, Beni; Elovici, Yuval; Puzis, Rami; Shabtai, Asaf","ATAG: AI-Agent Application Threat Assessment with Attack Graphs","","","","10.48550/arXiv.2506.02859","http://arxiv.org/abs/2506.02859","Evaluating the security of multi-agent systems (MASs) powered by large language models (LLMs) is challenging, primarily because of the systems' complex internal dynamics and the evolving nature of LLM vulnerabilities. Traditional attack graph (AG) methods often lack the specific capabilities to model attacks on LLMs. This paper introduces AI-agent application Threat assessment with Attack Graphs (ATAG), a novel framework designed to systematically analyze the security risks associated with AI-agent applications. ATAG extends the MulVAL logic-based AG generation tool with custom facts and interaction rules to accurately represent AI-agent topologies, vulnerabilities, and attack scenarios. As part of this research, we also created the LLM vulnerability database (LVD) to initiate the process of standardizing LLM vulnerabilities documentation. To demonstrate ATAG's efficacy, we applied it to two multi-agent applications. Our case studies demonstrated the framework's ability to model and generate AGs for sophisticated, multi-step attack scenarios exploiting vulnerabilities such as prompt injection, excessive agency, sensitive information disclosure, and insecure output handling across interconnected agents. ATAG is an important step toward a robust methodology and toolset to help understand, visualize, and prioritize complex attack paths in multi-agent AI systems (MAASs). It facilitates proactive identification and mitigation of AI-agent threats in multi-agent applications.","2025-06-03","2026-01-17 20:45:03","2026-01-17 20:45:03","2026-01-17 20:44:48","","","","","","","ATAG","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2506.02859 [cs]","","/home/arthurbrito/Zotero/storage/8MCS2GQY/Gandhi et al. - 2025 - ATAG AI-Agent Application Threat Assessment with Attack Graphs.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2506.02859","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8S2QXLX4","preprint","2025","Panebianco, Francesco; Bonfanti, Stefano; Trovò, Francesco; Carminati, Michele","LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks","","","","10.48550/arXiv.2508.00602","http://arxiv.org/abs/2508.00602","The generalization capabilities of Large Language Models (LLMs) have led to their widespread deployment across various applications. However, this increased adoption has introduced several security threats, notably in the forms of jailbreaking and data leakage attacks. Additionally, Retrieval Augmented Generation (RAG), while enhancing context-awareness in LLM responses, has inadvertently introduced vulnerabilities that can result in the leakage of sensitive information. Our contributions are twofold. First, we introduce a methodology to analyze historical interaction data from an LLM system, enabling the generation of usage maps categorized by topics (including adversarial interactions). This approach further provides forensic insights for tracking the evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a model-agnostic framework that combines static analysis for forensic insights with dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique identifies topic groups and detects anomalous patterns, allowing for proactive defense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1) jailbreak attempts, employing a public benchmark dataset, and (2) PII leakage, supported by a curated dataset of labeled LLM interactions. In the static setting, LeakSealer achieves the highest precision and recall on the ToxicChat dataset when identifying prompt injection. In the dynamic setting, PII leakage detection achieves an AUPRC of $0.97$, significantly outperforming baselines such as Llama Guard.","2025-08-01","2026-01-17 20:45:03","2026-01-17 20:45:03","2026-01-17 20:44:48","","","","","","","LeakSealer","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2508.00602 [cs]","Comment: 22 pages, preprint","/home/arthurbrito/Zotero/storage/SPYTID8W/Panebianco et al. - 2025 - LeakSealer A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2508.00602","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MSVHVM8W","preprint","2025","Nouailles, Rafaël","Evaluation empirique de la sécurisation et de l'alignement de ChatGPT et Gemini: analyse comparative des vulnérabilités par expérimentations de jailbreaks","","","","10.48550/arXiv.2506.10029","http://arxiv.org/abs/2506.10029","Large Language models (LLMs) are transforming digital usage, particularly in text generation, image creation, information retrieval and code development. ChatGPT, launched by OpenAI in November 2022, quickly became a reference, prompting the emergence of competitors such as Google's Gemini. However, these technological advances raise new cybersecurity challenges, including prompt injection attacks, the circumvention of regulatory measures (jailbreaking), the spread of misinformation (hallucinations) and risks associated with deep fakes. This paper presents a comparative analysis of the security and alignment levels of ChatGPT and Gemini, as well as a taxonomy of jailbreak techniques associated with experiments.","2025-06-10","2026-01-17 20:45:03","2026-01-17 20:45:03","2026-01-17 20:44:48","","","","","","","Evaluation empirique de la sécurisation et de l'alignement de ChatGPT et Gemini","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2506.10029 [cs]","Comment: in French language","/home/arthurbrito/Zotero/storage/J44VA4EG/Nouailles - 2025 - Evaluation empirique de la sécurisation et de l'alignement de ChatGPT et Gemini analyse comparative.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2506.10029","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5RNM2I4U","preprint","2025","Abdelnabi, Sahar; Fay, Aideen; Salem, Ahmed; Zverev, Egor; Liao, Kai-Chieh; Liu, Chi-Huang; Kuo, Chun-Chih; Weigend, Jannis; Manlangit, Danyael; Apostolov, Alex; Umair, Haris; Donato, João; Kawakita, Masayuki; Mahboob, Athar; Bach, Tran Huu; Chiang, Tsun-Han; Cho, Myeongjin; Choi, Hajin; Kim, Byeonghyeon; Lee, Hyeonjin; Pannell, Benjamin; McCauley, Conor; Russinovich, Mark; Paverd, Andrew; Cherubin, Giovanni","LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge","","","","10.48550/arXiv.2506.09956","http://arxiv.org/abs/2506.09956","Indirect Prompt Injection attacks exploit the inherent limitation of Large Language Models (LLMs) to distinguish between instructions and data in their inputs. Despite numerous defense proposals, the systematic evaluation against adaptive adversaries remains limited, even when successful attacks can have wide security and privacy implications, and many real-world LLM-based applications remain vulnerable. We present the results of LLMail-Inject, a public challenge simulating a realistic scenario in which participants adaptively attempted to inject malicious instructions into emails in order to trigger unauthorized tool calls in an LLM-based email assistant. The challenge spanned multiple defense strategies, LLM architectures, and retrieval configurations, resulting in a dataset of 208,095 unique attack submissions from 839 participants. We release the challenge code, the full dataset of submissions, and our analysis demonstrating how this data can provide new insights into the instruction-data separation problem. We hope this will serve as a foundation for future research towards practical structural solutions to prompt injection.","2025-06-11","2026-01-17 20:45:03","2026-01-17 20:45:03","2026-01-17 20:44:48","","","","","","","LLMail-Inject","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2506.09956 [cs]","Comment: Dataset at: https://huggingface.co/datasets/microsoft/llmail-inject-challenge","","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2506.09956","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KNX7LJRX","preprint","2025","Ward, Chris M.; Harguess, Josh","Adversarial Threat Vectors and Risk Mitigation for Retrieval-Augmented Generation Systems","","","","10.1117/12.3055931","http://arxiv.org/abs/2506.00281","Retrieval-Augmented Generation (RAG) systems, which integrate Large Language Models (LLMs) with external knowledge sources, are vulnerable to a range of adversarial attack vectors. This paper examines the importance of RAG systems through recent industry adoption trends and identifies the prominent attack vectors for RAG: prompt injection, data poisoning, and adversarial query manipulation. We analyze these threats under risk management lens, and propose robust prioritized control list that includes risk-mitigating actions like input validation, adversarial training, and real-time monitoring.","2025-05-30","2026-01-17 20:45:03","2026-01-17 20:45:03","2026-01-17 20:45:03","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv:2506.00281 [cs]","Comment: SPIE DCS: Proceedings Volume Assurance and Security for AI-enabled Systems 2025","","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TUGN9HJS","preprint","2025","Feng, Yang; Pan, Xudong","StruPhantom: Evolutionary Injection Attacks on Black-Box Tabular Agents Powered by Large Language Models","","","","10.48550/arXiv.2504.09841","http://arxiv.org/abs/2504.09841","The proliferation of autonomous agents powered by large language models (LLMs) has revolutionized popular business applications dealing with tabular data, i.e., tabular agents. Although LLMs are observed to be vulnerable against prompt injection attacks from external data sources, tabular agents impose strict data formats and predefined rules on the attacker's payload, which are ineffective unless the agent navigates multiple layers of structural data to incorporate the payload. To address the challenge, we present a novel attack termed StruPhantom which specifically targets black-box LLM-powered tabular agents. Our attack designs an evolutionary optimization procedure which continually refines attack payloads via the proposed constrained Monte Carlo Tree Search augmented by an off-topic evaluator. StruPhantom helps systematically explore and exploit the weaknesses of target applications to achieve goal hijacking. Our evaluation validates the effectiveness of StruPhantom across various LLM-based agents, including those on real-world platforms, and attack scenarios. Our attack achieves over 50% higher success rates than baselines in enforcing the application's response to contain phishing links or malicious codes.","2025-04-14","2026-01-17 20:45:30","2026-01-17 20:45:30","2026-01-17 20:45:30","","","","","","","StruPhantom","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2504.09841 [cs]","Comment: Work in Progress","/home/arthurbrito/Zotero/storage/LU89PLFW/Feng e Pan - 2025 - StruPhantom Evolutionary Injection Attacks on Black-Box Tabular Agents Powered by Large Language Mo.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2504.09841","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JZ4PNY2E","preprint","2025","Zhang, Tao; Li, Xiangtai; Huang, Zilong; Li, Yanwei; Lei, Weixian; Deng, Xueqing; Chen, Shihao; Ji, Shunping; Feng, Jiashi","Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding","","","","10.48550/arXiv.2504.10465","http://arxiv.org/abs/2504.10465","Multimodal Large Language Models (MLLMs) achieve remarkable performance for fine-grained pixel-level understanding tasks. However, all the works rely heavily on extra components, such as vision encoder (CLIP), segmentation experts, leading to high system complexity and limiting model scaling. In this work, our goal is to explore a highly simplified MLLM without introducing extra components. Our work is motivated by the recent works on Single trAnsformer as a unified vIsion-Language Model (SAIL) design, where these works jointly learn vision tokens and text tokens in transformers. We present Pixel-SAIL, a single transformer for pixel-wise MLLM tasks. In particular, we present three technical improvements on the plain baseline. First, we design a learnable upsampling module to refine visual token features. Secondly, we propose a novel visual prompt injection strategy to enable the single transformer to understand visual prompt inputs and benefit from the early fusion of visual prompt embeddings and vision tokens. Thirdly, we introduce a vision expert distillation strategy to efficiently enhance the single transformer's fine-grained feature extraction capability. In addition, we have collected a comprehensive pixel understanding benchmark (PerBench), using a manual check. It includes three tasks: detailed object description, visual prompt-based question answering, and visual-text referring segmentation. Extensive experiments on four referring segmentation benchmarks, one visual prompt benchmark, and our PerBench show that our Pixel-SAIL achieves comparable or even better results with a much simpler pipeline. Code and model will be released at https://github.com/magic-research/Sa2VA.","2025-04-14","2026-01-17 20:45:30","2026-01-17 20:45:30","2026-01-17 20:45:30","","","","","","","Pixel-SAIL","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2504.10465 [cs]","","/home/arthurbrito/Zotero/storage/BGYRLC89/Zhang et al. - 2025 - Pixel-SAIL Single Transformer For Pixel-Grounded Understanding.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2504.10465","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TDE4ELY3","preprint","2025","Li, Evan; Mallick, Tushin; Rose, Evan; Robertson, William; Oprea, Alina; Nita-Rotaru, Cristina","ACE: A Security Architecture for LLM-Integrated App Systems","","","","10.48550/arXiv.2504.20984","http://arxiv.org/abs/2504.20984","LLM-integrated app systems extend the utility of Large Language Models (LLMs) with third-party apps that are invoked by a system LLM using interleaved planning and execution phases to answer user queries. These systems introduce new attack vectors where malicious apps can cause integrity violation of planning or execution, availability breakdown, or privacy compromise during execution. In this work, we identify new attacks impacting the integrity of planning, as well as the integrity and availability of execution in LLM-integrated apps, and demonstrate them against IsolateGPT, a recent solution designed to mitigate attacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new secure architecture for LLM-integrated app systems that provides security guarantees for system planning and execution. Specifically, ACE decouples planning into two phases by first creating an abstract execution plan using only trusted information, and then mapping the abstract plan to a concrete plan using installed system apps. We verify that the plans generated by our system satisfy user-specified secure information flow constraints via static analysis on the structured plan output. During execution, ACE enforces data and capability barriers between apps, and ensures that the execution is conducted according to the trusted abstract plan. We show experimentally that ACE is secure against attacks from the InjecAgent and Agent Security Bench benchmarks for indirect prompt injection, and our newly introduced attacks. We also evaluate the utility of ACE in realistic environments, using the Tool Usage suite from the LangChain benchmark. Our architecture represents a significant advancement towards hardening LLM-based systems using system security principles.","2025-09-10","2026-01-17 20:45:30","2026-01-17 20:45:30","2026-01-17 20:45:30","","","","","","","ACE","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2504.20984 [cs]","Comment: 25 pages, 13 figures, 8 tables; accepted by Network and Distributed System Security Symposium (NDSS) 2026","/home/arthurbrito/Zotero/storage/6592IBFR/Li et al. - 2025 - ACE A Security Architecture for LLM-Integrated App Systems.pdf","","","Computer Science - Cryptography and Security; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2504.20984","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8A8Y89MA","preprint","2025","Pan, Jonathan; Wong, Swee Liang; Yuan, Yidi; Chia, Xin Wei","Prompt Inject Detection with Generative Explanation as an Investigative Tool","","","","10.48550/arXiv.2502.11006","http://arxiv.org/abs/2502.11006","Large Language Models (LLMs) are vulnerable to adversarial prompt based injects. These injects could jailbreak or exploit vulnerabilities within these models with explicit prompt requests leading to undesired responses. In the context of investigating prompt injects, the challenge is the sheer volume of input prompts involved that are likely to be largely benign. This investigative challenge is further complicated by the semantics and subjectivity of the input prompts involved in the LLM conversation with its user and the context of the environment to which the conversation is being carried out. Hence, the challenge for AI security investigators would be two-fold. The first is to identify adversarial prompt injects and then to assess whether the input prompt is contextually benign or adversarial. For the first step, this could be done using existing AI security solutions like guardrails to detect and protect the LLMs. Guardrails have been developed using a variety of approaches. A popular approach is to use signature based. Another popular approach to develop AI models to classify such prompts include the use of NLP based models like a language model. However, in the context of conducting an AI security investigation of prompt injects, these guardrails lack the ability to aid investigators in triaging or assessing the identified input prompts. In this applied research exploration, we explore the use of a text generation capabilities of LLM to detect prompt injects and generate explanation for its detections to aid AI security investigators in assessing and triaging of such prompt inject detections. The practical benefit of such a tool is to ease the task of conducting investigation into prompt injects.","2025-02-16","2026-01-17 20:45:30","2026-01-17 20:45:30","2026-01-17 20:45:30","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2502.11006 [cs]","Comment: 5 pages, 4 tables, 3 diagrams","/home/arthurbrito/Zotero/storage/X9GKUMQC/Pan et al. - 2025 - Prompt Inject Detection with Generative Explanation as an Investigative Tool.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2502.11006","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HTBLMPL6","preprint","2025","Wang, Wenxiao; Hosseini, Parsa; Feizi, Soheil","Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption","","","","10.48550/arXiv.2504.20769","http://arxiv.org/abs/2504.20769","Chain-of-thought prompting has demonstrated great success in facilitating the reasoning abilities of large language models. In this work, we explore how these enhanced reasoning abilities can be exploited to improve the robustness of large language models in tasks that are not necessarily reasoning-focused. In particular, we show how a wide range of large language models exhibit significantly improved robustness against reference corruption using a simple method called chain-of-defensive-thought, where only a few exemplars with structured and defensive reasoning are provided as demonstrations. Empirically, the improvements can be astounding, especially given the simplicity and applicability of the method. For example, in the Natural Questions task, the accuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting when 1 out of 10 references provided is corrupted with prompt injection attacks. In contrast, GPT-4o using chain-of-defensive-thought prompting maintains an accuracy of 50%.","2025-04-29","2026-01-17 20:45:30","2026-01-17 20:45:30","2026-01-17 20:45:30","","","","","","","Chain-of-Defensive-Thought","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2504.20769 [cs]","","/home/arthurbrito/Zotero/storage/XJDXDDUQ/Wang et al. - 2025 - Chain-of-Defensive-Thought Structured Reasoning Elicits Robustness in Large Language Models against.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2504.20769","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LZILKQDC","preprint","2025","Chen, Yulin; Li, Haoran; Sui, Yuan; Liu, Yue; He, Yufei; Song, Yangqiu; Hooi, Bryan","Robustness via Referencing: Defending against Prompt Injection Attacks by Referencing the Executed Instruction","","","","10.48550/arXiv.2504.20472","http://arxiv.org/abs/2504.20472","Large language models (LLMs) have demonstrated impressive performance and have come to dominate the field of natural language processing (NLP) across various tasks. However, due to their strong instruction-following capabilities and inability to distinguish between instructions and data content, LLMs are vulnerable to prompt injection attacks. These attacks manipulate LLMs into deviating from the original input instructions and executing maliciously injected instructions within data content, such as web documents retrieved from search engines. Existing defense methods, including prompt-engineering and fine-tuning approaches, typically instruct models to follow the original input instructions while suppressing their tendencies to execute injected instructions. However, our experiments reveal that suppressing instruction-following tendencies is challenging. Through analyzing failure cases, we observe that although LLMs tend to respond to any recognized instructions, they are aware of which specific instructions they are executing and can correctly reference them within the original prompt. Motivated by these findings, we propose a novel defense method that leverages, rather than suppresses, the instruction-following abilities of LLMs. Our approach prompts LLMs to generate responses that include both answers and their corresponding instruction references. Based on these references, we filter out answers not associated with the original input instructions. Comprehensive experiments demonstrate that our method outperforms prompt-engineering baselines and achieves performance comparable to fine-tuning methods, reducing the attack success rate (ASR) to 0 percent in some scenarios. Moreover, our approach has minimal impact on overall utility.","2025-04-29","2026-01-17 20:45:30","2026-01-17 20:45:30","2026-01-17 20:45:30","","","","","","","Robustness via Referencing","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2504.20472 [cs]","","/home/arthurbrito/Zotero/storage/UHTGMPPM/Chen et al. - 2025 - Robustness via Referencing Defending against Prompt Injection Attacks by Referencing the Executed I.pdf","","","Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2504.20472","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FJPKYU6V","preprint","2025","Li, Xitao; Wang, Haijun; Wu, Jiang; Liu, Ting","Separator Injection Attack: Uncovering Dialogue Biases in Large Language Models Caused by Role Separators","","","","10.48550/arXiv.2504.05689","http://arxiv.org/abs/2504.05689","Conversational large language models (LLMs) have gained widespread attention due to their instruction-following capabilities. To ensure conversational LLMs follow instructions, role separators are employed to distinguish between different participants in a conversation. However, incorporating role separators introduces potential vulnerabilities. Misusing roles can lead to prompt injection attacks, which can easily misalign the model's behavior with the user's intentions, raising significant security concerns. Although various prompt injection attacks have been proposed, recent research has largely overlooked the impact of role separators on safety. This highlights the critical need to thoroughly understand the systemic weaknesses in dialogue systems caused by role separators. This paper identifies modeling weaknesses caused by role separators. Specifically, we observe a strong positional bias associated with role separators, which is inherent in the format of dialogue modeling and can be triggered by the insertion of role separators. We further develop the Separators Injection Attack (SIA), a new orthometric attack based on role separators. The experiment results show that SIA is efficient and extensive in manipulating model behavior with an average gain of 18.2% for manual methods and enhances the attack success rate to 100% with automatic methods.","2025-04-08","2026-01-17 20:45:30","2026-01-17 20:45:30","2026-01-17 20:45:30","","","","","","","Separator Injection Attack","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2504.05689 [cs]","","/home/arthurbrito/Zotero/storage/JEQ6KMYB/Li et al. - 2025 - Separator Injection Attack Uncovering Dialogue Biases in Large Language Models Caused by Role Separ.pdf","","","Computer Science - Computation and Language; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2504.05689","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WPUCVZIS","preprint","2025","Pathade, Chetan","Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs","","","","10.48550/arXiv.2505.04806","http://arxiv.org/abs/2505.04806","Large Language Models (LLMs) are increasingly integrated into consumer and enterprise applications. Despite their capabilities, they remain susceptible to adversarial attacks such as prompt injection and jailbreaks that override alignment safeguards. This paper provides a systematic investigation of jailbreak strategies against various state-of-the-art LLMs. We categorize over 1,400 adversarial prompts, analyze their success against GPT-4, Claude 2, Mistral 7B, and Vicuna, and examine their generalizability and construction logic. We further propose layered mitigation strategies and recommend a hybrid red-teaming and sandboxing approach for robust LLM security.","2025-05-13","2026-01-17 20:45:30","2026-01-17 21:19:03","2026-01-17 20:45:30","","","","","","","Red Teaming the Mind of the Machine","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2505.04806 [cs]","Comment: 7 Pages, 6 Figures; Comment: 7 Pages, 6 Figures","/home/arthurbrito/Zotero/storage/WRDMBDZT/Pathade - 2025 - Red Teaming the Mind of the Machine A Systematic Evaluation of Prompt Injection and Jailbreak Vulne.pdf","","","Computer Science - Computation and Language; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2505.04806","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9GUGT2Q8","preprint","2025","Yao, Hongwei; Shi, Haoran; Chen, Yidou; Jiang, Yixin; Wang, Cong; Qin, Zhan","ControlNET: A Firewall for RAG-based LLM System","","","","10.48550/arXiv.2504.09593","http://arxiv.org/abs/2504.09593","Retrieval-Augmented Generation (RAG) has significantly enhanced the factual accuracy and domain adaptability of Large Language Models (LLMs). This advancement has enabled their widespread deployment across sensitive domains such as healthcare, finance, and enterprise applications. RAG mitigates hallucinations by integrating external knowledge, yet introduces privacy risk and security risk, notably data breaching risk and data poisoning risk. While recent studies have explored prompt injection and poisoning attacks, there remains a significant gap in comprehensive research on controlling inbound and outbound query flows to mitigate these threats. In this paper, we propose an AI firewall, ControlNET, designed to safeguard RAG-based LLM systems from these vulnerabilities. ControlNET controls query flows by leveraging activation shift phenomena to detect adversarial queries and mitigate their impact through semantic divergence. We conduct comprehensive experiments on four different benchmark datasets including Msmarco, HotpotQA, FinQA, and MedicalSys using state-of-the-art open source LLMs (Llama3, Vicuna, and Mistral). Our results demonstrate that ControlNET achieves over 0.909 AUROC in detecting and mitigating security threats while preserving system harmlessness. Overall, ControlNET offers an effective, robust, harmless defense mechanism, marking a significant advancement toward the secure deployment of RAG-based LLM systems.","2025-04-17","2026-01-17 20:45:30","2026-01-17 20:45:30","2026-01-17 20:45:30","","","","","","","ControlNET","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2504.09593 [cs]","Comment: Project Page: https://ai.zjuicsr.cn/firewall","/home/arthurbrito/Zotero/storage/SKGZ6NUZ/Yao et al. - 2025 - ControlNET A Firewall for RAG-based LLM System.pdf","","","Computer Science - Cryptography and Security; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2504.09593","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3CIBF8WV","preprint","2025","Rao, Vishisht; Kumar, Aounon; Lakkaraju, Himabindu; Shah, Nihar B.","Detecting LLM-Generated Peer Reviews","","","","10.48550/arXiv.2503.15772","http://arxiv.org/abs/2503.15772","The integrity of peer review is fundamental to scientific progress, but the rise of large language models (LLMs) has introduced concerns that some reviewers may rely on these tools to generate reviews rather than writing them independently. Although some venues have banned LLM-assisted reviewing, enforcement remains difficult as existing detection tools cannot reliably distinguish between fully generated reviews and those merely polished with AI assistance. In this work, we address the challenge of detecting LLM-generated reviews. We consider the approach of performing indirect prompt injection via the paper's PDF, prompting the LLM to embed a covert watermark in the generated review, and subsequently testing for presence of the watermark in the review. We identify and address several pitfalls in naïve implementations of this approach. Our primary contribution is a rigorous watermarking and detection framework that offers strong statistical guarantees. Specifically, we introduce watermarking schemes and hypothesis tests that control the family-wise error rate across multiple reviews, achieving higher statistical power than standard corrections such as Bonferroni, while making no assumptions about the nature of human-written reviews. We explore multiple indirect prompt injection strategies--including font-based embedding and obfuscated prompts--and evaluate their effectiveness under various reviewer defense scenarios. Our experiments find high success rates in watermark embedding across various LLMs. We also empirically find that our approach is resilient to common reviewer defenses, and that the bounds on error rates in our statistical tests hold in practice. In contrast, we find that Bonferroni-style corrections are too conservative to be useful in this setting.","2025-05-19","2026-01-17 20:45:30","2026-01-17 20:45:30","2026-01-17 20:45:30","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2503.15772 [cs]","Comment: 27 pages, 2 figures","/home/arthurbrito/Zotero/storage/3T5I3ZK8/Rao et al. - 2025 - Detecting LLM-Generated Peer Reviews.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Digital Libraries","","","","","","","","","","","","","","","","","","","arXiv:2503.15772","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XMQM495J","preprint","2024","Clop, Cody; Teglia, Yannick","Backdoored Retrievers for Prompt Injection Attacks on Retrieval Augmented Generation of Large Language Models","","","","10.48550/arXiv.2410.14479","http://arxiv.org/abs/2410.14479","Large Language Models (LLMs) have demonstrated remarkable capabilities in generating coherent text but remain limited by the static nature of their training data. Retrieval Augmented Generation (RAG) addresses this issue by combining LLMs with up-to-date information retrieval, but also expand the attack surface of the system. This paper investigates prompt injection attacks on RAG, focusing on malicious objectives beyond misinformation, such as inserting harmful links, promoting unauthorized services, and initiating denial-of-service behaviors. We build upon existing corpus poisoning techniques and propose a novel backdoor attack aimed at the fine-tuning process of the dense retriever component. Our experiments reveal that corpus poisoning can achieve significant attack success rates through the injection of a small number of compromised documents into the retriever corpus. In contrast, backdoor attacks demonstrate even higher success rates but necessitate a more complex setup, as the victim must fine-tune the retriever using the attacker poisoned dataset.","2024-10-18","2026-01-17 20:46:10","2026-01-17 20:46:10","2026-01-17 20:46:09","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2410.14479 [cs]","Comment: 12 pages, 5 figures","/home/arthurbrito/Zotero/storage/M8YHCX23/Clop e Teglia - 2024 - Backdoored Retrievers for Prompt Injection Attacks on Retrieval Augmented Generation of Large Langua.pdf","","","Computer Science - Cryptography and Security; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2410.14479","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ECAP7PI8","preprint","2024","Rahman, Md Abdur; Wu, Fan; Cuzzocrea, Alfredo; Ahamed, Sheikh Iqbal","Fine-tuned Large Language Models (LLMs): Improved Prompt Injection Attacks Detection","","","","10.48550/arXiv.2410.21337","http://arxiv.org/abs/2410.21337","Large language models (LLMs) are becoming a popular tool as they have significantly advanced in their capability to tackle a wide range of language-based tasks. However, LLMs applications are highly vulnerable to prompt injection attacks, which poses a critical problem. These attacks target LLMs applications through using carefully designed input prompts to divert the model from adhering to original instruction, thereby it could execute unintended actions. These manipulations pose serious security threats which potentially results in data leaks, biased outputs, or harmful responses. This project explores the security vulnerabilities in relation to prompt injection attacks. To detect whether a prompt is vulnerable or not, we follows two approaches: 1) a pre-trained LLM, and 2) a fine-tuned LLM. Then, we conduct a thorough analysis and comparison of the classification performance. Firstly, we use pre-trained XLM-RoBERTa model to detect prompt injections using test dataset without any fine-tuning and evaluate it by zero-shot classification. Then, this proposed work will apply supervised fine-tuning to this pre-trained LLM using a task-specific labeled dataset from deepset in huggingface, and this fine-tuned model achieves impressive results with 99.13\% accuracy, 100\% precision, 98.33\% recall and 99.15\% F1-score thorough rigorous experimentation and evaluation. We observe that our approach is highly efficient in detecting prompt injection attacks.","2024-11-07","2026-01-17 20:46:10","2026-01-17 20:46:10","2026-01-17 20:46:09","","","","","","","Fine-tuned Large Language Models (LLMs)","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2410.21337 [cs]","Comment: I am requesting the withdrawal of my paper due to critical issues identified in the methodology/results that may impact its accuracy and reliability. I also plan to make substantial revisions that go beyond minor corrections","","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2410.21337","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A5W8U7SN","preprint","2024","Ayub, Md Ahsan; Majumdar, Subhabrata","Embedding-based classifiers can detect prompt injection attacks","","","","10.48550/arXiv.2410.22284","http://arxiv.org/abs/2410.22284","Large Language Models (LLMs) are seeing significant adoption in every type of organization due to their exceptional generative capabilities. However, LLMs are found to be vulnerable to various adversarial attacks, particularly prompt injection attacks, which trick them into producing harmful or inappropriate content. Adversaries execute such attacks by crafting malicious prompts to deceive the LLMs. In this paper, we propose a novel approach based on embedding-based Machine Learning (ML) classifiers to protect LLM-based applications against this severe threat. We leverage three commonly used embedding models to generate embeddings of malicious and benign prompts and utilize ML classifiers to predict whether an input prompt is malicious. Out of several traditional ML methods, we achieve the best performance with classifiers built using Random Forest and XGBoost. Our classifiers outperform state-of-the-art prompt injection classifiers available in open-source implementations, which use encoder-only neural networks.","2024-10-29","2026-01-17 20:46:10","2026-01-17 20:46:10","2026-01-17 20:46:09","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2410.22284 [cs]","","/home/arthurbrito/Zotero/storage/XG3C8D6U/Ayub e Majumdar - 2024 - Embedding-based classifiers can detect prompt injection attacks.pdf","","","Computer Science - Cryptography and Security; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2410.22284","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V352AA5K","preprint","2024","Kokkula, Sahasra; R, Somanathan; R, Nandavardhan; Aashishkumar; Divya, G.","Palisade -- Prompt Injection Detection Framework","","","","10.48550/arXiv.2410.21146","http://arxiv.org/abs/2410.21146","The advent of Large Language Models LLMs marks a milestone in Artificial Intelligence, altering how machines comprehend and generate human language. However, LLMs are vulnerable to malicious prompt injection attacks, where crafted inputs manipulate the models behavior in unintended ways, compromising system integrity and causing incorrect outcomes. Conventional detection methods rely on static, rule-based approaches, which often fail against sophisticated threats like abnormal token sequences and alias substitutions, leading to limited adaptability and higher rates of false positives and false negatives.This paper proposes a novel NLP based approach for prompt injection detection, emphasizing accuracy and optimization through a layered input screening process. In this framework, prompts are filtered through three distinct layers rule-based, ML classifier, and companion LLM before reaching the target model, thereby minimizing the risk of malicious interaction.Tests show the ML classifier achieves the highest accuracy among individual layers, yet the multi-layer framework enhances overall detection accuracy by reducing false negatives. Although this increases false positives, it minimizes the risk of overlooking genuine injected prompts, thus prioritizing security.This multi-layered detection approach highlights LLM vulnerabilities and provides a comprehensive framework for future research, promoting secure interactions between humans and AI systems.","2024-10-28","2026-01-17 20:46:10","2026-01-17 20:46:10","2026-01-17 20:46:09","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2410.21146 [cs]","","/home/arthurbrito/Zotero/storage/AJ5QNCAQ/Kokkula et al. - 2024 - Palisade -- Prompt Injection Detection Framework.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2410.21146","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RYKRCLGF","preprint","2025","Zhang, Hanrong; Huang, Jingyuan; Mei, Kai; Yao, Yifei; Wang, Zhenting; Zhan, Chenlu; Wang, Hongwei; Zhang, Yongfeng","Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents","","","","10.48550/arXiv.2410.02644","http://arxiv.org/abs/2410.02644","Although LLM-based agents, powered by Large Language Models (LLMs), can use external tools and memory mechanisms to solve complex real-world tasks, they may also introduce critical security vulnerabilities. However, the existing literature does not comprehensively evaluate attacks and defenses against LLM-based agents. To address this, we introduce Agent Security Bench (ASB), a comprehensive framework designed to formalize, benchmark, and evaluate the attacks and defenses of LLM-based agents, including 10 scenarios (e.g., e-commerce, autonomous driving, finance), 10 agents targeting the scenarios, over 400 tools, 27 different types of attack/defense methods, and 7 evaluation metrics. Based on ASB, we benchmark 10 prompt injection attacks, a memory poisoning attack, a novel Plan-of-Thought backdoor attack, 4 mixed attacks, and 11 corresponding defenses across 13 LLM backbones. Our benchmark results reveal critical vulnerabilities in different stages of agent operation, including system prompt, user prompt handling, tool usage, and memory retrieval, with the highest average attack success rate of 84.30\%, but limited effectiveness shown in current defenses, unveiling important works to be done in terms of agent security for the community. We also introduce a new metric to evaluate the agents' capability to balance utility and security. Our code can be found at https://github.com/agiresearch/ASB.","2025-05-30","2026-01-17 20:46:10","2026-01-17 20:46:10","2026-01-17 20:46:09","","","","","","","Agent Security Bench (ASB)","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2410.02644 [cs]","Comment: Accepted by ICLR 2025","/home/arthurbrito/Zotero/storage/LN8N4EHM/Zhang et al. - 2025 - Agent Security Bench (ASB) Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2410.02644","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M4EZHXN2","preprint","2025","Zhang, Yuyang; Chen, Kangjie; Gao, Jiaxin; Cui, Ronghao; Wang, Run; Wang, Lina; Zhang, Tianwei","Towards Action Hijacking of Large Language Model-based Agent","","","","10.48550/arXiv.2412.10807","http://arxiv.org/abs/2412.10807","Recently, applications powered by Large Language Models (LLMs) have made significant strides in tackling complex tasks. By harnessing the advanced reasoning capabilities and extensive knowledge embedded in LLMs, these applications can generate detailed action plans that are subsequently executed by external tools. Furthermore, the integration of retrieval-augmented generation (RAG) enhances performance by incorporating up-to-date, domain-specific knowledge into the planning and execution processes. This approach has seen widespread adoption across various sectors, including healthcare, finance, and software development. Meanwhile, there are also growing concerns regarding the security of LLM-based applications. Researchers have disclosed various attacks, represented by jailbreak and prompt injection, to hijack the output actions of these applications. Existing attacks mainly focus on crafting semantically harmful prompts, and their validity could diminish when security filters are employed. In this paper, we introduce AI$\mathbf{^2}$, a novel attack to manipulate the action plans of LLM-based applications. Different from existing solutions, the innovation of AI$\mathbf{^2}$ lies in leveraging the knowledge from the application's database to facilitate the construction of malicious but semantically-harmless prompts. To this end, it first collects action-aware knowledge from the victim application. Based on such knowledge, the attacker can generate misleading input, which can mislead the LLM to generate harmful action plans, while bypassing possible detection mechanisms easily. Our evaluations on three real-world applications demonstrate the effectiveness of AI$\mathbf{^2}$: it achieves an average attack success rate of 84.30\% with the best of 99.70\%. Besides, it gets an average bypass rate of 92.7\% against common safety filters and 59.45\% against dedicated defense.","2025-06-12","2026-01-17 20:46:10","2026-01-17 20:46:10","2026-01-17 20:46:09","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2412.10807 [cs]","","/home/arthurbrito/Zotero/storage/7H7RAKJN/Zhang et al. - 2025 - Towards Action Hijacking of Large Language Model-based Agent.pdf","","","Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2412.10807","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VFEEUBRL","preprint","2024","Pasquini, Dario; Kornaropoulos, Evgenios M.; Ateniese, Giuseppe","Hacking Back the AI-Hacker: Prompt Injection as a Defense Against LLM-driven Cyberattacks","","","","10.48550/arXiv.2410.20911","http://arxiv.org/abs/2410.20911","Large language models (LLMs) are increasingly being harnessed to automate cyberattacks, making sophisticated exploits more accessible and scalable. In response, we propose a new defense strategy tailored to counter LLM-driven cyberattacks. We introduce Mantis, a defensive framework that exploits LLMs' susceptibility to adversarial inputs to undermine malicious operations. Upon detecting an automated cyberattack, Mantis plants carefully crafted inputs into system responses, leading the attacker's LLM to disrupt their own operations (passive defense) or even compromise the attacker's machine (active defense). By deploying purposefully vulnerable decoy services to attract the attacker and using dynamic prompt injections for the attacker's LLM, Mantis can autonomously hack back the attacker. In our experiments, Mantis consistently achieved over 95% effectiveness against automated LLM-driven attacks. To foster further research and collaboration, Mantis is available as an open-source tool: https://github.com/pasquini-dario/project_mantis","2024-11-18","2026-01-17 20:46:10","2026-01-17 21:18:21","2026-01-17 20:46:09","","","","","","","Hacking Back the AI-Hacker","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2410.20911 [cs]","Comment: v0.2 (evaluated on more agents); Comment: v0.2 (evaluated on more agents)","/home/arthurbrito/Zotero/storage/IP43KZDB/Pasquini et al. - 2024 - Hacking Back the AI-Hacker Prompt Injection as a Defense Against LLM-driven Cyberattacks.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2410.20911","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FL3IGSUL","preprint","2024","Gupta, Prannaya; Yau, Le Qi; Low, Hao Han; Lee, I.-Shiang; Lim, Hugo Maximus; Teoh, Yu Xin; Koh, Jia Hng; Liew, Dar Win; Bhardwaj, Rishabh; Bhardwaj, Rajat; Poria, Soujanya","WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models","","","","10.48550/arXiv.2408.03837","http://arxiv.org/abs/2408.03837","WalledEval is a comprehensive AI safety testing toolkit designed to evaluate large language models (LLMs). It accommodates a diverse range of models, including both open-weight and API-based ones, and features over 35 safety benchmarks covering areas such as multilingual safety, exaggerated safety, and prompt injections. The framework supports both LLM and judge benchmarking and incorporates custom mutators to test safety against various text-style mutations, such as future tense and paraphrasing. Additionally, WalledEval introduces WalledGuard, a new, small, and performant content moderation tool, and two datasets: SGXSTest and HIXSTest, which serve as benchmarks for assessing the exaggerated safety of LLMs and judges in cultural contexts. We make WalledEval publicly available at https://github.com/walledai/walledeval.","2024-08-19","2026-01-17 20:46:10","2026-01-17 20:46:10","2026-01-17 20:46:09","","","","","","","WalledEval","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2408.03837 [cs]","Comment: Under review","/home/arthurbrito/Zotero/storage/CSYVN3A5/Gupta et al. - 2024 - WalledEval A Comprehensive Safety Evaluation Toolkit for Large Language Models.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2408.03837","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2KUC6Q49","preprint","2024","Zhou, Leon; Yang, Junfeng; Mao, Chengzhi","SPIN: Self-Supervised Prompt INjection","","","","10.48550/arXiv.2410.13236","http://arxiv.org/abs/2410.13236","Large Language Models (LLMs) are increasingly used in a variety of important applications, yet their safety and reliability remain as major concerns. Various adversarial and jailbreak attacks have been proposed to bypass the safety alignment and cause the model to produce harmful responses. We introduce Self-supervised Prompt INjection (SPIN) which can detect and reverse these various attacks on LLMs. As our self-supervised prompt defense is done at inference-time, it is also compatible with existing alignment and adds an additional layer of safety for defense. Our benchmarks demonstrate that our system can reduce the attack success rate by up to 87.9%, while maintaining the performance on benign user requests. In addition, we discuss the situation of an adaptive attacker and show that our method is still resilient against attackers who are aware of our defense.","2024-10-17","2026-01-17 20:46:10","2026-01-17 20:46:10","2026-01-17 20:46:09","","","","","","","SPIN","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2410.13236 [cs]","","/home/arthurbrito/Zotero/storage/5WWU4XUM/Zhou et al. - 2024 - SPIN Self-Supervised Prompt INjection.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2410.13236","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZF8EGZYD","conferencePaper","2025","Chen, Sizhe; Zharmagambetov, Arman; Mahloujifar, Saeed; Chaudhuri, Kamalika; Wagner, David; Guo, Chuan","SecAlign: Defending Against Prompt Injection with Preference Optimization","Proceedings of the 2025 ACM SIGSAC Conference on Computer and Communications Security","","","10.1145/3719027.3744836","http://arxiv.org/abs/2410.05451","Large language models (LLMs) are becoming increasingly prevalent in modern software systems, interfacing between the user and the Internet to assist with tasks that require advanced language understanding. To accomplish these tasks, the LLM often uses external data sources such as user documents, web retrieval, results from API calls, etc. This opens up new avenues for attackers to manipulate the LLM via prompt injection. Adversarial prompts can be injected into external data sources to override the system's intended instruction and instead execute a malicious instruction. To mitigate this vulnerability, we propose a new defense called SecAlign based on the technique of preference optimization. Our defense first constructs a preference dataset with prompt-injected inputs, secure outputs (ones that respond to the legitimate instruction), and insecure outputs (ones that respond to the injection). We then perform preference optimization on this dataset to teach the LLM to prefer the secure output over the insecure one. This provides the first known method that reduces the success rates of various prompt injections to <10%, even against attacks much more sophisticated than ones seen during training. This indicates our defense generalizes well against unknown and yet-to-come attacks. Also, SecAlign models are still practical with similar utility to the one before defensive training in our evaluations. Our code is at https://github.com/facebookresearch/SecAlign","2025-11-19","2026-01-17 20:46:10","2026-01-17 21:19:16","2026-01-17 20:46:10","2833-2847","","","","","","SecAlign","","","","","","","","","","","","arXiv.org","","arXiv:2410.05451 [cs]","Comment: ACM CCS 2025. Key words: prompt injection defense, LLM security, LLM-integrated applications","/home/arthurbrito/Zotero/storage/KNZLMPXQ/Chen et al. - 2025 - SecAlign Defending Against Prompt Injection with Preference Optimization.pdf","","","Computer Science - Cryptography and Security; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WJV8HPS6","preprint","2025","Liu, Yi; Deng, Gelei; Li, Yuekang; Wang, Kailong; Wang, Zihao; Wang, Xiaofeng; Zhang, Tianwei; Liu, Yepang; Wang, Haoyu; Zheng, Yan; Zhang, Leo Yu; Liu, Yang","Prompt Injection attack against LLM-integrated Applications","","","","10.48550/arXiv.2306.05499","http://arxiv.org/abs/2306.05499","Large Language Models (LLMs), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual LLM-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate HouYi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. HouYi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging HouYi, we unveil previously unknown and severe attack outcomes, such as unrestricted arbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection. 10 vendors have validated our discoveries, including Notion, which has the potential to impact millions of users. Our investigation illuminates both the possible risks of prompt injection attacks and the possible tactics for mitigation.","2025-12-29","2026-01-17 20:48:40","2026-01-17 20:48:40","2026-01-17 20:48:25","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2306.05499 [cs]","","/home/arthurbrito/Zotero/storage/A6VQG6BP/Liu et al. - 2025 - Prompt Injection attack against LLM-integrated Applications.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Cryptography and Security; Computer Science - Software Engineering","","","","","","","","","","","","","","","","","","","arXiv:2306.05499","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L2CAX6LH","conferencePaper","2023","Greshake, Kai; Abdelnabi, Sahar; Mishra, Shailesh; Endres, Christoph; Holz, Thorsten; Fritz, Mario","Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection","Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security","979-8-4007-0260-0","","10.1145/3605764.3623985","https://dl.acm.org/doi/10.1145/3605764.3623985","","2023-11-30","2026-01-17 20:48:40","2026-01-17 20:48:40","2026-01-17 20:48:26","79-90","","","","","","Not What You've Signed Up For","","","","","ACM","Copenhagen Denmark","en","","","","","DOI.org (Crossref)","","","","/home/arthurbrito/Zotero/storage/NQUX9AHS/Greshake et al. - 2023 - Not What You've Signed Up For Compromising Real-World LLM-Integrated Applications with Indirect Pro.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","CCS '23: ACM SIGSAC Conference on Computer and Communications Security","","","","","","","","","","","","","","",""
"XMZEBKYI","journalArticle","2024","Debenedetti, Edoardo; Zhang, Jie; Balunovic, Mislav; Beurer-Kellner, Luca; Fischer, Marc; Tramèr, Florian","Agentdojo: A dynamic environment to evaluate prompt injection attacks and defenses for llm agents","Advances in Neural Information Processing Systems","","","","https://proceedings.neurips.cc/paper_files/paper/2024/hash/97091a5177d8dc64b1da8bf3e1f6fb54-Abstract-Datasets_and_Benchmarks_Track.html","","2024","2026-01-17 20:48:40","2026-01-17 20:48:40","2026-01-17 20:48:29","82895–82920","","","37","","","Agentdojo","","","","","","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/FLJAEYKQ/Debenedetti et al. - 2024 - Agentdojo A dynamic environment to evaluate prompt injection attacks and defenses for llm agents.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AD7898QA","conferencePaper","2024","Shi, Jiawen; Yuan, Zenghui; Liu, Yinuo; Huang, Yue; Zhou, Pan; Sun, Lichao; Gong, Neil Zhenqiang","Optimization-based Prompt Injection Attack to LLM-as-a-Judge","Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security","979-8-4007-0636-3","","10.1145/3658644.3690291","https://dl.acm.org/doi/10.1145/3658644.3690291","","2024-12-02","2026-01-17 20:48:40","2026-01-17 20:48:40","2026-01-17 20:48:32","660-674","","","","","","","","","","","ACM","Salt Lake City UT USA","en","","","","","DOI.org (Crossref)","","","","/home/arthurbrito/Zotero/storage/LJJN8YVD/Shi et al. - 2024 - Optimization-based Prompt Injection Attack to LLM-as-a-Judge.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","CCS '24: ACM SIGSAC Conference on Computer and Communications Security","","","","","","","","","","","","","","",""
"HH25SQAE","conferencePaper","2025","Hackett, William; Birch, Lewis; Trawicki, Stefan; Suri, Neeraj; Garraghan, Peter","Bypassing LLM guardrails: An empirical analysis of evasion attacks against prompt injection and jailbreak detection systems","Proceedings of the The First Workshop on LLM Security (LLMSEC)","","","","https://aclanthology.org/2025.llmsec-1.8/","","2025","2026-01-17 20:48:40","2026-01-17 20:48:40","2026-01-17 20:48:32","101–114","","","","","","Bypassing LLM guardrails","","","","","","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/2GBAWAVM/Hackett et al. - 2025 - Bypassing LLM guardrails An empirical analysis of evasion attacks against prompt injection and jail.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JWDR8CNW","preprint","2025","Chen, Sizhe; Zharmagambetov, Arman; Wagner, David; Guo, Chuan","Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks","","","","10.48550/arXiv.2507.02735","http://arxiv.org/abs/2507.02735","Prompt injection attack has been listed as the top-1 security threat to LLM-integrated applications, which interact with external environment data for complex tasks. The untrusted data may contain an injected prompt trying to arbitrarily manipulate the system. Model-level prompt injection defenses have shown strong effectiveness, but are currently deployed into commercial-grade models in a closed-source manner. We believe open-source secure models are needed by the AI security community, where co-development of attacks and defenses through open research drives scientific progress in mitigating prompt injection attacks. To this end, we develop Meta SecAlign, the first fully open-source LLM with built-in model-level defense that achieves commercial-grade performance, powerful enough for complex agentic tasks. We provide complete details of our training recipe, an improved version of the SOTA SecAlign defense. We perform the most comprehensive evaluation to date on 9 utility benchmarks and 7 security benchmarks on general knowledge, instruction following, and agentic workflows. Results show that Meta SecAlign, despite being trained on generic instruction-tuning samples only, surprisingly confers security in unseen downstream tasks, including tool-calling and web-navigation, in addition to general instruction-following. Our best model -- Meta-SecAlign-70B -- establishes a new frontier of utility-security trade-off for open-source LLMs. Even compared to closed-course commercial models such as GPT-5, our model is much securer than most of them. Below are links for the code (https://github.com/facebookresearch/Meta_SecAlign), Meta-SecAlign-70B(https://huggingface.co/facebook/Meta-SecAlign-70B), and Meta-SecAlign-8B(https://huggingface.co/facebook/Meta-SecAlign-8B) models.","2025-11-10","2026-01-17 20:48:40","2026-01-17 20:48:40","2026-01-17 20:48:34","","","","","","","Meta SecAlign","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2507.02735 [cs]","","/home/arthurbrito/Zotero/storage/DSFTNI86/Chen et al. - 2025 - Meta SecAlign A Secure Foundation LLM Against Prompt Injection Attacks.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2507.02735","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CB3PREL7","journalArticle","2024","Mathew, Eleena","Enhancing security in large language models: A comprehensive review of prompt injection attacks and defenses","Authorea Preprints","","","","https://www.techrxiv.org/doi/full/10.36227/techrxiv.172954263.32914470","","2024","2026-01-17 20:48:40","2026-01-17 20:48:40","2026-01-17 20:48:34","","","","","","","Enhancing security in large language models","","","","","Authorea","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/8N6IVFDN/Mathew - 2024 - Enhancing security in large language models A comprehensive review of prompt injection attacks and.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7KEUJTMJ","preprint","2025","Li, Yucheng; Ahn, Surin; Jiang, Huiqiang; Abdi, Amir H.; Yang, Yuqing; Qiu, Lili","SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression","","","","10.48550/arXiv.2506.12707","http://arxiv.org/abs/2506.12707","Large language models (LLMs) have achieved widespread adoption across numerous applications. However, many LLMs are vulnerable to malicious attacks even after safety alignment. These attacks typically bypass LLMs' safety guardrails by wrapping the original malicious instructions inside adversarial jailbreaks prompts. Previous research has proposed methods such as adversarial training and prompt rephrasing to mitigate these safety vulnerabilities, but these methods often reduce the utility of LLMs or lead to significant computational overhead and online latency. In this paper, we propose SecurityLingua, an effective and efficient approach to defend LLMs against jailbreak attacks via security-oriented prompt compression. Specifically, we train a prompt compressor designed to discern the ""true intention"" of the input prompt, with a particular focus on detecting the malicious intentions of adversarial prompts. Then, in addition to the original prompt, the intention is passed via the system prompt to the target LLM to help it identify the true intention of the request. SecurityLingua ensures a consistent user experience by leaving the original input prompt intact while revealing the user's potentially malicious intention and stimulating the built-in safety guardrails of the LLM. Moreover, thanks to prompt compression, SecurityLingua incurs only a negligible overhead and extra token cost compared to all existing defense methods, making it an especially practical solution for LLM defense. Experimental results demonstrate that SecurityLingua can effectively defend against malicious attacks and maintain utility of the LLM with negligible compute and latency overhead. Our code is available at https://aka.ms/SecurityLingua.","2025-06-15","2026-01-17 20:48:40","2026-01-17 20:48:40","2026-01-17 20:48:38","","","","","","","SecurityLingua","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2506.12707 [cs]","","/home/arthurbrito/Zotero/storage/D5GM57VU/Li et al. - 2025 - SecurityLingua Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression.pdf","","","Computer Science - Computation and Language; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2506.12707","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TCWY5PG9","preprint","2024","Lin, Zhihao; Ma, Wei; Zhou, Mingyi; Zhao, Yanjie; Wang, Haoyu; Liu, Yang; Wang, Jun; Li, Li","PathSeeker: Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based Jailbreak Approach","","","","10.48550/arXiv.2409.14177","http://arxiv.org/abs/2409.14177","In recent years, Large Language Models (LLMs) have gained widespread use, raising concerns about their security. Traditional jailbreak attacks, which often rely on the model internal information or have limitations when exploring the unsafe behavior of the victim model, limiting their reducing their general applicability. In this paper, we introduce PathSeeker, a novel black-box jailbreak method, which is inspired by the game of rats escaping a maze. We think that each LLM has its unique ""security maze"", and attackers attempt to find the exit learning from the received feedback and their accumulated experience to compromise the target LLM's security defences. Our approach leverages multi-agent reinforcement learning, where smaller models collaborate to guide the main LLM in performing mutation operations to achieve the attack objectives. By progressively modifying inputs based on the model's feedback, our system induces richer, harmful responses. During our manual attempts to perform jailbreak attacks, we found that the vocabulary of the response of the target model gradually became richer and eventually produced harmful responses. Based on the observation, we also introduce a reward mechanism that exploits the expansion of vocabulary richness in LLM responses to weaken security constraints. Our method outperforms five state-of-the-art attack techniques when tested across 13 commercial and open-source LLMs, achieving high attack success rates, especially in strongly aligned commercial models like GPT-4o-mini, Claude-3.5, and GLM-4-air with strong safety alignment. This study aims to improve the understanding of LLM security vulnerabilities and we hope that this sturdy can contribute to the development of more robust defenses.","2024-10-03","2026-01-17 20:48:40","2026-01-17 20:48:40","2026-01-17 20:48:40","","","","","","","PathSeeker","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2409.14177 [cs]","Comment: update the abstract and cite a new related work","/home/arthurbrito/Zotero/storage/IX6W398Z/Lin et al. - 2024 - PathSeeker Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based Jailbreak App.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2409.14177","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RXWAIER4","journalArticle","2025","Naik, Dishita; Naik, Ishita; Naik, Nitin","When Generative AI Prompts Bite Back: Investigating Different Types of Prompt Injection Attacks on Large Language Models (LLMs) and Their Prevention Methods","Authorea Preprints","","","","https://www.techrxiv.org/doi/full/10.36227/techrxiv.176551675.52333626","","2025","2026-01-17 20:48:40","2026-01-17 20:48:40","2026-01-17 20:48:40","","","","","","","When Generative AI Prompts Bite Back","","","","","Authorea","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/PLMGK7RI/Naik et al. - 2025 - When Generative AI Prompts Bite Back Investigating Different Types of Prompt Injection Attacks on L.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EMCE69TD","conferencePaper","2025","Chen, Sizhe; Wang, Yizhu; Carlini, Nicholas; Sitawarin, Chawin; Wagner, David","Defending Against Prompt Injection With a Few DefensiveTokens","Proceedings of the 18th ACM Workshop on Artificial Intelligence and Security","979-8-4007-1895-3","","10.1145/3733799.3762982","https://dl.acm.org/doi/10.1145/3733799.3762982","","2025-10-13","2026-01-17 20:50:17","2026-01-17 20:50:17","2026-01-17 20:49:59","242-252","","","","","","","","","","","ACM","Taipei , Taiwan","en","","","","","DOI.org (Crossref)","","","","/home/arthurbrito/Zotero/storage/6M2WDT5A/Chen et al. - 2025 - Defending Against Prompt Injection With a Few DefensiveTokens.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AISec '25: Proceedings of the 2025 Workshop on Artificial Intelligence and Security","","","","","","","","","","","","","","",""
"IPW3ZJUF","conferencePaper","2025","Joseph, Jefferson Kanjirakkattu; Daniel, Esther; Kathiresan, V.; MAP, Manimegalai","Prompt Injection in Large Language Model Exploitation: A Security Perspective","2025 International Conference on Electronics, Computing, Communication and Control Technology (ICECCC)","","","","https://ieeexplore.ieee.org/abstract/document/11064209/","","2025","2026-01-17 20:50:17","2026-01-17 20:50:17","2026-01-17 20:50:06","1–8","","","","","","Prompt Injection in Large Language Model Exploitation","","","","","IEEE","","","","","","","Google Scholar","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TEVUHB4S","preprint","2025","Hackett, William; Birch, Lewis; Trawicki, Stefan; Suri, Neeraj; Garraghan, Peter","Bypassing LLM Guardrails: An Empirical Analysis of Evasion Attacks against Prompt Injection and Jailbreak Detection Systems","","","","10.48550/arXiv.2504.11168","http://arxiv.org/abs/2504.11168","Large Language Models (LLMs) guardrail systems are designed to protect against prompt injection and jailbreak attacks. However, they remain vulnerable to evasion techniques. We demonstrate two approaches for bypassing LLM prompt injection and jailbreak detection systems via traditional character injection methods and algorithmic Adversarial Machine Learning (AML) evasion techniques. Through testing against six prominent protection systems, including Microsoft's Azure Prompt Shield and Meta's Prompt Guard, we show that both methods can be used to evade detection while maintaining adversarial utility achieving in some instances up to 100% evasion success. Furthermore, we demonstrate that adversaries can enhance Attack Success Rates (ASR) against black-box targets by leveraging word importance ranking computed by offline white-box models. Our findings reveal vulnerabilities within current LLM protection mechanisms and highlight the need for more robust guardrail systems.","2025-07-14","2026-01-17 20:50:17","2026-01-17 20:50:17","2026-01-17 20:50:07","","","","","","","Bypassing LLM Guardrails","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2504.11168 [cs]","Comment: 14 pages, 5 figures, 11 tables. To be published in LLMSec 2025","/home/arthurbrito/Zotero/storage/D8DRSU3V/Hackett et al. - 2025 - Bypassing LLM Guardrails An Empirical Analysis of Evasion Attacks against Prompt Injection and Jail.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2504.11168","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3XPBLP5W","journalArticle","2025","Gulyamov, Saidakhror; Gulyamov, Said; Rodionov, Andrey; Khursanov, Rustam; Mekhmonov, Kambariddin; Babaev, Djakhongir; Rakhimjonov, Akmaljon","Prompt Injection Attacks in Large Language Models and AI Agent Systems: A Comprehensive Review of Vulnerabilities, Attack Vectors, and Defense Mechanisms","","","","","https://www.preprints.org/manuscript/202511.0088","","2025","2026-01-17 20:50:17","2026-01-17 20:50:17","2026-01-17 20:50:07","","","","","","","Prompt Injection Attacks in Large Language Models and AI Agent Systems","","","","","Preprints","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/LXJU2UCK/Gulyamov et al. - 2025 - Prompt Injection Attacks in Large Language Models and AI Agent Systems A Comprehensive Review of Vu.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7U8TZG76","journalArticle","2024","Panterino, Samuel; Fellington, Matthew","Dynamic moving target defense for mitigating targeted llm prompt injection","Authorea Preprints","","","","https://www.techrxiv.org/doi/full/10.36227/techrxiv.171822345.56781952","","2024","2026-01-17 20:50:17","2026-01-17 20:50:17","2026-01-17 20:50:10","","","","","","","","","","","","Authorea","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/9JMYEN4G/Panterino e Fellington - 2024 - Dynamic moving target defense for mitigating targeted llm prompt injection.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XHZWR8GS","bookSection","2025","Khomsky, Daniil; Maloyan, Narek; Nutfullin, Bulat","Prompt Injection Attacks in Defended Systems","Distributed Computer and Communication Networks","978-3-031-80852-4 978-3-031-80853-1","","10.1007/978-3-031-80853-1_30","https://link.springer.com/10.1007/978-3-031-80853-1_30","","2025","2026-01-17 20:50:17","2026-01-17 20:50:17","2026-01-17 20:50:11","404-416","","","15460","","","","","","","","Springer Nature Switzerland","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-031-80853-1_30","","/home/arthurbrito/Zotero/storage/NPRVK9BF/Khomsky et al. - 2025 - Prompt Injection Attacks in Defended Systems.pdf","","","","Vishnevsky, Vladimir M.; Samouylov, Konstantin E.; Kozyrev, Dmitry V.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4N8R74S5","preprint","2025","Yeo, Andrew; Choi, Daeseon","Multimodal Prompt Injection Attacks: Risks and Defenses for Modern LLMs","","","","10.48550/arXiv.2509.05883","http://arxiv.org/abs/2509.05883","Large Language Models (LLMs) have seen rapid adoption in recent years, with industries increasingly relying on them to maintain a competitive advantage. These models excel at interpreting user instructions and generating human-like responses, leading to their integration across diverse domains, including consulting and information retrieval. However, their widespread deployment also introduces substantial security risks, most notably in the form of prompt injection and jailbreak attacks. To systematically evaluate LLM vulnerabilities -- particularly to external prompt injection -- we conducted a series of experiments on eight commercial models. Each model was tested without supplementary sanitization, relying solely on its built-in safeguards. The results exposed exploitable weaknesses and emphasized the need for stronger security measures. Four categories of attacks were examined: direct injection, indirect (external) injection, image-based injection, and prompt leakage. Comparative analysis indicated that Claude 3 demonstrated relatively greater robustness; nevertheless, empirical findings confirm that additional defenses, such as input normalization, remain necessary to achieve reliable protection.","2025-09-07","2026-01-17 20:50:17","2026-01-17 20:50:17","2026-01-17 20:50:13","","","","","","","Multimodal Prompt Injection Attacks","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2509.05883 [cs]","Comment: 8 pages, 4 figures, 2 tables","/home/arthurbrito/Zotero/storage/SPP225P3/Yeo e Choi - 2025 - Multimodal Prompt Injection Attacks Risks and Defenses for Modern LLMs.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2509.05883","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SQECJWS7","journalArticle","2025","Liao, Zhiyu; Chen, Kang; Lin, Yuanguo; Li, Kangkang; Liu, Yunxuan; Chen, Hefeng; Huang, Xingwang; Yu, Yuanhui","Attack and defense techniques in large language models: A survey and new perspectives","Neural Networks","","","","https://www.sciencedirect.com/science/article/pii/S0893608025012699","","2025","2026-01-17 20:50:17","2026-01-17 20:50:17","2026-01-17 20:50:13","108388","","","","","","Attack and defense techniques in large language models","","","","","Elsevier","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/ZLFS6X4Z/Liao et al. - 2025 - Attack and defense techniques in large language models A survey and new perspectives.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"34BGLBS9","preprint","2024","Yi, Sibo; Liu, Yule; Sun, Zhen; Cong, Tianshuo; He, Xinlei; Song, Jiaxing; Xu, Ke; Li, Qi","Jailbreak Attacks and Defenses Against Large Language Models: A Survey","","","","10.48550/arXiv.2407.04295","http://arxiv.org/abs/2407.04295","Large Language Models (LLMs) have performed exceptionally in various text-generative tasks, including question answering, translation, code completion, etc. However, the over-assistance of LLMs has raised the challenge of ""jailbreaking"", which induces the model to generate malicious responses against the usage policy and society by designing adversarial prompts. With the emergence of jailbreak attack methods exploiting different vulnerabilities in LLMs, the corresponding safety alignment measures are also evolving. In this paper, we propose a comprehensive and detailed taxonomy of jailbreak attack and defense methods. For instance, the attack methods are divided into black-box and white-box attacks based on the transparency of the target model. Meanwhile, we classify defense methods into prompt-level and model-level defenses. Additionally, we further subdivide these attack and defense methods into distinct sub-classes and present a coherent diagram illustrating their relationships. We also conduct an investigation into the current evaluation methods and compare them from different perspectives. Our findings aim to inspire future research and practical implementations in safeguarding LLMs against adversarial attacks. Above all, although jailbreak remains a significant concern within the community, we believe that our work enhances the understanding of this domain and provides a foundation for developing more secure LLMs.","2024-08-30","2026-01-17 20:50:17","2026-01-17 20:50:17","2026-01-17 20:50:15","","","","","","","Jailbreak Attacks and Defenses Against Large Language Models","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2407.04295 [cs]","","/home/arthurbrito/Zotero/storage/AR2DDA5D/Yi et al. - 2024 - Jailbreak Attacks and Defenses Against Large Language Models A Survey.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Cryptography and Security; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2407.04295","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GLG5H4MZ","preprint","2024","Xu, Zihao; Liu, Yi; Deng, Gelei; Li, Yuekang; Picek, Stjepan","A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models","","","","10.48550/arXiv.2402.13457","http://arxiv.org/abs/2402.13457","Large Language Models (LLMS) have increasingly become central to generating content with potential societal impacts. Notably, these models have demonstrated capabilities for generating content that could be deemed harmful. To mitigate these risks, researchers have adopted safety training techniques to align model outputs with societal values to curb the generation of malicious content. However, the phenomenon of ""jailbreaking"", where carefully crafted prompts elicit harmful responses from models, persists as a significant challenge. This research conducts a comprehensive analysis of existing studies on jailbreaking LLMs and their defense techniques. We meticulously investigate nine attack techniques and seven defense techniques applied across three distinct language models: Vicuna, LLama, and GPT-3.5 Turbo. We aim to evaluate the effectiveness of these attack and defense techniques. Our findings reveal that existing white-box attacks underperform compared to universal techniques and that including special tokens in the input significantly affects the likelihood of successful attacks. This research highlights the need to concentrate on the security facets of LLMs. Additionally, we contribute to the field by releasing our datasets and testing framework, aiming to foster further research into LLM security. We believe these contributions will facilitate the exploration of security measures within this domain.","2024-05-17","2026-01-17 20:50:17","2026-01-17 20:50:17","2026-01-17 20:50:17","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2402.13457 [cs]","Comment: 18 pages, 9 figures, Accepted in ACL 2024","/home/arthurbrito/Zotero/storage/3N9BHAVF/Xu et al. - 2024 - A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2402.13457","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3SKW4MF2","preprint","2025","Zhan, Qiusi; Fang, Richard; Panchal, Henil Shalin; Kang, Daniel","Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents","","","","10.48550/arXiv.2503.00061","http://arxiv.org/abs/2503.00061","Large Language Model (LLM) agents exhibit remarkable performance across diverse applications by using external tools to interact with environments. However, integrating external tools introduces security risks, such as indirect prompt injection (IPI) attacks. Despite defenses designed for IPI attacks, their robustness remains questionable due to insufficient testing against adaptive attacks. In this paper, we evaluate eight different defenses and bypass all of them using adaptive attacks, consistently achieving an attack success rate of over 50%. This reveals critical vulnerabilities in current defenses. Our research underscores the need for adaptive attack evaluation when designing defenses to ensure robustness and reliability. The code is available at https://github.com/uiuc-kang-lab/AdaptiveAttackAgent.","2025-03-04","2026-01-17 20:51:31","2026-01-17 20:51:31","2026-01-17 20:51:12","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2503.00061 [cs]","Comment: 17 pages, 5 figures, 6 tables (NAACL 2025 Findings)","/home/arthurbrito/Zotero/storage/TZBTLDZL/Zhan et al. - 2025 - Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents.pdf","","","Computer Science - Cryptography and Security; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2503.00061","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IMIZVFZV","document","2025","ŞAŞAL, A.; Can, Özgü","Prompt Injection Attacks on Large Language Models: Multi-Model Security Analysis with Categorized Attack Types","","","","","https://www.scitepress.org/Papers/2025/138384/138384.pdf","","2025","2026-01-17 20:51:31","2026-01-17 20:51:31","2026-01-17 20:51:14","","","","","","","Prompt Injection Attacks on Large Language Models","","","","","ICAART","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/NHLKSLXA/ŞAŞAL e Can - 2025 - Prompt Injection Attacks on Large Language Models Multi-Model Security Analysis with Categorized At.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SDDFY78Q","conferencePaper","2024","Suo, Xuchen","Signed-prompt: A new approach to prevent prompt injection attacks against llm-integrated applications","AIP Conference Proceedings","","","","https://pubs.aip.org/aip/acp/article-abstract/3194/1/040013/3325235","","2024","2026-01-17 20:51:31","2026-01-17 20:51:31","2026-01-17 20:51:15","040013","","1","3194","","","Signed-prompt","","","","","AIP Publishing LLC","","","","","","","Google Scholar","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C82PR83P","preprint","2025","Peng, Benji; Chen, Keyu; Li, Ming; Feng, Pohsun; Bi, Ziqian; Liu, Junyu; Song, Xinyuan; Niu, Qian","Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks","","","","10.48550/arXiv.2409.08087","http://arxiv.org/abs/2409.08087","Large Language Models (LLMs) demonstrate impressive capabilities across various fields, yet their increasing use raises critical security concerns. This article reviews recent literature addressing key issues in LLM security, with a focus on accuracy, bias, content detection, and vulnerability to attacks. Issues related to inaccurate or misleading outputs from LLMs is discussed, with emphasis on the implementation from fact-checking methodologies to enhance response reliability. Inherent biases within LLMs are critically examined through diverse evaluation techniques, including controlled input studies and red teaming exercises. A comprehensive analysis of bias mitigation strategies is presented, including approaches from pre-processing interventions to in-training adjustments and post-processing refinements. The article also probes the complexity of distinguishing LLM-generated content from human-produced text, introducing detection mechanisms like DetectGPT and watermarking techniques while noting the limitations of machine learning enabled classifiers under intricate circumstances. Moreover, LLM vulnerabilities, including jailbreak attacks and prompt injection exploits, are analyzed by looking into different case studies and large-scale competitions like HackAPrompt. This review is concluded by retrospecting defense mechanisms to safeguard LLMs, accentuating the need for more extensive research into the LLM security field.","2025-11-25","2026-01-17 20:51:31","2026-01-17 20:51:31","2026-01-17 20:51:15","","","","","","","Securing Large Language Models","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2409.08087 [cs]","Comment: 17 pages, 1 figure","/home/arthurbrito/Zotero/storage/SBSP8WFQ/Peng et al. - 2025 - Securing Large Language Models Addressing Bias, Misinformation, and Prompt Attacks.pdf","","","Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2409.08087","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FIULFZQA","conferencePaper","2023","Lee, Dylan; Xie, Shaoyuan; Rahman, Shagoto; Pat, Kenneth; Lee, David; Chen, Qi Alfred","""Prompter Says"": A Linguistic Approach to Understanding and Detecting Jailbreak Attacks Against Large-Language Models","Proceedings of the 1st ACM Workshop on Large AI Systems and Models with Privacy and Safety Analysis","979-8-4007-1209-8","","10.1145/3689217.3690618","https://dl.acm.org/doi/10.1145/3689217.3690618","","2023-11-19","2026-01-17 20:51:31","2026-01-17 20:51:31","2026-01-17 20:51:17","77-87","","","","","","""Prompter Says""","","","","","ACM","Salt Lake City UT USA","en","","","","","DOI.org (Crossref)","","","","/home/arthurbrito/Zotero/storage/TGQQA74A/Lee et al. - 2023 - Prompter Says A Linguistic Approach to Understanding and Detecting Jailbreak Attacks Against Larg.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","CCS '24: ACM SIGSAC Conference on Computer and Communications Security","","","","","","","","","","","","","","",""
"NCSDGZ76","preprint","2024","Yu, Miao; Fang, Junfeng; Zhou, Yingjie; Fan, Xing; Wang, Kun; Pan, Shirui; Wen, Qingsong","LLM-Virus: Evolutionary Jailbreak Attack on Large Language Models","","","","10.48550/arXiv.2501.00055","http://arxiv.org/abs/2501.00055","While safety-aligned large language models (LLMs) are increasingly used as the cornerstone for powerful systems such as multi-agent frameworks to solve complex real-world problems, they still suffer from potential adversarial queries, such as jailbreak attacks, which attempt to induce harmful content. Researching attack methods allows us to better understand the limitations of LLM and make trade-offs between helpfulness and safety. However, existing jailbreak attacks are primarily based on opaque optimization techniques (e.g. token-level gradient descent) and heuristic search methods like LLM refinement, which fall short in terms of transparency, transferability, and computational cost. In light of these limitations, we draw inspiration from the evolution and infection processes of biological viruses and propose LLM-Virus, a jailbreak attack method based on evolutionary algorithm, termed evolutionary jailbreak. LLM-Virus treats jailbreak attacks as both an evolutionary and transfer learning problem, utilizing LLMs as heuristic evolutionary operators to ensure high attack efficiency, transferability, and low time cost. Our experimental results on multiple safety benchmarks show that LLM-Virus achieves competitive or even superior performance compared to existing attack methods.","2024-12-28","2026-01-17 20:51:31","2026-01-17 20:51:31","2026-01-17 20:51:19","","","","","","","LLM-Virus","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2501.00055 [cs]","","/home/arthurbrito/Zotero/storage/GPFJ4574/Yu et al. - 2024 - LLM-Virus Evolutionary Jailbreak Attack on Large Language Models.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2501.00055","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VT3A42B8","preprint","2024","Lee, Donghyun; Tiwari, Mo","Prompt Infection: LLM-to-LLM Prompt Injection within Multi-Agent Systems","","","","10.48550/arXiv.2410.07283","http://arxiv.org/abs/2410.07283","As Large Language Models (LLMs) grow increasingly powerful, multi-agent systems are becoming more prevalent in modern AI applications. Most safety research, however, has focused on vulnerabilities in single-agent LLMs. These include prompt injection attacks, where malicious prompts embedded in external content trick the LLM into executing unintended or harmful actions, compromising the victim's application. In this paper, we reveal a more dangerous vector: LLM-to-LLM prompt injection within multi-agent systems. We introduce Prompt Infection, a novel attack where malicious prompts self-replicate across interconnected agents, behaving much like a computer virus. This attack poses severe threats, including data theft, scams, misinformation, and system-wide disruption, all while propagating silently through the system. Our extensive experiments demonstrate that multi-agent systems are highly susceptible, even when agents do not publicly share all communications. To address this, we propose LLM Tagging, a defense mechanism that, when combined with existing safeguards, significantly mitigates infection spread. This work underscores the urgent need for advanced security measures as multi-agent LLM systems become more widely adopted.","2024-10-09","2026-01-17 20:51:31","2026-01-17 20:51:31","2026-01-17 20:51:23","","","","","","","Prompt Infection","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2410.07283 [cs]","","/home/arthurbrito/Zotero/storage/WF7GJI3Z/Lee e Tiwari - 2024 - Prompt Infection LLM-to-LLM Prompt Injection within Multi-Agent Systems.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Multiagent Systems","","","","","","","","","","","","","","","","","","","arXiv:2410.07283","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SDW4BKCZ","journalArticle","2025","Bayhan, Fatih","PROMPT INJECTION ATTACKS ON LARGE LANGUAGE MODELS: A SYSTEMATIC LITERATURE REVIEW","","","","","https://open.metu.edu.tr/handle/11511/115034","","2025","2026-01-17 20:51:31","2026-01-17 20:51:31","2026-01-17 20:51:25","","","","","","","PROMPT INJECTION ATTACKS ON LARGE LANGUAGE MODELS","","","","","Middle East Technical University","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/79X3G6TG/Bayhan - 2025 - PROMPT INJECTION ATTACKS ON LARGE LANGUAGE MODELS A SYSTEMATIC LITERATURE REVIEW.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TLUKWQUV","conferencePaper","2024","Muliarevych, Oleksandr","Enhancing System Security: LLM-Driven Defense Against Prompt Injection Vulnerabilities","2024 IEEE 17th International Conference on Advanced Trends in Radioelectronics, Telecommunications and Computer Engineering (TCSET)","","","","https://ieeexplore.ieee.org/abstract/document/10755823/","","2024","2026-01-17 20:51:31","2026-01-17 21:18:04","2026-01-17 20:51:29","420–423","","","","","","Enhancing System Security","","","","","IEEE","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/ULNHZ8AT/Muliarevych - 2024 - Enhancing System Security LLM-Driven Defense Against Prompt Injection Vulnerabilities.pdf","","","Computer security; ChatGPT; Cyberattack; cybersecurity for LLMs; Distance measurement; Fault tolerance; Focusing; language model; Market research; prompt injection attack; Search methods; security layer integration; Telecommunications; Time factors; Wrapping","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UGAD5REP","journalArticle","2025","Li, Miles Q.; Fung, Benjamin CM","Security concerns for large language models: A survey","Journal of Information Security and Applications","","","","https://www.sciencedirect.com/science/article/pii/S2214212625003217","","2025","2026-01-17 20:51:31","2026-01-17 20:51:31","2026-01-17 20:51:31","104284","","","95","","","Security concerns for large language models","","","","","Elsevier","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/P2DPB5Y4/Li e Fung - 2025 - Security concerns for large language models A survey.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8G462W3H","preprint","2025","Shi, Jiawen; Yuan, Zenghui; Tie, Guiyao; Zhou, Pan; Gong, Neil Zhenqiang; Sun, Lichao","Prompt Injection Attack to Tool Selection in LLM Agents","","","","10.48550/arXiv.2504.19793","http://arxiv.org/abs/2504.19793","Tool selection is a key component of LLM agents. A popular approach follows a two-step process - \emph{retrieval} and \emph{selection} - to pick the most appropriate tool from a tool library for a given task. In this work, we introduce \textit{ToolHijacker}, a novel prompt injection attack targeting tool selection in no-box scenarios. ToolHijacker injects a malicious tool document into the tool library to manipulate the LLM agent's tool selection process, compelling it to consistently choose the attacker's malicious tool for an attacker-chosen target task. Specifically, we formulate the crafting of such tool documents as an optimization problem and propose a two-phase optimization strategy to solve it. Our extensive experimental evaluation shows that ToolHijacker is highly effective, significantly outperforming existing manual-based and automated prompt injection attacks when applied to tool selection. Moreover, we explore various defenses, including prevention-based defenses (StruQ and SecAlign) and detection-based defenses (known-answer detection, DataSentinel, perplexity detection, and perplexity windowed detection). Our experimental results indicate that these defenses are insufficient, highlighting the urgent need for developing new defense strategies.","2025-08-24","2026-01-17 20:53:03","2026-01-17 20:53:03","2026-01-17 20:52:41","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2504.19793 [cs]","","/home/arthurbrito/Zotero/storage/GJB93EJK/Shi et al. - 2025 - Prompt Injection Attack to Tool Selection in LLM Agents.pdf","","","Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2504.19793","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WQKRZKWQ","preprint","2025","Nasr, Milad; Carlini, Nicholas; Sitawarin, Chawin; Schulhoff, Sander V.; Hayes, Jamie; Ilie, Michael; Pluto, Juliette; Song, Shuang; Chaudhari, Harsh; Shumailov, Ilia; Thakurta, Abhradeep; Xiao, Kai Yuanqing; Terzis, Andreas; Tramèr, Florian","The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prompt Injections","","","","10.48550/arXiv.2510.09023","http://arxiv.org/abs/2510.09023","How should we evaluate the robustness of language model defenses? Current defenses against jailbreaks and prompt injections (which aim to prevent an attacker from eliciting harmful knowledge or remotely triggering malicious actions, respectively) are typically evaluated either against a static set of harmful attack strings, or against computationally weak optimization methods that were not designed with the defense in mind. We argue that this evaluation process is flawed. Instead, we should evaluate defenses against adaptive attackers who explicitly modify their attack strategy to counter a defense's design while spending considerable resources to optimize their objective. By systematically tuning and scaling general optimization techniques-gradient descent, reinforcement learning, random search, and human-guided exploration-we bypass 12 recent defenses (based on a diverse set of techniques) with attack success rate above 90% for most; importantly, the majority of defenses originally reported near-zero attack success rates. We believe that future defense work must consider stronger attacks, such as the ones we describe, in order to make reliable and convincing claims of robustness.","2025-10-10","2026-01-17 20:53:03","2026-01-17 20:53:03","2026-01-17 20:52:43","","","","","","","The Attacker Moves Second","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2510.09023 [cs]","","/home/arthurbrito/Zotero/storage/3N4QN4E2/Nasr et al. - 2025 - The Attacker Moves Second Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prom.pdf","","","Computer Science - Cryptography and Security; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2510.09023","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4JXNEHR3","preprint","2024","Wong, Aidan; Cao, He; Liu, Zijing; Li, Yu","SMILES-Prompting: A Novel Approach to LLM Jailbreak Attacks in Chemical Synthesis","","","","10.48550/arXiv.2410.15641","http://arxiv.org/abs/2410.15641","The increasing integration of large language models (LLMs) across various fields has heightened concerns about their potential to propagate dangerous information. This paper specifically explores the security vulnerabilities of LLMs within the field of chemistry, particularly their capacity to provide instructions for synthesizing hazardous substances. We evaluate the effectiveness of several prompt injection attack methods, including red-teaming, explicit prompting, and implicit prompting. Additionally, we introduce a novel attack technique named SMILES-prompting, which uses the Simplified Molecular-Input Line-Entry System (SMILES) to reference chemical substances. Our findings reveal that SMILES-prompting can effectively bypass current safety mechanisms. These findings highlight the urgent need for enhanced domain-specific safeguards in LLMs to prevent misuse and improve their potential for positive social impact.","2024-10-21","2026-01-17 20:53:03","2026-01-17 20:53:03","2026-01-17 20:52:51","","","","","","","SMILES-Prompting","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2410.15641 [cs]","","/home/arthurbrito/Zotero/storage/3U4M6479/Wong et al. - 2024 - SMILES-Prompting A Novel Approach to LLM Jailbreak Attacks in Chemical Synthesis.pdf","","","Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2410.15641","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3W9PYD44","journalArticle","2024","Kwon, Hyeokjin; Pak, Wooguil","Text-based prompt injection attack using mathematical functions in modern large language models","Electronics","","","","https://www.mdpi.com/2079-9292/13/24/5008","","2024","2026-01-17 20:53:03","2026-01-17 20:53:03","2026-01-17 20:52:52","5008","","24","13","","","","","","","","MDPI","","","","","","","Google Scholar","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UXW44JRY","preprint","2025","Shang, Zhengchun; Wei, Wenlan; Bai, Weiheng","Evolving Security in LLMs: A Study of Jailbreak Attacks and Defenses","","","","10.48550/arXiv.2504.02080","http://arxiv.org/abs/2504.02080","Large Language Models (LLMs) are increasingly popular, powering a wide range of applications. Their widespread use has sparked concerns, especially through jailbreak attacks that bypass safety measures to produce harmful content. In this paper, we present a comprehensive security analysis of large language models (LLMs), addressing critical research questions on the evolution and determinants of model safety. Specifically, we begin by identifying the most effective techniques for detecting jailbreak attacks. Next, we investigate whether newer versions of LLMs offer improved security compared to their predecessors. We also assess the impact of model size on overall security and explore the potential benefits of integrating multiple defense strategies to enhance the security. Our study evaluates both open-source (e.g., LLaMA and Mistral) and closed-source models (e.g., GPT-4) by employing four state-of-the-art attack techniques and assessing the efficacy of three new defensive approaches.","2025-12-24","2026-01-17 20:53:03","2026-01-17 20:53:03","2026-01-17 20:52:53","","","","","","","Evolving Security in LLMs","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2504.02080 [cs]","","/home/arthurbrito/Zotero/storage/N52JA7QL/Shang et al. - 2025 - Evolving Security in LLMs A Study of Jailbreak Attacks and Defenses.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2504.02080","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"359K6CA9","preprint","2025","Mao, Yanxu; Cui, Tiehan; Liu, Peipei; You, Datao; Zhu, Hongsong","From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in Jailbreak Attacks and Defenses within LLM Ecosystem","","","","10.48550/arXiv.2506.15170","http://arxiv.org/abs/2506.15170","Large language models (LLMs) are rapidly evolving from single-modal systems to multimodal LLMs and intelligent agents, significantly expanding their capabilities while introducing increasingly severe security risks. This paper presents a systematic survey of the growing complexity of jailbreak attacks and corresponding defense mechanisms within the expanding LLM ecosystem. We first trace the developmental trajectory from LLMs to MLLMs and Agents, highlighting the core security challenges emerging at each stage. Next, we categorize mainstream jailbreak techniques from both the attack impact and visibility perspectives, and provide a comprehensive analysis of representative attack methods, related datasets, and evaluation metrics. On the defense side, we organize existing strategies based on response timing and technical approach, offering a structured understanding of their applicability and implementation. Furthermore, we identify key limitations in existing surveys, such as insufficient attention to agent-specific security issues, the absence of a clear taxonomy for hybrid jailbreak methods, a lack of detailed analysis of experimental setups, and outdated coverage of recent advancements. To address these limitations, we provide an updated synthesis of recent work and outline future research directions in areas such as dataset construction, evaluation framework optimization, and strategy generalization. Our study seeks to enhance the understanding of jailbreak mechanisms and facilitate the advancement of more resilient and adaptive defense strategies in the context of ever more capable LLMs.","2025-08-01","2026-01-17 20:53:03","2026-01-17 20:53:03","2026-01-17 20:52:57","","","","","","","From LLMs to MLLMs to Agents","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2506.15170 [cs]","","/home/arthurbrito/Zotero/storage/JF7CHVNG/Mao et al. - 2025 - From LLMs to MLLMs to Agents A Survey of Emerging Paradigms in Jailbreak Attacks and Defenses withi.pdf","","","Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2506.15170","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LLQB5I77","journalArticle","2024","Pingua, Bhagyajit; Murmu, Deepak; Kandpal, Meenakshi; Rautaray, Jyotirmayee; Mishra, Pranati; Barik, Rabindra Kumar; Saikia, Manob Jyoti","Mitigating adversarial manipulation in LLMs: a prompt-based approach to counter Jailbreak attacks (Prompt-G)","PeerJ Computer Science","","","","https://peerj.com/articles/cs-2374/?td=tw","","2024","2026-01-17 20:53:03","2026-01-17 20:53:03","2026-01-17 20:52:57","e2374","","","10","","","Mitigating adversarial manipulation in LLMs","","","","","PeerJ Inc.","","","","","","","Google Scholar","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IBJU3ZCX","journalArticle","2025","Shaha, Hridoy","Understanding prompt injection attacks in Web-based LLM applications and basic mitigation strategies","","","","","https://www.theseus.fi/handle/10024/889533","","2025","2026-01-17 20:53:03","2026-01-17 20:53:03","2026-01-17 20:52:59","","","","","","","","","","","","","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/96E6A7X3/Shaha - 2025 - Understanding prompt injection attacks in Web-based LLM applications and basic mitigation strategies.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2KEIRXTQ","conferencePaper","2024","Kim, Minjae; Kwon, Taehyeong; Shim, Kibeom; Kim, Beonghoon","Protection of LLM Environment Using Prompt Security","2024 15th International Conference on Information and Communication Technology Convergence (ICTC)","","","","https://ieeexplore.ieee.org/abstract/document/10827351/","","2024","2026-01-17 20:53:03","2026-01-17 21:19:01","2026-01-17 20:53:03","1715–1719","","","","","","","","","","","IEEE","","","","","","","Google Scholar","","","","","","","Generative AI; Large language models; Security; LLM; Codes; Protection; AI Security; Convergence; Data privacy; Identification of persons; Information and communication technology; Malware; Privacy Protection; Prompt Detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YXDDSA9Y","preprint","2025","Peng, Benji; Chen, Keyu; Niu, Qian; Bi, Ziqian; Liu, Ming; Feng, Pohsun; Wang, Tianyang; Yan, Lawrence K. Q.; Wen, Yizhu; Zhang, Yichao; Yin, Caitlyn Heqi; Song, Xinyuan","Jailbreaking and Mitigation of Vulnerabilities in Large Language Models","","","","10.48550/arXiv.2410.15236","http://arxiv.org/abs/2410.15236","Large Language Models (LLMs) have transformed artificial intelligence by advancing natural language understanding and generation, enabling applications across fields beyond healthcare, software engineering, and conversational systems. Despite these advancements in the past few years, LLMs have shown considerable vulnerabilities, particularly to prompt injection and jailbreaking attacks. This review analyzes the state of research on these vulnerabilities and presents available defense strategies. We roughly categorize attack approaches into prompt-based, model-based, multimodal, and multilingual, covering techniques such as adversarial prompting, backdoor injections, and cross-modality exploits. We also review various defense mechanisms, including prompt filtering, transformation, alignment techniques, multi-agent defenses, and self-regulation, evaluating their strengths and shortcomings. We also discuss key metrics and benchmarks used to assess LLM safety and robustness, noting challenges like the quantification of attack success in interactive contexts and biases in existing datasets. Identifying current research gaps, we suggest future directions for resilient alignment strategies, advanced defenses against evolving attacks, automation of jailbreak detection, and consideration of ethical and societal impacts. This review emphasizes the need for continued research and cooperation within the AI community to enhance LLM security and ensure their safe deployment.","2025-11-25","2026-01-17 20:55:33","2026-01-17 20:55:33","2026-01-17 20:55:18","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2410.15236 [cs]","","/home/arthurbrito/Zotero/storage/UM5J237P/Peng et al. - 2025 - Jailbreaking and Mitigation of Vulnerabilities in Large Language Models.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2410.15236","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IGV3GK59","conferencePaper","2024","Zhang, Wenxiao; Kong, Xiangrui; Dewitt, Conan; Braunl, Thomas; Hong, Jin B.","A study on prompt injection attack against llm-integrated mobile robotic systems","2024 IEEE 35th International Symposium on Software Reliability Engineering Workshops (ISSREW)","","","","https://ieeexplore.ieee.org/abstract/document/10771340/","","2024","2026-01-17 20:55:33","2026-01-17 20:55:33","2026-01-17 20:55:19","361–368","","","","","","","","","","","IEEE","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/CVJCDQJ2/Zhang et al. - 2024 - A study on prompt injection attack against llm-integrated mobile robotic systems.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9QT59X4U","preprint","2024","Zhou, Yuqi; Lu, Lin; Sun, Hanchi; Zhou, Pan; Sun, Lichao","Virtual Context: Enhancing Jailbreak Attacks with Special Token Injection","","","","10.48550/arXiv.2406.19845","http://arxiv.org/abs/2406.19845","Jailbreak attacks on large language models (LLMs) involve inducing these models to generate harmful content that violates ethics or laws, posing a significant threat to LLM security. Current jailbreak attacks face two main challenges: low success rates due to defensive measures and high resource requirements for crafting specific prompts. This paper introduces Virtual Context, which leverages special tokens, previously overlooked in LLM security, to improve jailbreak attacks. Virtual Context addresses these challenges by significantly increasing the success rates of existing jailbreak methods and requiring minimal background knowledge about the target model, thus enhancing effectiveness in black-box settings without additional overhead. Comprehensive evaluations show that Virtual Context-assisted jailbreak attacks can improve the success rates of four widely used jailbreak methods by approximately 40% across various LLMs. Additionally, applying Virtual Context to original malicious behaviors still achieves a notable jailbreak effect. In summary, our research highlights the potential of special tokens in jailbreak attacks and recommends including this threat in red-teaming testing to comprehensively enhance LLM security.","2024-07-11","2026-01-17 20:55:33","2026-01-17 20:55:33","2026-01-17 20:55:21","","","","","","","Virtual Context","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2406.19845 [cs]","","/home/arthurbrito/Zotero/storage/HQVJXRXT/Zhou et al. - 2024 - Virtual Context Enhancing Jailbreak Attacks with Special Token Injection.pdf","","","Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2406.19845","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SJ2WP6BW","preprint","2025","Maloyan, Narek; Ashinov, Bislan; Namiot, Dmitry","Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks","","","","10.48550/arXiv.2505.13348","http://arxiv.org/abs/2505.13348","Large Language Models (LLMs) are increasingly employed as evaluators (LLM-as-a-Judge) for assessing the quality of machine-generated text. This paradigm offers scalability and cost-effectiveness compared to human annotation. However, the reliability and security of such systems, particularly their robustness against adversarial manipulations, remain critical concerns. This paper investigates the vulnerability of LLM-as-a-Judge architectures to prompt-injection attacks, where malicious inputs are designed to compromise the judge's decision-making process. We formalize two primary attack strategies: Comparative Undermining Attack (CUA), which directly targets the final decision output, and Justification Manipulation Attack (JMA), which aims to alter the model's generated reasoning. Using the Greedy Coordinate Gradient (GCG) optimization method, we craft adversarial suffixes appended to one of the responses being compared. Experiments conducted on the MT-Bench Human Judgments dataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and Falcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves an Attack Success Rate (ASR) exceeding 30\%, while JMA also shows notable effectiveness. These findings highlight substantial vulnerabilities in current LLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and further research into adversarial evaluation and trustworthiness in LLM-based assessment frameworks.","2025-05-19","2026-01-17 20:55:33","2026-01-17 20:55:33","2026-01-17 20:55:23","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2505.13348 [cs]","","/home/arthurbrito/Zotero/storage/MGFUZJAE/Maloyan et al. - 2025 - Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks.pdf","","","Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2505.13348","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DBRQ32BD","journalArticle","2024","Rai, Parijat; Sood, Saumil; Madisetti, Vijay K.; Bahga, Arshdeep","Guardian: A multi-tiered defense architecture for thwarting prompt injection attacks on llms","Journal of Software Engineering and Applications","","","","","","2024","2026-01-17 20:55:33","2026-01-17 20:55:33","","43–68","","1","17","","","Guardian","","","","","Scientific Research Publishing","","","","","","","Google Scholar","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RGX9EV67","conferencePaper","2025","Shao, Zedian; Liu, Hongbin; Mu, Jaden; Gong, Neil","Enhancing Prompt Injection Attacks to LLMs via Poisoning Alignment","Proceedings of the 18th ACM Workshop on Artificial Intelligence and Security","979-8-4007-1895-3","","10.1145/3733799.3762963","https://dl.acm.org/doi/10.1145/3733799.3762963","","2025-10-13","2026-01-17 20:55:33","2026-01-17 20:55:33","2026-01-17 20:55:28","13-27","","","","","","","","","","","ACM","Taipei , Taiwan","en","","","","","DOI.org (Crossref)","","","","/home/arthurbrito/Zotero/storage/QJFUQ5TL/Shao et al. - 2025 - Enhancing Prompt Injection Attacks to LLMs via Poisoning Alignment.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AISec '25: Proceedings of the 2025 Workshop on Artificial Intelligence and Security","","","","","","","","","","","","","","",""
"BJLGCY9Y","conferencePaper","2024","Rababah, Baha; Wu, Shang Tommy; Kwiatkowski, Matthew; Leung, Carson K.; Akcora, Cuneyt Gurcan","SoK: prompt hacking of large language models","2024 IEEE International Conference on Big Data (BigData)","","","","https://ieeexplore.ieee.org/abstract/document/10825103/","","2024","2026-01-17 20:55:33","2026-01-17 20:55:33","2026-01-17 20:55:29","5392–5401","","","","","","SoK","","","","","IEEE","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/58ZVH8AM/Rababah et al. - 2024 - SoK prompt hacking of large language models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AWB8NQ4E","preprint","2025","Liu, Xu; Chen, Yan; Ling, Kan; Zhu, Yichi; Zhang, Hengrun; Fan, Guisheng; Yu, Huiqun","An Automated Framework for Strategy Discovery, Retrieval, and Evolution in LLM Jailbreak Attacks","","","","10.48550/arXiv.2511.02356","http://arxiv.org/abs/2511.02356","The widespread deployment of Large Language Models (LLMs) as public-facing web services and APIs has made their security a core concern for the web ecosystem. Jailbreak attacks, as one of the significant threats to LLMs, have recently attracted extensive research. In this paper, we reveal a jailbreak strategy which can effectively evade current defense strategies. It can extract valuable information from failed or partially successful attack attempts and contains self-evolution from attack interactions, resulting in sufficient strategy diversity and adaptability. Inspired by continuous learning and modular design principles, we propose ASTRA, a jailbreak framework that autonomously discovers, retrieves, and evolves attack strategies to achieve more efficient and adaptive attacks. To enable this autonomous evolution, we design a closed-loop ""attack-evaluate-distill-reuse"" core mechanism that not only generates attack prompts but also automatically distills and generalizes reusable attack strategies from every interaction. To systematically accumulate and apply this attack knowledge, we introduce a three-tier strategy library that categorizes strategies into Effective, Promising, and Ineffective based on their performance scores. The strategy library not only provides precise guidance for attack generation but also possesses exceptional extensibility and transferability. We conduct extensive experiments under a black-box setting, and the results show that ASTRA achieves an average Attack Success Rate (ASR) of 82.7%, significantly outperforming baselines.","2025-11-04","2026-01-17 20:55:33","2026-01-17 20:55:33","2026-01-17 20:55:31","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2511.02356 [cs]","","/home/arthurbrito/Zotero/storage/S4ENJL8B/Liu et al. - 2025 - An Automated Framework for Strategy Discovery, Retrieval, and Evolution in LLM Jailbreak Attacks.pdf","","","Computer Science - Cryptography and Security; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2511.02356","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HYHEKIC3","conferencePaper","2025","Rahman, Md Abdur; Shahriar, Hossain; Francia, Guillermo; Wu, Fan; Cuzzocrea, Alfredo; Rahman, Muhammad; Faruk, Md Jobair Hossain; Ahamed, Sheikh Iqbal","Fine-tuned large language models (llms): Improved prompt injection attacks detection","2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC)","","","","https://ieeexplore.ieee.org/abstract/document/11126597/","","2025","2026-01-17 20:56:28","2026-01-17 20:56:28","2026-01-17 20:56:14","1033–1039","","","","","","Fine-tuned large language models (llms)","","","","","IEEE","","","","","","","Google Scholar","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VY23YWYM","preprint","2024","Liu, Frank Weizhen; Hu, Chenhui","Exploring Vulnerabilities and Protections in Large Language Models: A Survey","","","","10.48550/arXiv.2406.00240","http://arxiv.org/abs/2406.00240","As Large Language Models (LLMs) increasingly become key components in various AI applications, understanding their security vulnerabilities and the effectiveness of defense mechanisms is crucial. This survey examines the security challenges of LLMs, focusing on two main areas: Prompt Hacking and Adversarial Attacks, each with specific types of threats. Under Prompt Hacking, we explore Prompt Injection and Jailbreaking Attacks, discussing how they work, their potential impacts, and ways to mitigate them. Similarly, we analyze Adversarial Attacks, breaking them down into Data Poisoning Attacks and Backdoor Attacks. This structured examination helps us understand the relationships between these vulnerabilities and the defense strategies that can be implemented. The survey highlights these security challenges and discusses robust defensive frameworks to protect LLMs against these threats. By detailing these security issues, the survey contributes to the broader discussion on creating resilient AI systems that can resist sophisticated attacks.","2024-06-01","2026-01-17 20:56:28","2026-01-17 20:56:28","2026-01-17 20:56:16","","","","","","","Exploring Vulnerabilities and Protections in Large Language Models","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2406.00240 [cs]","","/home/arthurbrito/Zotero/storage/AJY5VPWX/Liu e Hu - 2024 - Exploring Vulnerabilities and Protections in Large Language Models A Survey.pdf","","","Computer Science - Computation and Language; Computer Science - Cryptography and Security; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2406.00240","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2LSDXGLD","conferencePaper","2024","Colares, Daniel Mendonça; Holanda Filho, Raimir; Gouveia, Luis Borges","A proposal framework security assessment for large language models","Proceedings of the First International Conference on Natural Language Processing and Artificial Intelligence for Cyber Security","","","","https://aclanthology.org/2024.nlpaics-1.23/","","2024","2026-01-17 20:56:28","2026-01-17 20:56:28","2026-01-17 20:56:16","212–219","","","","","","","","","","","","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/ZKJIPBIG/Colares et al. - 2024 - A proposal framework security assessment for large language models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P9TQHG3Z","preprint","2025","Xu, Wenrui; Parhi, Keshab K.","A Survey of Attacks on Large Language Models","","","","10.48550/arXiv.2505.12567","http://arxiv.org/abs/2505.12567","Large language models (LLMs) and LLM-based agents have been widely deployed in a wide range of applications in the real world, including healthcare diagnostics, financial analysis, customer support, robotics, and autonomous driving, expanding their powerful capability of understanding, reasoning, and generating natural languages. However, the wide deployment of LLM-based applications exposes critical security and reliability risks, such as the potential for malicious misuse, privacy leakage, and service disruption that weaken user trust and undermine societal safety. This paper provides a systematic overview of the details of adversarial attacks targeting both LLMs and LLM-based agents. These attacks are organized into three phases in LLMs: Training-Phase Attacks, Inference-Phase Attacks, and Availability & Integrity Attacks. For each phase, we analyze the details of representative and recently introduced attack methods along with their corresponding defenses. We hope our survey will provide a good tutorial and a comprehensive understanding of LLM security, especially for attacks on LLMs. We desire to raise attention to the risks inherent in widely deployed LLM-based applications and highlight the urgent need for robust mitigation strategies for evolving threats.","2025-05-18","2026-01-17 20:56:28","2026-01-17 20:56:28","2026-01-17 20:56:18","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2505.12567 [cs]","","/home/arthurbrito/Zotero/storage/5CS73VMB/Xu e Parhi - 2025 - A Survey of Attacks on Large Language Models.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2505.12567","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A3HDI3H7","conferencePaper","2024","Kumar, Surender Suresh; Cummings, M.; Stimpson, Alexander","Strengthening LLM trust boundaries: a survey of prompt injection attacks","2024 IEEE 4th International Conference on Human-Machine Systems (ICHMS)","","","","https://www.researchgate.net/profile/Missy-Cummings/publication/381564420_Strengthening_LLM_Trust_Boundaries_A_Survey_of_Prompt_Injection_Attacks_Surender_Suresh_Kumar_Dr_ML_Cummings_Dr_Alexander_Stimpson/links/67e153d7e2c0ea36cd9b8d71/Strengthening-LLM-Trust-Boundaries-A-Survey-of-Prompt-Injection-Attacks-Surender-Suresh-Kumar-Dr-ML-Cummings-Dr-Alexander-Stimpson.pdf","","2024","2026-01-17 20:56:28","2026-01-17 20:56:28","2026-01-17 20:56:18","1–6","","","","","","Strengthening LLM trust boundaries","","","","","","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/SKSA248P/Kumar et al. - 2024 - Strengthening LLM trust boundaries a survey of prompt injection attacks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y7WUQSWF","preprint","2025","Wang, Yizhu; Chen, Sizhe; Alkhudair, Raghad; Alomair, Basel; Wagner, David","Defending Against Prompt Injection with DataFilter","","","","10.48550/arXiv.2510.19207","http://arxiv.org/abs/2510.19207","When large language model (LLM) agents are increasingly deployed to automate tasks and interact with untrusted external data, prompt injection emerges as a significant security threat. By injecting malicious instructions into the data that LLMs access, an attacker can arbitrarily override the original user task and redirect the agent toward unintended, potentially harmful actions. Existing defenses either require access to model weights (fine-tuning), incur substantial utility loss (detection-based), or demand non-trivial system redesign (system-level). Motivated by this, we propose DataFilter, a test-time model-agnostic defense that removes malicious instructions from the data before it reaches the backend LLM. DataFilter is trained with supervised fine-tuning on simulated injections and leverages both the user's instruction and the data to selectively strip adversarial content while preserving benign information. Across multiple benchmarks, DataFilter consistently reduces the prompt injection attack success rates to near zero while maintaining the LLMs' utility. DataFilter delivers strong security, high utility, and plug-and-play deployment, making it a strong practical defense to secure black-box commercial LLMs against prompt injection. Our DataFilter model is released at https://huggingface.co/JoyYizhu/DataFilter for immediate use, with the code to reproduce our results at https://github.com/yizhu-joy/DataFilter.","2025-10-22","2026-01-17 20:56:28","2026-01-17 20:56:28","2026-01-17 20:56:22","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2510.19207 [cs]","","/home/arthurbrito/Zotero/storage/MVV4DUII/Wang et al. - 2025 - Defending Against Prompt Injection with DataFilter.pdf","","","Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2510.19207","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TKBGJBVI","journalArticle","2025","Rashid, Sharaf; Bollis, Edson; Pellicer, Lucas; Rabbani, Darian; Palacios, Rafael; Gupta, Aneesh; Gupta, Amar","Evaluating Prompt Injection Attacks with LSTM-Based Generative Adversarial Networks: A Lightweight Alternative to Large Language Models","Machine Learning and Knowledge Extraction","","","","https://www.mdpi.com/2504-4990/7/3/77","","2025","2026-01-17 20:56:28","2026-01-17 20:56:28","2026-01-17 20:56:24","77","","3","7","","","Evaluating Prompt Injection Attacks with LSTM-Based Generative Adversarial Networks","","","","","MDPI","","","","","","","Google Scholar","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FT66UIL9","preprint","2023","Shayegani, Erfan; Mamun, Md Abdullah Al; Fu, Yu; Zaree, Pedram; Dong, Yue; Abu-Ghazaleh, Nael","Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks","","","","10.48550/arXiv.2310.10844","http://arxiv.org/abs/2310.10844","Large Language Models (LLMs) are swiftly advancing in architecture and capability, and as they integrate more deeply into complex systems, the urgency to scrutinize their security properties grows. This paper surveys research in the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield of trustworthy ML, combining the perspectives of Natural Language Processing and Security. Prior work has shown that even safety-aligned LLMs (via instruction tuning and reinforcement learning through human feedback) can be susceptible to adversarial attacks, which exploit weaknesses and mislead AI systems, as evidenced by the prevalence of `jailbreak' attacks on models like ChatGPT and Bard. In this survey, we first provide an overview of large language models, describe their safety alignment, and categorize existing research based on various learning structures: textual-only attacks, multi-modal attacks, and additional attack methods specifically targeting complex systems, such as federated learning or multi-agent systems. We also offer comprehensive remarks on works that focus on the fundamental sources of vulnerabilities and potential defenses. To make this field more accessible to newcomers, we present a systematic review of existing works, a structured typology of adversarial attack concepts, and additional resources, including slides for presentations on related topics at the 62nd Annual Meeting of the Association for Computational Linguistics (ACL'24).","2023-10-16","2026-01-17 20:56:28","2026-01-17 20:56:28","2026-01-17 20:56:28","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2310.10844 [cs]","","/home/arthurbrito/Zotero/storage/267AMK6E/Shayegani et al. - 2023 - Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks.pdf","","","Computer Science - Computation and Language; Computer Science - Cryptography and Security; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2310.10844","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6LL29HHN","journalArticle","2025","Lee, Seyong; Kim, Jaebeom; Pak, Wooguil","Mind Mapping Prompt Injection: Visual Prompt Injection Attacks in Modern Large Language Models","Electronics","","","","https://www.mdpi.com/2079-9292/14/10/1907","","2025","2026-01-17 20:56:28","2026-01-17 20:56:28","2026-01-17 20:56:28","1907","","10","14","","","Mind Mapping Prompt Injection","","","","","MDPI","","","","","","","Google Scholar","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SV9YW238","preprint","2024","Tete, Stephen Burabari","Threat Modelling and Risk Analysis for Large Language Model (LLM)-Powered Applications","","","","10.48550/arXiv.2406.11007","http://arxiv.org/abs/2406.11007","The advent of Large Language Models (LLMs) has revolutionized various applications by providing advanced natural language processing capabilities. However, this innovation introduces new cybersecurity challenges. This paper explores the threat modeling and risk analysis specifically tailored for LLM-powered applications. Focusing on potential attacks like data poisoning, prompt injection, SQL injection, jailbreaking, and compositional injection, we assess their impact on security and propose mitigation strategies. We introduce a framework combining STRIDE and DREAD methodologies for proactive threat identification and risk assessment. Furthermore, we examine the feasibility of an end-to-end threat model through a case study of a custom-built LLM-powered application. This model follows Shostack's Four Question Framework, adjusted for the unique threats LLMs present. Our goal is to propose measures that enhance the security of these powerful AI tools, thwarting attacks, and ensuring the reliability and integrity of LLM-integrated systems.","2024-06-16","2026-01-17 20:57:15","2026-01-17 20:57:15","2026-01-17 20:56:52","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2406.11007 [cs]","","/home/arthurbrito/Zotero/storage/84K7XZ8S/Tete - 2024 - Threat Modelling and Risk Analysis for Large Language Model (LLM)-Powered Applications.pdf","","","Computer Science - Cryptography and Security; Computer Science - Software Engineering","","","","","","","","","","","","","","","","","","","arXiv:2406.11007","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3XAEXB59","preprint","2025","Aguilera-Martínez, Francisco; Berzal, Fernando","LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures","","","","10.48550/arXiv.2505.01177","http://arxiv.org/abs/2505.01177","As large language models (LLMs) continue to evolve, it is critical to assess the security threats and vulnerabilities that may arise both during their training phase and after models have been deployed. This survey seeks to define and categorize the various attacks targeting LLMs, distinguishing between those that occur during the training phase and those that affect already trained models. A thorough analysis of these attacks is presented, alongside an exploration of defense mechanisms designed to mitigate such threats. Defenses are classified into two primary categories: prevention-based and detection-based defenses. Furthermore, our survey summarizes possible attacks and their corresponding defense strategies. It also provides an evaluation of the effectiveness of the known defense mechanisms for the different security threats. Our survey aims to offer a structured framework for securing LLMs, while also identifying areas that require further research to improve and strengthen defenses against emerging security challenges.","2025-05-02","2026-01-17 20:57:15","2026-01-17 20:57:15","2026-01-17 20:56:54","","","","","","","LLM Security","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2505.01177 [cs]","","/home/arthurbrito/Zotero/storage/7DYLKYEU/Aguilera-Martínez e Berzal - 2025 - LLM Security Vulnerabilities, Attacks, Defenses, and Countermeasures.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","arXiv:2505.01177","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8ILYQELN","journalArticle","2025","Tsmindashvili, Tatia; Kolkhidashvili, Ana; Kurtskhalia, Dachi; Maghlakelidze, Nino; Mekvabishvili, Elene; Dentoshvili, Guram; Shamilov, Orkhan; Gachechiladze, Zaal; Saporta, Steven; Choladze, David Dachi","Improving LLM Outputs Against Jailbreak Attacks with Expert Model Integration","IEEE Access","","2169-3536","10.1109/ACCESS.2025.3592458","http://arxiv.org/abs/2505.17066","Using LLMs in a production environment presents security challenges that include vulnerabilities to jailbreaks and prompt injections, which can result in harmful outputs for humans or the enterprise. The challenge is amplified when working within a specific domain, as topics generally accepted for LLMs to address may be irrelevant to that field. These problems can be mitigated, for example, by fine-tuning large language models with domain-specific and security-focused data. However, these alone are insufficient, as jailbreak techniques evolve. Additionally, API-accessed models do not offer the flexibility needed to tailor behavior to industry-specific objectives, and in-context learning is not always sufficient or reliable. In response to these challenges, we introduce Archias, an expert model adept at distinguishing between in-domain and out-of-domain communications. Archias classifies user inquiries into several categories: in-domain (specifically for the automotive industry), malicious questions, price injections, prompt injections, and out-of-domain examples. Our methodology integrates outputs from the expert model (Archias) into prompts, which are then processed by the LLM to generate responses. This method increases the model's ability to understand the user's intention and give appropriate answers. Archias can be adjusted, fine-tuned, and used for many different purposes due to its small size. Therefore, it can be easily customized to the needs of any industry. To validate our approach, we created a benchmark dataset for the automotive industry. Furthermore, in the interest of advancing research and development, we release our benchmark dataset to the community.","2025","2026-01-17 20:57:15","2026-01-17 20:57:15","2026-01-17 20:56:57","134976-134988","","","13","","IEEE Access","","","","","","","","","","","","","arXiv.org","","arXiv:2505.17066 [cs]","","/home/arthurbrito/Zotero/storage/KLH4B4WN/Tsmindashvili et al. - 2025 - Improving LLM Outputs Against Jailbreak Attacks with Expert Model Integration.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SNGEWAEF","conferencePaper","2025","Yi, Jingwei; Xie, Yueqi; Zhu, Bin; Kiciman, Emre; Sun, Guangzhong; Xie, Xing; Wu, Fangzhao","Benchmarking and Defending against Indirect Prompt Injection Attacks on Large Language Models","Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1","979-8-4007-1245-6","","10.1145/3690624.3709179","https://dl.acm.org/doi/10.1145/3690624.3709179","","2025-07-20","2026-01-17 20:57:15","2026-01-17 20:57:15","2026-01-17 20:57:00","1809-1820","","","","","","","","","","","ACM","Toronto ON Canada","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","KDD '25: The 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining","","","","","","","","","","","","","","",""
"A7KL7WTG","journalArticle","2025","Feng, Yingchaojie; Chen, Zhizhang; Kang, Zhining; Wang, Sijia; Tian, Haoyu; Zhang, Wei; Zhu, Minfeng; Chen, Wei","Jailbreaklens: Visual analysis of jailbreak attacks against large language models","IEEE Transactions on Visualization and Computer Graphics","","","","https://ieeexplore.ieee.org/abstract/document/11020711/","","2025","2026-01-17 20:57:15","2026-01-17 20:57:15","2026-01-17 20:57:03","","","","","","","Jailbreaklens","","","","","IEEE","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/I6HH4PDS/Feng et al. - 2025 - Jailbreaklens Visual analysis of jailbreak attacks against large language models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J3FCFQVJ","journalArticle","2025","Shu, Dong; Zhang, Chong; Jin, Mingyu; Zhou, Zihao; Li, Lingyao","AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models","ACM SIGKDD Explorations Newsletter","","1931-0145, 1931-0153","10.1145/3748239.3748242","https://dl.acm.org/doi/10.1145/3748239.3748242","Jailbreak attacks represent one of the most sophisticated threats to the security of large language models (LLMs). To deal with such risks, we introduce an innovative framework that can help evaluate the effectiveness of jailbreak attacks on LLMs. Unlike traditional binary evaluations focusing solely on the robustness of LLMs, our method assesses the attacking prompts' effectiveness. We present two distinct evaluation frameworks: a coarse-grained evaluation and a fine-grained evaluation. Each framework uses a scoring range from 0 to 1, offering unique perspectives and allowing for the assessment of attack effectiveness in different scenarios. Additionally, we develop a comprehensive ground truth dataset specifically tailored for jailbreak prompts. This dataset is a crucial benchmark for our current study and provides a foundational resource for future research. By comparing with traditional evaluation methods, our study shows that the current results align with baseline metrics while offering a more nuanced and fine-grained assessment. It also helps identify potentially harmful attack prompts that might appear harmless in traditional evaluations. Overall, our work establishes a solid foundation for assessing a broader range of attack prompts in prompt injection.","2025-07-07","2026-01-17 20:57:15","2026-01-17 21:17:32","2026-01-17 20:57:05","10-19","","1","27","","SIGKDD Explor. Newsl.","AttackEval","","","","","","","en","","","","","DOI.org (Crossref)","","","","/home/arthurbrito/Zotero/storage/8TELMGSU/Zhou et al. - 2024 - AttackEval How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P3T62L4T","preprint","2025","Wang, Nan; Walter, Kane; Gao, Yansong; Abuadbba, Alsharif","Large Language Model Adversarial Landscape Through the Lens of Attack Objectives","","","","10.48550/arXiv.2502.02960","http://arxiv.org/abs/2502.02960","Large Language Models (LLMs) represent a transformative leap in artificial intelligence, enabling the comprehension, generation, and nuanced interaction with human language on an unparalleled scale. However, LLMs are increasingly vulnerable to a range of adversarial attacks that threaten their privacy, reliability, security, and trustworthiness. These attacks can distort outputs, inject biases, leak sensitive information, or disrupt the normal functioning of LLMs, posing significant challenges across various applications. In this paper, we provide a novel comprehensive analysis of the adversarial landscape of LLMs, framed through the lens of attack objectives. By concentrating on the core goals of adversarial actors, we offer a fresh perspective that examines threats from the angles of privacy, integrity, availability, and misuse, moving beyond conventional taxonomies that focus solely on attack techniques. This objective-driven adversarial landscape not only highlights the strategic intent behind different adversarial approaches but also sheds light on the evolving nature of these threats and the effectiveness of current defenses. Our analysis aims to guide researchers and practitioners in better understanding, anticipating, and mitigating these attacks, ultimately contributing to the development of more resilient and robust LLM systems.","2025-02-05","2026-01-17 20:57:15","2026-01-17 20:57:15","2026-01-17 20:57:07","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2502.02960 [cs]","Comment: 15 pages","/home/arthurbrito/Zotero/storage/UXEA2ZRB/Wang et al. - 2025 - Large Language Model Adversarial Landscape Through the Lens of Attack Objectives.pdf","","","Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2502.02960","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LY9BHEHQ","conferencePaper","2025","Reddy, Pavan; Gujral, Aditya Sanjay","EchoLeak: The First Real-World Zero-Click Prompt Injection Exploit in a Production LLM System","Proceedings of the AAAI Symposium Series","","","","https://ojs.aaai.org/index.php/AAAI-SS/article/view/36899","","2025","2026-01-17 20:57:15","2026-01-17 20:57:15","2026-01-17 20:57:07","303–311","","1","7","","","EchoLeak","","","","","","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/CARMKT7J/Reddy e Gujral - 2025 - EchoLeak The First Real-World Zero-Click Prompt Injection Exploit in a Production LLM System.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5KSHZL9H","preprint","2025","Cui, Tiehan; Mao, Yanxu; Liu, Peipei; Liu, Congying; You, Datao","Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion","","","","10.48550/arXiv.2505.14316","http://arxiv.org/abs/2505.14316","Although large language models (LLMs) have achieved remarkable advancements, their security remains a pressing concern. One major threat is jailbreak attacks, where adversarial prompts bypass model safeguards to generate harmful or objectionable content. Researchers study jailbreak attacks to understand security and robustness of LLMs. However, existing jailbreak attack methods face two main challenges: (1) an excessive number of iterative queries, and (2) poor generalization across models. In addition, recent jailbreak evaluation datasets focus primarily on question-answering scenarios, lacking attention to text generation tasks that require accurate regeneration of toxic content. To tackle these challenges, we propose two contributions: (1) ICE, a novel black-box jailbreak method that employs Intent Concealment and divErsion to effectively circumvent security constraints. ICE achieves high attack success rates (ASR) with a single query, significantly improving efficiency and transferability across different models. (2) BiSceneEval, a comprehensive dataset designed for assessing LLM robustness in question-answering and text-generation tasks. Experimental results demonstrate that ICE outperforms existing jailbreak techniques, revealing critical vulnerabilities in current defense mechanisms. Our findings underscore the necessity of a hybrid security strategy that integrates predefined security mechanisms with real-time semantic decomposition to enhance the security of LLMs.","2025-05-20","2026-01-17 20:57:15","2026-01-17 20:57:15","2026-01-17 20:57:08","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2505.14316 [cs]","","/home/arthurbrito/Zotero/storage/BZDA65YG/Cui et al. - 2025 - Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2505.14316","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2LEJUGG4","journalArticle","2025","Fathima, Madiha","LLM Adversarial Prompt Attack Detection and Mitigation Engine: A Novel Framework for Securing Generative AI Systems","Authorea Preprints","","","","https://www.techrxiv.org/doi/full/10.36227/techrxiv.175416873.36395198","","2025","2026-01-17 20:57:15","2026-01-17 20:57:15","2026-01-17 20:57:15","","","","","","","LLM Adversarial Prompt Attack Detection and Mitigation Engine","","","","","Authorea","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/7HI6ALVD/Fathima - 2025 - LLM Adversarial Prompt Attack Detection and Mitigation Engine A Novel Framework for Securing Genera.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YE9DK8V2","bookSection","2024","Piet, Julien; Alrashed, Maha; Sitawarin, Chawin; Chen, Sizhe; Wei, Zeming; Sun, Elizabeth; Alomair, Basel; Wagner, David","Jatmo: Prompt Injection Defense by Task-Specific Finetuning","Computer Security – ESORICS 2024","978-3-031-70878-7 978-3-031-70879-4","","10.1007/978-3-031-70879-4_6","https://link.springer.com/10.1007/978-3-031-70879-4_6","","2024","2026-01-17 20:59:44","2026-01-17 20:59:44","2026-01-17 20:59:29","105-124","","","14982","","","Jatmo","","","","","Springer Nature Switzerland","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-031-70879-4_6","","/home/arthurbrito/Zotero/storage/5FYMP334/Piet et al. - 2024 - Jatmo Prompt Injection Defense by Task-Specific Finetuning.pdf","","","","Garcia-Alfaro, Joaquin; Kozik, Rafał; Choraś, Michał; Katsikas, Sokratis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QTL2VLZ7","preprint","2024","Chang, Zhiyuan; Li, Mingyang; Liu, Yi; Wang, Junjie; Wang, Qing; Liu, Yang","Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues","","","","10.48550/arXiv.2402.09091","http://arxiv.org/abs/2402.09091","With the development of LLMs, the security threats of LLMs are getting more and more attention. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks primarily utilize scenario camouflage techniques. However their explicitly mention of malicious intent will be easily recognized and defended by LLMs. In this paper, we propose an indirect jailbreak attack approach, Puzzler, which can bypass the LLM's defense strategy and obtain malicious response by implicitly providing LLMs with some clues about the original malicious query. In addition, inspired by the wisdom of ""When unable to attack, defend"" from Sun Tzu's Art of War, we adopt a defensive stance to gather clues about the original malicious query through LLMs. Extensive experimental results show that Puzzler achieves a query success rate of 96.6% on closed-source LLMs, which is 57.9%-82.7% higher than baselines. Furthermore, when tested against the state-of-the-art jailbreak detection approaches, Puzzler proves to be more effective at evading detection compared to baselines.","2024-02-16","2026-01-17 20:59:44","2026-01-17 20:59:44","2026-01-17 20:59:31","","","","","","","Play Guessing Game with LLM","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2402.09091 [cs]","Comment: 13 pages, 6 figures","/home/arthurbrito/Zotero/storage/9KKD67A6/Chang et al. - 2024 - Play Guessing Game with LLM Indirect Jailbreak Attack with Implicit Clues.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Human-Computer Interaction","","","","","","","","","","","","","","","","","","","arXiv:2402.09091","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QE4L539Q","journalArticle","","Thirumalaisamy, Karthikeyan","A Systematic Review of Vulnerability Scanning Tools for Large Language Models (LLMs)","Volume","","","","https://www.researchgate.net/profile/Karthikeyan-Thirumalaisamy/publication/398585540_A_Systematic_Review_of_Vulnerability_Scanning_Tools_for_Large_Language_Models_LLMs/links/693b087c27359023a00b184b/A-Systematic-Review-of-Vulnerability-Scanning-Tools-for-Large-Language-Models-LLMs.pdf","","","2026-01-17 20:59:44","2026-01-17 20:59:44","2026-01-17 20:59:31","543–548","","","14","","","","","","","","","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/SBLDUXWF/Thirumalaisamy - A Systematic Review of Vulnerability Scanning Tools for Large Language Models (LLMs).pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DPA649VX","journalArticle","2026","Zhang, Xiaoyu; Zhang, Cen; Li, Tianlin; Huang, Yihao; Jia, Xiaojun; Hu, Ming; Zhang, Jie; Liu, Yang; Ma, Shiqing; Shen, Chao","<span style=""font-variant:small-caps;"">JailGuard</span> : A Universal Detection Framework for Prompt-based Attacks on LLM Systems","ACM Transactions on Software Engineering and Methodology","","1049-331X, 1557-7392","10.1145/3724393","https://dl.acm.org/doi/10.1145/3724393","The systems and software powered by Large Language Models (LLMs) and Multi-Modal Large Language Models (MLLMs) have played a critical role in numerous scenarios. However, current LLM systems are vulnerable to prompt-based attacks, with jailbreaking attacks enabling the LLM system to generate harmful content, while hijacking attacks manipulate the LLM system to perform attacker-desired tasks, underscoring the necessity for detection tools. Unfortunately, existing detecting approaches are usually tailored to specific attacks, resulting in poor generalization in detecting various attacks across different modalities. To address it, we propose               JailGuard               , a universal detection framework deployed on top of LLM systems for prompt-based attacks across text and image modalities.               JailGuard               operates on the principle that attacks are inherently less robust than benign ones. Specifically,               JailGuard               mutates untrusted inputs to generate variants and leverages the discrepancy of the variants’ responses on the target model to distinguish attack samples from benign samples. We implement 18 mutators for text and image inputs and design a mutator combination policy to further improve detection generalization. The evaluation on the dataset containing 15 known attack types suggests that               JailGuard               achieves the best detection accuracy of 86.14%/82.90% on text and image inputs, outperforming state-of-the-art methods by 11.81–25.73% and 12.20–21.40%.","2026-01-31","2026-01-17 20:59:44","2026-01-17 20:59:44","2026-01-17 20:59:37","1-40","","1","35","","ACM Trans. Softw. Eng. Methodol.","<span style=""font-variant","","","","","","","en","","","","","DOI.org (Crossref)","","","","/home/arthurbrito/Zotero/storage/R7ZJ8B3W/Zhang et al. - 2026 - JailGuard  A Universal Detection Framework for Prompt.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SENV7SN7","conferencePaper","2025","Heverin, Thomas; Benjamin, Victoria; Braca, Emily; Carter, Israel; Kanchwala, Hafsa; Khojasteh, Nava; Landow, Charly; Luo, Yi; Ma, Caroline; Magarelli, Anna","Systematically Analysing Prompt Injection Vulnerabilities in Diverse LLM Architectures","International Conference on Cyber Warfare and Security","","","","https://search.proquest.com/openview/d4922b100257e7c5294ca579b35cf488/1?pq-origsite=gscholar&cbl=396500","","2025","2026-01-17 20:59:44","2026-01-17 20:59:44","2026-01-17 20:59:37","142–150","","","","","","","","","","","Academic Conferences International Limited","","","","","","","Google Scholar","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V79JSCDW","conferencePaper","2024","Hui, Bo; Yuan, Haolin; Gong, Neil; Burlina, Philippe; Cao, Yinzhi","<span style=""font-variant:small-caps;"">PLeak:</span> Prompt Leaking Attacks against Large Language Model Applications","Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security","979-8-4007-0636-3","","10.1145/3658644.3670370","https://dl.acm.org/doi/10.1145/3658644.3670370","","2024-12-02","2026-01-17 20:59:44","2026-01-17 20:59:44","2026-01-17 20:59:39","3600-3614","","","","","","<span style=""font-variant","","","","","ACM","Salt Lake City UT USA","en","","","","","DOI.org (Crossref)","","","","/home/arthurbrito/Zotero/storage/AIVK6DDY/Hui et al. - 2024 - PLeak Prompt Leaking Attacks against Large Language M.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","CCS '24: ACM SIGSAC Conference on Computer and Communications Security","","","","","","","","","","","","","","",""
"4KD8WG7I","journalArticle","2025","Choi, Wan Chong; Chang, Chi In; Ng, Sok I.; Choi, Iek Chong","A Review of “Do Anything Now” Jailbreak Attacks in Large Language Models: Potential Risks, Impacts, and Defense Strategies","ResearchGate","","","","https://www.researchgate.net/profile/Chi-In-Chang/publication/395135247_A_Review_of_Do_Anything_Now_Jailbreak_Attacks_in_Large_Language_Models_Potential_Risks_Impacts_and_Defense_Strategies/links/68b5a6a7360112563e0faaab/A-Review-of-Do-Anything-Now-Jailbreak-Attacks-in-Large-Language-Models-Potential-Risks-Impacts-and-Defense-Strategies.pdf","","2025","2026-01-17 20:59:44","2026-01-17 20:59:44","2026-01-17 20:59:39","","","","","","","A Review of “Do Anything Now” Jailbreak Attacks in Large Language Models","","","","","","","","","","","","Google Scholar","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S56YPWX5","preprint","2024","Wu, Fangzhou; Cecchetti, Ethan; Xiao, Chaowei","System-Level Defense against Indirect Prompt Injection Attacks: An Information Flow Control Perspective","","","","10.48550/arXiv.2409.19091","http://arxiv.org/abs/2409.19091","Large Language Model-based systems (LLM systems) are information and query processing systems that use LLMs to plan operations from natural-language prompts and feed the output of each successive step into the LLM to plan the next. This structure results in powerful tools that can process complex information from diverse sources but raises critical security concerns. Malicious information from any source may be processed by the LLM and can compromise the query processing, resulting in nearly arbitrary misbehavior. To tackle this problem, we present a system-level defense based on the principles of information flow control that we call an f-secure LLM system. An f-secure LLM system disaggregates the components of an LLM system into a context-aware pipeline with dynamically generated structured executable plans, and a security monitor filters out untrusted input into the planning process. This structure prevents compromise while maximizing flexibility. We provide formal models for both existing LLM systems and our f-secure LLM system, allowing analysis of critical security guarantees. We further evaluate case studies and benchmarks showing that f-secure LLM systems provide robust security while preserving functionality and efficiency. Our code is released at https://github.com/fzwark/Secure_LLM_System.","2024-10-10","2026-01-17 20:59:44","2026-01-17 20:59:44","2026-01-17 20:59:40","","","","","","","System-Level Defense against Indirect Prompt Injection Attacks","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2409.19091 [cs]","Comment: 23 pages","","","","Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2409.19091","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GS7SYWIX","preprint","2024","Xu, Jiacen; Stokes, Jack W.; McDonald, Geoff; Bai, Xuesong; Marshall, David; Wang, Siyue; Swaminathan, Adith; Li, Zhou","AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks","","","","10.48550/arXiv.2403.01038","http://arxiv.org/abs/2403.01038","Large language models (LLMs) have demonstrated impressive results on natural language tasks, and security researchers are beginning to employ them in both offensive and defensive systems. In cyber-security, there have been multiple research efforts that utilize LLMs focusing on the pre-breach stage of attacks like phishing and malware generation. However, so far there lacks a comprehensive study regarding whether LLM-based systems can be leveraged to simulate the post-breach stage of attacks that are typically human-operated, or ""hands-on-keyboard"" attacks, under various attack techniques and environments. As LLMs inevitably advance, they may be able to automate both the pre- and post-breach attack stages. This shift may transform organizational attacks from rare, expert-led events to frequent, automated operations requiring no expertise and executed at automation speed and scale. This risks fundamentally changing global computer security and correspondingly causing substantial economic impacts, and a goal of this work is to better understand these risks now so we can better prepare for these inevitable ever-more-capable LLMs on the horizon. On the immediate impact side, this research serves three purposes. First, an automated LLM-based, post-breach exploitation framework can help analysts quickly test and continually improve their organization's network security posture against previously unseen attacks. Second, an LLM-based penetration test system can extend the effectiveness of red teams with a limited number of human analysts. Finally, this research can help defensive systems and teams learn to detect novel attack behaviors preemptively before their use in the wild....","2024-03-02","2026-01-17 20:59:44","2026-01-17 20:59:44","2026-01-17 20:59:43","","","","","","","AutoAttacker","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2403.01038 [cs]","","","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2403.01038","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UB6WRLQW","preprint","2024","Niu, Zhenxing; Ren, Haodong; Gao, Xinbo; Hua, Gang; Jin, Rong","Jailbreaking Attack against Multimodal Large Language Model","","","","10.48550/arXiv.2402.02309","http://arxiv.org/abs/2402.02309","This paper focuses on jailbreaking attacks against multi-modal large language models (MLLMs), seeking to elicit MLLMs to generate objectionable responses to harmful user queries. A maximum likelihood-based algorithm is proposed to find an \emph{image Jailbreaking Prompt} (imgJP), enabling jailbreaks against MLLMs across multiple unseen prompts and images (i.e., data-universal property). Our approach exhibits strong model-transferability, as the generated imgJP can be transferred to jailbreak various models, including MiniGPT-v2, LLaVA, InstructBLIP, and mPLUG-Owl2, in a black-box manner. Moreover, we reveal a connection between MLLM-jailbreaks and LLM-jailbreaks. As a result, we introduce a construction-based method to harness our approach for LLM-jailbreaks, demonstrating greater efficiency than current state-of-the-art methods. The code is available here. \textbf{Warning: some content generated by language models may be offensive to some readers.}","2024-02-04","2026-01-17 20:59:44","2026-01-17 20:59:44","2026-01-17 20:59:44","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2402.02309 [cs]","","","","","Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Cryptography and Security; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2402.02309","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D8XUWBZJ","preprint","2025","Lian, Zhuotao; Wang, Weiyu; Zeng, Qingkui; Nakanishi, Toru; Kitasuka, Teruaki; Su, Chunhua","Prompt-in-Content Attacks: Exploiting Uploaded Inputs to Hijack LLM Behavior","","","","10.48550/arXiv.2508.19287","http://arxiv.org/abs/2508.19287","Large Language Models (LLMs) are widely deployed in applications that accept user-submitted content, such as uploaded documents or pasted text, for tasks like summarization and question answering. In this paper, we identify a new class of attacks, prompt in content injection, where adversarial instructions are embedded in seemingly benign inputs. When processed by the LLM, these hidden prompts can manipulate outputs without user awareness or system compromise, leading to biased summaries, fabricated claims, or misleading suggestions. We demonstrate the feasibility of such attacks across popular platforms, analyze their root causes including prompt concatenation and insufficient input isolation, and discuss mitigation strategies. Our findings reveal a subtle yet practical threat in real-world LLM workflows.","2025-08-25","2026-01-17 21:01:08","2026-01-17 21:18:57","2026-01-17 21:00:47","","","","","","","Prompt-in-Content Attacks","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2508.19287 [cs]","","/home/arthurbrito/Zotero/storage/SZ2HEL5C/Lian et al. - 2025 - Prompt-in-Content Attacks Exploiting Uploaded Inputs to Hijack LLM Behavior.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2508.19287","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EDSU2W7N","conferencePaper","2024","Muliarevych, Oleksandr","Mitigating input prompt attack vulnerabilities in systems with a language model interface","2024 14th International Conference on Dependable Systems, Services and Technologies (DESSERT)","","","","https://ieeexplore.ieee.org/abstract/document/11122258/","","2024","2026-01-17 21:01:08","2026-01-17 21:18:50","2026-01-17 21:00:50","1–8","","","","","","","","","","","IEEE","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/JCPIEK4E/Muliarevych - 2024 - Mitigating input prompt attack vulnerabilities in systems with a language model interface.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DTJLSEPF","preprint","2024","Chowdhury, Arijit Ghosh; Islam, Md Mofijul; Kumar, Vaibhav; Shezan, Faysal Hossain; Kumar, Vaibhav; Jain, Vinija; Chadha, Aman","Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models","","","","10.48550/arXiv.2403.04786","http://arxiv.org/abs/2403.04786","Large Language Models (LLMs) have become a cornerstone in the field of Natural Language Processing (NLP), offering transformative capabilities in understanding and generating human-like text. However, with their rising prominence, the security and vulnerability aspects of these models have garnered significant attention. This paper presents a comprehensive survey of the various forms of attacks targeting LLMs, discussing the nature and mechanisms of these attacks, their potential impacts, and current defense strategies. We delve into topics such as adversarial attacks that aim to manipulate model outputs, data poisoning that affects model training, and privacy concerns related to training data exploitation. The paper also explores the effectiveness of different attack methodologies, the resilience of LLMs against these attacks, and the implications for model integrity and user trust. By examining the latest research, we provide insights into the current landscape of LLM vulnerabilities and defense mechanisms. Our objective is to offer a nuanced understanding of LLM attacks, foster awareness within the AI community, and inspire robust solutions to mitigate these risks in future developments.","2024-03-23","2026-01-17 21:01:08","2026-01-17 21:17:45","2026-01-17 21:00:51","","","","","","","Breaking Down the Defenses","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2403.04786 [cs]","","/home/arthurbrito/Zotero/storage/XM9NCYMJ/Chowdhury et al. - 2024 - Breaking Down the Defenses A Comparative Survey of Attacks on Large Language Models.pdf","","","Computer Science - Computation and Language; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2403.04786","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"956F4M5S","conferencePaper","2024","Wan, Gwok-Waa; Wong, Sam-Zaak; Wang, Xi","Jailbreaking Pre-trained Large Language Models Towards Hardware Vulnerability Insertion Ability","Proceedings of the Great Lakes Symposium on VLSI 2024","979-8-4007-0605-9","","10.1145/3649476.3658799","https://dl.acm.org/doi/10.1145/3649476.3658799","","2024-06-12","2026-01-17 21:01:08","2026-01-17 21:18:29","2026-01-17 21:00:53","579-582","","","","","","","","","","","ACM","Clearwater FL USA","en","","","","","DOI.org (Crossref)","","","","/home/arthurbrito/Zotero/storage/NL6JEVSL/Wan et al. - 2024 - Jailbreaking Pre-trained Large Language Models Towards Hardware Vulnerability Insertion Ability.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","GLSVLSI '24: Great Lakes Symposium on VLSI 2024","","","","","","","","","","","","","","",""
"C4ADMV77","conferencePaper","2025","Donato, João","Benchmarking LLM Robustness Against Prompt-Based Adversarial Attacks","2025 20th European Dependable Computing Conference Companion Proceedings (EDCC-C)","","","","https://ieeexplore.ieee.org/abstract/document/11144805/","","2025","2026-01-17 21:01:08","2026-01-17 21:17:42","2026-01-17 21:00:54","60–63","","","","","","","","","","","IEEE","","","","","","","Google Scholar","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5DXJ3YCH","conferencePaper","2024","Deng, Gelei; Liu, Yi; Li, Yuekang; Wang, Kailong; Zhang, Ying; Li, Zefeng; Wang, Haoyu; Zhang, Tianwei; Liu, Yang","MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots","Proceedings 2024 Network and Distributed System Security Symposium","","","10.14722/ndss.2024.24188","http://arxiv.org/abs/2307.08715","Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI) services due to their exceptional proficiency in understanding and generating human-like text. LLM chatbots, in particular, have seen widespread adoption, transforming human-machine interactions. However, these LLM chatbots are susceptible to ""jailbreak"" attacks, where malicious users manipulate prompts to elicit inappropriate or sensitive responses, contravening service policies. Despite existing attempts to mitigate such threats, our research reveals a substantial gap in our understanding of these vulnerabilities, largely due to the undisclosed defensive measures implemented by LLM service providers. In this paper, we present Jailbreaker, a comprehensive framework that offers an in-depth understanding of jailbreak attacks and countermeasures. Our work makes a dual contribution. First, we propose an innovative methodology inspired by time-based SQL injection techniques to reverse-engineer the defensive strategies of prominent LLM chatbots, such as ChatGPT, Bard, and Bing Chat. This time-sensitive approach uncovers intricate details about these services' defenses, facilitating a proof-of-concept attack that successfully bypasses their mechanisms. Second, we introduce an automatic generation method for jailbreak prompts. Leveraging a fine-tuned LLM, we validate the potential of automated jailbreak generation across various commercial LLM chatbots. Our method achieves a promising average success rate of 21.58%, significantly outperforming the effectiveness of existing techniques. We have responsibly disclosed our findings to the concerned service providers, underscoring the urgent need for more robust defenses. Jailbreaker thus marks a significant step towards understanding and mitigating jailbreak threats in the realm of LLM chatbots.","2024","2026-01-17 21:01:08","2026-01-17 21:18:40","2026-01-17 21:00:56","","","","","","","MasterKey","","","","","","","","","","","","arXiv.org","","arXiv:2307.08715 [cs]","","/home/arthurbrito/Zotero/storage/M9BS59MV/Deng et al. - 2024 - MasterKey Automated Jailbreak Across Multiple Large Language Model Chatbots.pdf","","","Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CDGIBPWL","preprint","2024","Benjamin, Victoria; Braca, Emily; Carter, Israel; Kanchwala, Hafsa; Khojasteh, Nava; Landow, Charly; Luo, Yi; Ma, Caroline; Magarelli, Anna; Mirin, Rachel; Moyer, Avery; Simpson, Kayla; Skawinski, Amelia; Heverin, Thomas","Systematically Analyzing Prompt Injection Vulnerabilities in Diverse LLM Architectures","","","","10.48550/arXiv.2410.23308","http://arxiv.org/abs/2410.23308","This study systematically analyzes the vulnerability of 36 large language models (LLMs) to various prompt injection attacks, a technique that leverages carefully crafted prompts to elicit malicious LLM behavior. Across 144 prompt injection tests, we observed a strong correlation between model parameters and vulnerability, with statistical analyses, such as logistic regression and random forest feature analysis, indicating that parameter size and architecture significantly influence susceptibility. Results revealed that 56 percent of tests led to successful prompt injections, emphasizing widespread vulnerability across various parameter sizes, with clustering analysis identifying distinct vulnerability profiles associated with specific model configurations. Additionally, our analysis uncovered correlations between certain prompt injection techniques, suggesting potential overlaps in vulnerabilities. These findings underscore the urgent need for robust, multi-layered defenses in LLMs deployed across critical infrastructure and sensitive industries. Successful prompt injection attacks could result in severe consequences, including data breaches, unauthorized access, or misinformation. Future research should explore multilingual and multi-step defenses alongside adaptive mitigation strategies to strengthen LLM security in diverse, real-world environments.","2024-10-28","2026-01-17 21:01:08","2026-01-17 21:19:09","2026-01-17 21:00:58","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2410.23308 [cs]","","/home/arthurbrito/Zotero/storage/EK6WD6NX/Benjamin et al. - 2024 - Systematically Analyzing Prompt Injection Vulnerabilities in Diverse LLM Architectures.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Cryptography and Security; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2410.23308","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZLRPIBY7","journalArticle","2024","Zyryanova, I. N.; Chernavskiy, A. S.; Trubachev, S. O.","Prompt injection-the problem of linguistic vulnerabilities of large language models at the present stage","","","","","","","2024","2026-01-17 21:01:08","2026-01-17 21:18:55","","","","","","","","","","","","","","","","","","","","Google Scholar","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KX5M845Q","journalArticle","2025","Liu, Sheng; Sheng, Qiang; Wang, Danding; Li, Yang; Yang, Guang; Cao, Juan","Forewarned is forearmed: Pre-synthesizing jailbreak-like instructions to enhance llm safety guardrail to potential attacks","Preprint","","","","https://aclanthology.org/anthology-files/pdf/findings/2025.findings-emnlp.266.pdf","","2025","2026-01-17 21:01:08","2026-01-17 21:18:15","2026-01-17 21:01:06","","","","","","","Forewarned is forearmed","","","","","","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/E57CLQ5H/Liu et al. - 2025 - Forewarned is forearmed Pre-synthesizing jailbreak-like instructions to enhance llm safety guardrai.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BPU49INY","conferencePaper","2024","Ramakrishna, Anil; Majmudar, Jimit; Gupta, Rahul; Hazarika, Devamanyu","LLM-PIRATE: A benchmark for indirect prompt injection attacks in large language models","The Third Workshop on New Frontiers in Adversarial Machine Learning","","","","https://openreview.net/forum?id=qzEzXnw4ng","","2024","2026-01-17 21:04:32","2026-01-17 21:04:32","2026-01-17 21:04:16","","","","","","","LLM-PIRATE","","","","","","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/ETNMVBN5/Ramakrishna et al. - 2024 - LLM-PIRATE A benchmark for indirect prompt injection attacks in large language models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JS47YWSX","journalArticle","2025","Das, Badhan Chandra; Amini, M. Hadi; Wu, Yanzhao","Security and Privacy Challenges of Large Language Models: A Survey","ACM Computing Surveys","","0360-0300, 1557-7341","10.1145/3712001","https://dl.acm.org/doi/10.1145/3712001","Large language models (LLMs) have demonstrated extraordinary capabilities and contributed to multiple fields, such as generating and summarizing text, language translation, and question-answering. Today, LLMs have become quite popular tools in natural language processing tasks, with the capability to analyze complicated linguistic patterns and provide relevant responses depending on the context. While offering significant advantages, these models are also vulnerable to security and privacy attacks, such as jailbreaking attacks, data poisoning attacks, and personally identifiable information leakage attacks. This survey provides a thorough review of the security and privacy challenges of LLMs, along with the application-based risks in various domains, such as transportation, education, and healthcare. We assess the extent of LLM vulnerabilities, investigate emerging security and privacy attacks against LLMs, and review potential defense mechanisms. Additionally, the survey outlines existing research gaps and highlights future research directions.","2025-06-30","2026-01-17 21:04:32","2026-01-17 21:04:32","2026-01-17 21:04:17","1-39","","6","57","","ACM Comput. Surv.","Security and Privacy Challenges of Large Language Models","","","","","","","en","","","","","DOI.org (Crossref)","","","","/home/arthurbrito/Zotero/storage/EPLMNPGT/Das et al. - 2025 - Security and Privacy Challenges of Large Language Models A Survey.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GUUDX97Q","journalArticle","2025","Günay, Bengi","A Review of LLM Security: Threats and Mitigations","","","","","https://open.metu.edu.tr/handle/11511/113064","","2025","2026-01-17 21:04:32","2026-01-17 21:04:32","2026-01-17 21:04:18","","","","","","","A Review of LLM Security","","","","","Middle East Technical University","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/ZFMENMED/Günay - 2025 - A Review of LLM Security Threats and Mitigations.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2FGGRPJ8","preprint","2024","Luo, Weidi; Ma, Siyuan; Liu, Xiaogeng; Guo, Xiaoyu; Xiao, Chaowei","JailBreakV: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks","","","","10.48550/arXiv.2404.03027","http://arxiv.org/abs/2404.03027","With the rapid advancements in Multimodal Large Language Models (MLLMs), securing these models against malicious inputs while aligning them with human values has emerged as a critical challenge. In this paper, we investigate an important and unexplored question of whether techniques that successfully jailbreak Large Language Models (LLMs) can be equally effective in jailbreaking MLLMs. To explore this issue, we introduce JailBreakV-28K, a pioneering benchmark designed to assess the transferability of LLM jailbreak techniques to MLLMs, thereby evaluating the robustness of MLLMs against diverse jailbreak attacks. Utilizing a dataset of 2, 000 malicious queries that is also proposed in this paper, we generate 20, 000 text-based jailbreak prompts using advanced jailbreak attacks on LLMs, alongside 8, 000 image-based jailbreak inputs from recent MLLMs jailbreak attacks, our comprehensive dataset includes 28, 000 test cases across a spectrum of adversarial scenarios. Our evaluation of 10 open-source MLLMs reveals a notably high Attack Success Rate (ASR) for attacks transferred from LLMs, highlighting a critical vulnerability in MLLMs that stems from their text-processing capabilities. Our findings underscore the urgent need for future research to address alignment vulnerabilities in MLLMs from both textual and visual inputs.","2024-11-24","2026-01-17 21:04:32","2026-01-17 21:04:32","2026-01-17 21:04:19","","","","","","","JailBreakV","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2404.03027 [cs]","","/home/arthurbrito/Zotero/storage/TIAHRIGW/Luo et al. - 2024 - JailBreakV A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jai.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2404.03027","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BFESJT9V","conferencePaper","2024","Yu, Zhiyuan; Liu, Xiaogeng; Liang, Shunning; Cameron, Zach; Xiao, Chaowei; Zhang, Ning","Don't listen to me: Understanding and exploring jailbreak prompts of large language models","33rd USENIX Security Symposium (USENIX Security 24)","","","","https://www.usenix.org/conference/usenixsecurity24/presentation/yu-zhiyuan","","2024","2026-01-17 21:04:32","2026-01-17 21:04:32","2026-01-17 21:04:20","4675–4692","","","","","","Don't listen to me","","","","","","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/M7J4UDJD/Yu et al. - 2024 - Don't listen to me Understanding and exploring jailbreak prompts of large language models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SRYTLF2Y","preprint","2025","Abdali, Sara; Anarfi, Richard; Barberan, C. J.; He, Jia; Shayegani, Erfan","Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices","","","","10.48550/arXiv.2403.12503","http://arxiv.org/abs/2403.12503","Large language models (LLMs) have significantly transformed the landscape of Natural Language Processing (NLP). Their impact extends across a diverse spectrum of tasks, revolutionizing how we approach language understanding and generations. Nevertheless, alongside their remarkable utility, LLMs introduce critical security and risk considerations. These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities. This research paper thoroughly investigates security and privacy concerns related to LLMs from five thematic perspectives: security and privacy concerns, vulnerabilities against adversarial attacks, potential harms caused by misuses of LLMs, mitigation strategies to address these challenges while identifying limitations of current strategies. Lastly, the paper recommends promising avenues for future research to enhance the security and risk management of LLMs.","2025-06-11","2026-01-17 21:04:32","2026-01-17 21:04:32","2026-01-17 21:04:21","","","","","","","Securing Large Language Models","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2403.12503 [cs]","","/home/arthurbrito/Zotero/storage/4B6B98RU/Abdali et al. - 2025 - Securing Large Language Models Threats, Vulnerabilities and Responsible Practices.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2403.12503","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T2XSPRUU","conferencePaper","2025","Cheng, Wen; Sun, Ke; Zhang, Xinyu; Wang, Wei","Security attacks on llm-based code completion tools","Proceedings of the AAAI Conference on Artificial Intelligence","","","","https://ojs.aaai.org/index.php/AAAI/article/view/34537","","2025","2026-01-17 21:04:32","2026-01-17 21:04:32","2026-01-17 21:04:27","23669–23677","","22","39","","","","","","","","","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/XC2822HV/Cheng et al. - 2025 - Security attacks on llm-based code completion tools.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EU8CMB4W","preprint","2025","Lin, Liang; Xu, Zhihao; Tang, Xuehai; Liu, Shi; Zhou, Biyu; Zhu, Fuqing; Han, Jizhong; Hu, Songlin","Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers","","","","10.48550/arXiv.2507.13474","http://arxiv.org/abs/2507.13474","The safety of large language models (LLMs) has garnered significant research attention. In this paper, we argue that previous empirical studies demonstrate LLMs exhibit a propensity to trust information from authoritative sources, such as academic papers, implying new possible vulnerabilities. To verify this possibility, a preliminary analysis is designed to illustrate our two findings. Based on this insight, a novel jailbreaking method, Paper Summary Attack (\llmname{PSA}), is proposed. It systematically synthesizes content from either attack-focused or defense-focused LLM safety paper to construct an adversarial prompt template, while strategically infilling harmful query as adversarial payloads within predefined subsections. Extensive experiments show significant vulnerabilities not only in base LLMs, but also in state-of-the-art reasoning model like Deepseek-R1. PSA achieves a 97\% attack success rate (ASR) on well-aligned models like Claude3.5-Sonnet and an even higher 98\% ASR on Deepseek-R1. More intriguingly, our work has further revealed diametrically opposed vulnerability bias across different base models, and even between different versions of the same model, when exposed to either attack-focused or defense-focused papers. This phenomenon potentially indicates future research clues for both adversarial methodologies and safety alignment.Code is available at https://github.com/233liang/Paper-Summary-Attack","2025-07-17","2026-01-17 21:04:32","2026-01-17 21:04:32","2026-01-17 21:04:29","","","","","","","Paper Summary Attack","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2507.13474 [cs]","","/home/arthurbrito/Zotero/storage/7TFLDNSH/Lin et al. - 2025 - Paper Summary Attack Jailbreaking LLMs through LLM Safety Papers.pdf","","","Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2507.13474","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MSUP5XZX","conferencePaper","2024","Zhang, Chong; Jin, Mingyu; Yu, Qinkai; Liu, Chengzhi; Xue, Haochen; Jin, Xiaobo","Goal-guided generative prompt injection attack on large language models","2024 IEEE International Conference on Data Mining (ICDM)","","","","https://ieeexplore.ieee.org/abstract/document/10884369/","","2024","2026-01-17 21:04:32","2026-01-17 21:04:32","2026-01-17 21:04:30","941–946","","","","","","","","","","","IEEE","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/BN48YVGN/Zhang et al. - 2024 - Goal-guided generative prompt injection attack on large language models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CY4R82W6","conferencePaper","2025","Wang, Hao; Li, Hao; Zhu, Junda; Wang, Xinyuan; Pan, Chengwei; Huang, MinLie; Sha, Lei","Diffusionattacker: Diffusion-driven prompt manipulation for llm jailbreak","Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing","","","","https://aclanthology.org/2025.emnlp-main.1128/","","2025","2026-01-17 21:04:32","2026-01-17 21:04:32","2026-01-17 21:04:32","22193–22205","","","","","","Diffusionattacker","","","","","","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/WGNG79FT/Wang et al. - 2025 - Diffusionattacker Diffusion-driven prompt manipulation for llm jailbreak.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CU9EWWHK","preprint","2025","Maloyan, Narek; Namiot, Dmitry","Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections","","","","10.48550/arXiv.2504.18333","http://arxiv.org/abs/2504.18333","LLM as judge systems used to assess text quality code correctness and argument strength are vulnerable to prompt injection attacks. We introduce a framework that separates content author attacks from system prompt attacks and evaluate five models Gemma 3.27B Gemma 3.4B Llama 3.2 3B GPT 4 and Claude 3 Opus on four tasks with various defenses using fifty prompts per condition. Attacks achieved up to seventy three point eight percent success smaller models proved more vulnerable and transferability ranged from fifty point five to sixty two point six percent. Our results contrast with Universal Prompt Injection and AdvPrompter We recommend multi model committees and comparative scoring and release all code and datasets","2025-04-25","2026-01-17 21:05:41","2026-01-17 21:17:26","2026-01-17 21:05:23","","","","","","","Adversarial Attacks on LLM-as-a-Judge Systems","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2504.18333 [cs]","","/home/arthurbrito/Zotero/storage/5KPSFXL4/Maloyan e Namiot - 2025 - Adversarial Attacks on LLM-as-a-Judge Systems Insights from Prompt Injections.pdf","","","Computer Science - Computation and Language; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2504.18333","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ASKSLNXI","preprint","2025","Lee, Youngjoon; Park, Taehyun; Lee, Yunho; Gong, Jinu; Kang, Joonhyuk","Exploring Potential Prompt Injection Attacks in Federated Military LLMs and Their Mitigation","","","","10.48550/arXiv.2501.18416","http://arxiv.org/abs/2501.18416","Federated Learning (FL) is increasingly being adopted in military collaborations to develop Large Language Models (LLMs) while preserving data sovereignty. However, prompt injection attacks-malicious manipulations of input prompts-pose new threats that may undermine operational security, disrupt decision-making, and erode trust among allies. This perspective paper highlights four vulnerabilities in federated military LLMs: secret data leakage, free-rider exploitation, system disruption, and misinformation spread. To address these risks, we propose a human-AI collaborative framework with both technical and policy countermeasures. On the technical side, our framework uses red/blue team wargaming and quality assurance to detect and mitigate adversarial behaviors of shared LLM weights. On the policy side, it promotes joint AI-human policy development and verification of security protocols.","2025-11-23","2026-01-17 21:05:41","2026-01-17 21:18:07","2026-01-17 21:05:26","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2501.18416 [cs]","Comment: Accepted to the 3rd International Workshop on Dataspaces and Digital Twins for Critical Entities and Smart Urban Communities - IEEE BigData 2025; Comment: Accepted to the 3rd International Workshop on Dataspaces and Digital Twins for Critical Entities and Smart Urban Communities - IEEE BigData 2025","/home/arthurbrito/Zotero/storage/2GJPP9EW/Lee et al. - 2025 - Exploring Potential Prompt Injection Attacks in Federated Military LLMs and Their Mitigation.pdf","","","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2501.18416","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FYP7LTR5","preprint","2025","Zhang, Xiaoyu; Zhang, Cen; Li, Tianlin; Huang, Yihao; Jia, Xiaojun; Hu, Ming; Zhang, Jie; Liu, Yang; Ma, Shiqing; Shen, Chao","JailGuard: A Universal Detection Framework for LLM Prompt-based Attacks","","","","10.48550/arXiv.2312.10766","http://arxiv.org/abs/2312.10766","The systems and software powered by Large Language Models (LLMs) and Multi-Modal LLMs (MLLMs) have played a critical role in numerous scenarios. However, current LLM systems are vulnerable to prompt-based attacks, with jailbreaking attacks enabling the LLM system to generate harmful content, while hijacking attacks manipulate the LLM system to perform attacker-desired tasks, underscoring the necessity for detection tools. Unfortunately, existing detecting approaches are usually tailored to specific attacks, resulting in poor generalization in detecting various attacks across different modalities. To address it, we propose JailGuard, a universal detection framework deployed on top of LLM systems for prompt-based attacks across text and image modalities. JailGuard operates on the principle that attacks are inherently less robust than benign ones. Specifically, JailGuard mutates untrusted inputs to generate variants and leverages the discrepancy of the variants' responses on the target model to distinguish attack samples from benign samples. We implement 18 mutators for text and image inputs and design a mutator combination policy to further improve detection generalization. The evaluation on the dataset containing 15 known attack types suggests that JailGuard achieves the best detection accuracy of 86.14%/82.90% on text and image inputs, outperforming state-of-the-art methods by 11.81%-25.73% and 12.20%-21.40%.","2025-03-15","2026-01-17 21:05:41","2026-01-17 21:18:33","2026-01-17 21:05:28","","","","","","","JailGuard","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2312.10766 [cs]","Comment: 40 pages, 12 figures; Comment: 40 pages, 12 figures","/home/arthurbrito/Zotero/storage/8SPSCJWF/Zhang et al. - 2025 - JailGuard A Universal Detection Framework for LLM Prompt-based Attacks.pdf","","","Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2312.10766","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ETM4PFY5","conferencePaper","2024","Yan, Jun; Yadav, Vikas; Li, Shiyang; Chen, Lichang; Tang, Zheng; Wang, Hai; Srinivasan, Vijay; Ren, Xiang; Jin, Hongxia","Backdooring instruction-tuned large language models with virtual prompt injection","Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)","","","","https://aclanthology.org/2024.naacl-long.337/","","2024","2026-01-17 21:05:41","2026-01-17 21:17:35","2026-01-17 21:05:29","6065–6086","","","","","","","","","","","","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/IRCNKZTG/Yan et al. - 2024 - Backdooring instruction-tuned large language models with virtual prompt injection.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TPMVQ9KU","conferencePaper","2025","Ramesh, Aditya; Bhardwaj, Shivam; Saibewar, Aditya; Kaul, Manohar","Efficient jailbreak attack sequences on large language models via multi-armed bandit-based context switching","The Thirteenth International Conference on Learning Representations","","","","https://openreview.net/forum?id=jCDF7G3LpF","","2025","2026-01-17 21:05:41","2026-01-17 21:18:01","2026-01-17 21:05:35","","","","","","","","","","","","","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/GDDMRTNB/Ramesh et al. - 2025 - Efficient jailbreak attack sequences on large language models via multi-armed bandit-based context s.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TIGN5QMD","preprint","2025","Pedro, Rodrigo; Castro, Daniel; Carreira, Paulo; Santos, Nuno","From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?","","","","10.48550/arXiv.2308.01990","http://arxiv.org/abs/2308.01990","Large Language Models (LLMs) have found widespread applications in various domains, including web applications, where they facilitate human interaction via chatbots with natural language interfaces. Internally, aided by an LLM-integration middleware such as Langchain, user prompts are translated into SQL queries used by the LLM to provide meaningful responses to users. However, unsanitized user prompts can lead to SQL injection attacks, potentially compromising the security of the database. Despite the growing interest in prompt injection vulnerabilities targeting LLMs, the specific risks of generating SQL injection attacks through prompt injections have not been extensively studied. In this paper, we present a comprehensive examination of prompt-to-SQL (P$_2$SQL) injections targeting web applications based on the Langchain framework. Using Langchain as our case study, we characterize P$_2$SQL injections, exploring their variants and impact on application security through multiple concrete examples. Furthermore, we evaluate 7 state-of-the-art LLMs, demonstrating the pervasiveness of P$_2$SQL attacks across language models. Our findings indicate that LLM-integrated applications based on Langchain are highly susceptible to P$_2$SQL injection attacks, warranting the adoption of robust defenses. To counter these attacks, we propose four effective defense techniques that can be integrated as extensions to the Langchain framework. We validate the defenses through an experimental evaluation with a real-world use case application.","2025-01-27","2026-01-17 21:05:41","2026-01-17 21:18:18","2026-01-17 21:05:36","","","","","","","From Prompt Injections to SQL Injection Attacks","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2308.01990 [cs]","Comment: 12 pages, 3 figures, 3 tables, 5 listings. 47th IEEE/ACM International Conference on Software Engineering (2025); Comment: 12 pages, 3 figures, 3 tables, 5 listings. 47th IEEE/ACM International Conference on Software Engineering (2025)","/home/arthurbrito/Zotero/storage/QH5CMSVC/Pedro et al. - 2025 - From Prompt Injections to SQL Injection Attacks How Protected is Your LLM-Integrated Web Applicatio.pdf","","","Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2308.01990","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IZN7KHV3","journalArticle","2025","Zhang, Shenyi; Zhai, Yuchen; Guo, Keyan; Hu, Hongxin; Guo, Shengnan; Fang, Zheng; Zhao, Lingchen; Shen, Chao; Wang, Cong; Wang, Qian","Jbshield: Defending large language models from jailbreak attacks through activated concept analysis and manipulation","arXiv preprint arXiv:2502.07557","","","","https://www.usenix.org/system/files/conference/usenixsecurity25/sec25cycle1-prepub-341-zhang-shenyi.pdf","","2025","2026-01-17 21:05:41","2026-01-17 21:18:36","2026-01-17 21:05:37","","","","","","","Jbshield","","","","","","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/5F38YRFG/Zhang et al. - 2025 - Jbshield Defending large language models from jailbreak attacks through activated concept analysis.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N7J5F557","preprint","2025","Andriushchenko, Maksym; Croce, Francesco; Flammarion, Nicolas","Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks","","","","10.48550/arXiv.2404.02151","http://arxiv.org/abs/2404.02151","We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize a target logprob (e.g., of the token ""Sure""), potentially with multiple restarts. In this way, we achieve 100% attack success rate -- according to GPT-4 as a judge -- on Vicuna-13B, Mistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B, Llama-3-Instruct-8B, Gemma-7B, GPT-3.5, GPT-4o, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with a 100% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many similarities with jailbreaking -- which is the algorithm that brought us the first place in the SaTML'24 Trojan Detection Competition. The common theme behind these attacks is that adaptivity is crucial: different models are vulnerable to different prompting templates (e.g., R2D2 is very sensitive to in-context learning prompts), some models have unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and in some settings, it is crucial to restrict the token search space based on prior knowledge (e.g., for trojan detection). For reproducibility purposes, we provide the code, logs, and jailbreak artifacts in the JailbreakBench format at https://github.com/tml-epfl/llm-adaptive-attacks.","2025-04-17","2026-01-17 21:05:41","2026-01-17 21:18:26","2026-01-17 21:05:38","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2404.02151 [cs]","Comment: Accepted at ICLR 2025. Updates in the v3: GPT-4o and Claude 3.5 Sonnet results, improved writing. Updates in the v2: more models (Llama3, Phi-3, Nemotron-4-340B), jailbreak artifacts for all attacks are available, evaluation with different judges (Llama-3-70B and Llama Guard 2), more experiments (convergence plots, ablation on the suffix length for random search), examples of jailbroken generation; Comment: Accepted at ICLR 2025. Updates in the v3: GPT-4o and Claude 3.5 Sonnet results, improved writing. Updates in the v2: more models (Llama3, Phi-3, Nemotron-4-340B), jailbreak artifacts for all attacks are available, evaluation with different judges (Llama-3-70B and Llama Guard 2), more experiments (convergence plots, ablation on the suffix length for random search), examples of jailbroken generation","/home/arthurbrito/Zotero/storage/UJLUKD5B/Andriushchenko et al. - 2025 - Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2404.02151","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UTUDHIXP","journalArticle","2024","Xu, Zhao; Liu, Fan; Liu, Hao","Bag of tricks: Benchmarking of jailbreak attacks on llms","Advances in Neural Information Processing Systems","","","","https://proceedings.neurips.cc/paper_files/paper/2024/hash/38c1dfb4f7625907b15e9515365e7803-Abstract-Datasets_and_Benchmarks_Track.html","","2024","2026-01-17 21:05:41","2026-01-17 21:17:39","2026-01-17 21:05:39","32219–32250","","","37","","","Bag of tricks","","","","","","","","","","","","Google Scholar","","","","/home/arthurbrito/Zotero/storage/BGVBFEXG/Xu et al. - 2024 - Bag of tricks Benchmarking of jailbreak attacks on llms.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CBAZEQ52","preprint","2025","Wang, Zhilong; Nagaraja, Neha; Zhang, Lan; Bahsi, Hayretdin; Patil, Pawan; Liu, Peng","To Protect the LLM Agent Against the Prompt Injection Attack with Polymorphic Prompt","","","","10.48550/arXiv.2506.05739","http://arxiv.org/abs/2506.05739","LLM agents are widely used as agents for customer support, content generation, and code assistance. However, they are vulnerable to prompt injection attacks, where adversarial inputs manipulate the model's behavior. Traditional defenses like input sanitization, guard models, and guardrails are either cumbersome or ineffective. In this paper, we propose a novel, lightweight defense mechanism called Polymorphic Prompt Assembling (PPA), which protects against prompt injection with near-zero overhead. The approach is based on the insight that prompt injection requires guessing and breaking the structure of the system prompt. By dynamically varying the structure of system prompts, PPA prevents attackers from predicting the prompt structure, thereby enhancing security without compromising performance. We conducted experiments to evaluate the effectiveness of PPA against existing attacks and compared it with other defense methods.","2025-06-06","2026-01-17 21:05:41","2026-01-17 21:19:11","2026-01-17 21:05:41","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2506.05739 [cs]","Comment: To appear in the Industry Track of the 55th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2025); Comment: To appear in the Industry Track of the 55th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2025)","/home/arthurbrito/Zotero/storage/S4GFERH6/Wang et al. - 2025 - To Protect the LLM Agent Against the Prompt Injection Attack with Polymorphic Prompt.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:2506.05739","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UX7HFNCA","conferencePaper","2025","ElSaify, Basma; Baderelden, Mohamed","Adversarial and Multilingual Threats in Retrieval-Augmented Generation: From Prompt Injection to Model Exploitation","2025 2nd International Generative AI and Computational Language Modelling Conference (GACLM)","","","10.1109/GACLM67198.2025.11231998","https://ieeexplore.ieee.org/document/11231998/","Retrieval-Augmented Generation (RAG) systems have emerged as a cornerstone of modern generative AI by coupling large language models (LLMs) with external knowledge retrieval, thereby enabling more grounded, accurate, and context-aware outputs. Yet this paradigm simultaneously broadens the attack surface—extending vulnerabilities from generation into retrieval pipelines, language interfaces, and exposed APIs. In this paper, we propose a comprehensive threat taxonomy and adversarial framework targeting both conventional and agentic RAG systems, in which LLMs autonomously plan and execute multi-step tasks using retrieved knowledge. Our analysis spans a wide spectrum of adversarial techniques, including prompt injection, jailbreak exploits, API-driven model extraction, and multilingual prompt manipulation. Unlike prior work that primarily focuses on English-only vulnerabilities, we show that multilingual RAG pipelines—particularly those involving language blending (e.g., Arabic-English code-switching)—introduce novel attack vectors that bypass safety filters and induce unintended behaviors. To study these risks, we develop a multilingual adversarial benchmark suite that integrates both synthetic and real-world corpora across English, Arabic, and Spanish. Through systematic evaluation, we demonstrate how the retrieval layer not only amplifies leakage and inversion risks but also enables adversaries to infer system logic, extract private data from vector stores, and manipulate autonomous agent behavior through tool-integrated APIs. To mitigate these threats, we assess multiple defense strategies, including prompt sanitization, cross-lingual content filtering, retriever-stage anomaly detection, and behavior-based profiling of API interactions. Our findings reveal that existing safeguards are insufficient—particularly in agentic or low-resource language scenarios—underscoring the need for architecture-specific and multilingual defenses. This work fills a critical gap in GenAI security by presenting the first holistic, multilingual, and agent-aware security framework for RAG, offering actionable guidance for building robust and trustworthy generative AI applications across diverse linguistic and deployment contexts.","2025-08","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:10:00","155-162","","","","","","Adversarial and Multilingual Threats in Retrieval-Augmented Generation","","","","","","","","","","","","IEEE Xplore","","","","","","","Agentic AI; API Abuse; Computational modeling; Context modeling; Contextual Prompt Injection; Filters; Generative AI; Generative AI Security; Jailbreak Attacks; Large language models; Large Language Models (LLMs); LLM Systems Defense Strategies; Model Extraction; Multilingual; Multilingual NLP; Multilingual Prompt Exploits; Pipelines; Prompt Injection; Retrieval augmented generation; Retrieval-Augmented Generation (RAG); Secure Generative AI; Security; Trustworthy Generative AI and LLM Systems; Vectors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2025 2nd International Generative AI and Computational Language Modelling Conference (GACLM)","","","","","","","","","","","","","","",""
"SYPG7SBR","conferencePaper","2024","Omri, Sihem; Abdelkader, Manel; Hamdi, Mohamed","MalPID: Malicious Prompt Injection Detection Dataset for Large Language Model based Applications","2024 IEEE Eleventh International Conference on Communications and Networking (ComNet)","","2473-7585","10.1109/ComNet64071.2024.10987374","https://ieeexplore.ieee.org/document/10987374/","Despite the significant transformation made by large language model (LLM)-based chatbots in the field of conversational artificial intelligence (AI), these systems are vulnerable to the attack known as prompt injection by malicious users to make them ignore their guardrails and generate objectionable content. However, few previous works have been addressing this issue using complex and costly mechanisms and there is a lack of large dataset that deal with prompt injection examples. In this work, we introduce MalPID, a novel dataset for malicious prompt injection detection. This benchmark contains various malicious and legitimate prompts collected from different sources and labelled manually. Our systematic evaluation of models trained on our dataset has shown impressive results in detecting prompt injection data. In the future, MalPID could be a valuable resource for advancing the creation of safe conversational AI systems.","2024-11","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:10:03","1-5","","","","","","MalPID","","","","","","","","","","","","IEEE Xplore","","ISSN: 2473-7585","","","","","AI application; Benchmark testing; chatbot; Chatbots; classification; conversational AI; Data models; dataset; Ethics; Large language models; LLM; Machine learning; Prevention and mitigation; prompt injection; Systematics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE Eleventh International Conference on Communications and Networking (ComNet)","","","","","","","","","","","","","","",""
"BZQCXUIB","conferencePaper","2024","Chernyshev, Maxim; Baig, Zubair; Doss, Robin","[Short Paper] Forensic Analysis of Indirect Prompt Injection Attacks on LLM Agents","2024 IEEE 6th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)","","","10.1109/TPS-ISA62245.2024.00053","https://ieeexplore.ieee.org/document/10835636/","Large language model (LLM) agents are vulnerable to a range of evolving attacks including Indirect Prompt Injection (IPI). Digital investigations involving IPI attacks on LLM agents are challenging due to growing data volumes and agent flow complexity. We introduce a novel approach for forensic analysis of LLM agent execution logs to identify IPI attacks. We implement a custom digital forensic analysis framework featuring a realistic agentic task benchmark to generate agent logs, for subsequent analysis using 12 state-of-the-art LLMs. Our preliminary results show promising potential for malicious trail detection, with varying performance across domain-specific test suites. We discuss key findings, limitations, and future work directions to support the empirical evaluation of LLMs for digital forensic applications.","2024-10","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:10:08","409-411","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","","","","agent; Benchmark testing; Complexity theory; digital forensics; Digital forensics; indirect prompt injection; Intelligent systems; Large language models; LLM; Privacy; Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE 6th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)","","","","","","","","","","","","","","",""
"YVFS9QWJ","conferencePaper","2025","Wang, Zhilong; Nagaraja, Neha; Zhang, Lan; Bahsi, Hayretdin; Patil, Pawan; Liu, Peng","To Protect the LLM Agent Against the Prompt Injection Attack with Polymorphic Prompt","2025 55th Annual IEEE/IFIP International Conference on Dependable Systems and Networks - Supplemental Volume (DSN-S)","","2833-292X","10.1109/DSN-S65789.2025.00037","https://ieeexplore.ieee.org/document/11068353/","LLM agents are widely used as agents for customer support, content generation, and code assistance. However, they are vulnerable to prompt injection attacks, where adversarial inputs manipulate the model’s behavior. Traditional defenses like input sanitization, guard models, and guardrails are either cumbersome or ineffective. In this paper, we propose a novel, lightweight defense mechanism called Polymorphic Prompt Assembling (PPA), which protects against prompt injection with near-zero overhead. The approach is based on the insight that prompt injection requires guessing and breaking the structure of the system prompt. By dynamically varying the structure of system prompts, PPA prevents attackers from predicting the prompt structure, thereby enhancing security without compromising performance. We conducted experiments to evaluate the effectiveness of PPA against existing attacks and compared it with other defense methods.","2025-06","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:10:12","22-28","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2833-292X","","","","","Adaptation models; Codes; Degradation; Genetic algorithms; LLM; Manuals; Multi-agent systems; Particle separators; Prompt Injection; Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2025 55th Annual IEEE/IFIP International Conference on Dependable Systems and Networks - Supplemental Volume (DSN-S)","","","","","","","","","","","","","","",""
"EWIXIMRT","conferencePaper","2025","Kim, Kyungho; Seo, Jaejin; Park, Seongmin; Lee, Jihwa","Prompt Crossing: Evaluating Whether LLM Response Stem from Jailbreak or Normal Prompt","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2379-190X","10.1109/ICASSP49660.2025.10889949","https://ieeexplore.ieee.org/document/10889949/","The evolution of Large Language Models (LLMs) has sparked growing concerns about jailbreak, crafted prompts that bypass safety guardrails and lead to the generation of harmful information. While recent research primarily focuses on identifying harmful content within LLM outputs, limited attention has been paid to detecting jailbreak intent directly from user prompts. This paper proposes a novel framework, called Prompt-Crossing, for analyzing user prompts and identifying those harboring potential jailbreak intent based on the response of LLM. Moreover, while existing methods determine jailbreak intent by looking at whether the response of LLM contains harmful information, our proposed framework can determine whether the user prompt, triggering the response of LLM, has jailbreak intent even if the response does not contain harmful information. In addition, our method performs filtering based on the likelihood calculated in a forward-step manner, deviates from traditional inference-based LLM usage, demonstrating a new application for LLMs. To demonstrate our proposed framework, we achieve state-of-the-art on the AdvBench benchmark using two LLMs, LLAMA and Falcon.","2025-04","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:10:16","1-5","","","","","","Prompt Crossing","","","","","","","","","","","","IEEE Xplore","","ISSN: 2379-190X","","","","","Acoustics; Benchmark testing; Filtering; Filters; Guidelines; Jailbreak; Large language models; LLM; Probability; Safety; Scalability; Speech processing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","","","","","","","","","","","","","",""
"LH4J6LZ2","conferencePaper","2024","Chen, Qiuyu; Yamaguchi, Shingo; Yamamoto, Yudai","Defending Against GCG Jailbreak Attacks with Syntax Trees and Perplexity in LLMs","2024 IEEE 13th Global Conference on Consumer Electronics (GCCE)","","2693-0854","10.1109/GCCE62371.2024.10760963","https://ieeexplore.ieee.org/document/10760963/","In this paper, we propose a novel classification method that utilizes syntax trees and perplexity to identify jailbreak attacks that use hostile suffixes to make large language models (LLMs) more likely to generate dangerous content, such as Greedy Coordinate Gradient (GCG) attacks. GCG jailbreak attacks deceive the model with hostile suffixes, prompting LLMs to produce responses that exceed ethical limits. To address this issue, we propose a classifier that can confirm whether the input to the LLM contains a suffix indicative of a GCG jailbreak attack. The STPC (Syntax Trees and Perplexity Classifier) introduces syntax trees to analyze the structural characteristics of GCG suffixes and combines perplexity for comprehensive classification judgment. It accurately detects most jailbreak attacks in the test set, mitigates false positives, and improves the reliability of LLM responses. In all STPC test results, the classification accuracy reached 96.2%.","2024-10","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:10:19","1411-1415","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2693-0854","","","","","Accuracy; Consumer electronics; Ethics; GCG; jailbreak attack; large language model; Large language models; perplexity; Random forests; Reliability; Syntactics; syntax tree","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE 13th Global Conference on Consumer Electronics (GCCE)","","","","","","","","","","","","","","",""
"HV6XQ6MA","conferencePaper","2023","Yip, Daniel Wankit; Esmradi, Aysan; Chan, Chun Fai","A Novel Evaluation Framework for Assessing Resilience Against Prompt Injection Attacks in Large Language Models","2023 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)","","","10.1109/CSDE59766.2023.10487667","https://ieeexplore.ieee.org/document/10487667/","Prompt injection attacks exploit vulnerabilities in large language models (LLMs) to manipulate the model into unintended actions or generate malicious content. As LLM-integrated applications gain wider adoption, they face growing susceptibility to such attacks. This study introduces a novel evaluation framework for quantifying the resilience of applications. The framework incorporates innovative techniques designed to ensure representativeness, interpretability, and robustness. To ensure the representativeness of simulated attacks on the application, a meticulous selection process was employed, resulting in 115 carefully chosen attacks based on coverage and relevance. For enhanced interpretability, a second LLM was utilized to evaluate the responses generated from these simulated attacks. Unlike conventional malicious content classifiers that provide only a confidence score, the LLM-based evaluation produces a score accompanied by an explanation, thereby enhancing interpretability. Subsequently, a resilience score is computed by assigning higher weights to attacks with greater impact, thus providing a robust measurement of the application's resilience. To assess the framework's efficacy, it was applied on two LLMs, namely Llama2 and ChatGLM. Results revealed that Llama2, the newer model exhibited higher resilience compared to ChatGLM. This finding substantiates the effectiveness of the framework, aligning with the prevailing notion that newer models tend to possess greater resilience. Moreover, the framework exhibited exceptional versatility, requiring only minimal adjustments to accommodate emerging attack techniques and classifications, thereby establishing itself as an effective and practical solution. Overall, the framework offers valuable insights that empower organizations to make well-informed decisions to fortify their applications against potential threats from prompt injection.","2023-12","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:10:23","1-5","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","/home/arthurbrito/Zotero/storage/CPAHTQ3G/Yip et al. - 2023 - A Novel Evaluation Framework for Assessing Resilience Against Prompt Injection Attacks in Large Lang.pdf","","","Computational modeling; Cyber Security; Data engineering; Data models; Large Language Model; Organizations; Prompt Injection; Robustness; Software architecture; Weight measurement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2023 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)","","","","","","","","","","","","","","",""
"LXZ7YBNH","conferencePaper","2025","Dubey, Shreyash R; Lamkuche, Hemraj Shobharam","Breaking the Shield: Adversarial Jailbreak Attacks and Defense Mechanisms in Large Language Models","2025 International Conference on Emerging Information Technology and Engineering Solutions (EITES)","","","10.1109/EITES66543.2025.00023","https://ieeexplore.ieee.org/document/11206147/","Large Language Models (LLMs) are increasingly used across diverse applications, raising security concerns, especially regarding jail-break attacks that bypass safety measures. This paper presents a systematic security evaluation of LLMs, exploring the effectiveness of jail-break detection methods, the security progression across model versions, the relationship between model size and vulnerability, and the impact of combined defense strategies. We evaluate both open-source (e.g., LLama, Mistral) and proprietary models (e.g., GPT-4) using four state-of-the-art attack techniques and three novel defenses. Our findings offer insights into the trade-offs between performance and safety in LLM deployment.","2025-07","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:10:27","82-86","","","","","","Breaking the Shield","","","","","","","","","","","","IEEE Xplore","","","","","","","hybrid defenses; Information technology; jailbreak attacks; Large language models; LLM security; model robustness; prompt injection; Robustness; Safety; Security; Systematics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2025 International Conference on Emerging Information Technology and Engineering Solutions (EITES)","","","","","","","","","","","","","","",""
"2VV4DMD7","conferencePaper","2025","Goh, Adison; Chee, Benjamin; Vagnoli, Matteo; Baldassarre, Luca; Narayan, Akshay","Scaling Responsible Generative AI: Automating Red Teaming of LLM Applications","2025 IEEE Conference on Artificial Intelligence (CAI)","","","10.1109/CAI64502.2025.00159","https://ieeexplore.ieee.org/document/11050586/","Large Language Models (LLMs) present both significant business potential and substantial risks. This paper addresses the critical need for robust and scalable red teaming processes to identify and mitigate risks before LLM solution deployment. First, we define a comprehensive set of 48 LLM-associated risks reflecting emerging AI threats. Additionally, we introduce an automated adversarial prompt generation pipeline involving five types of generators that cover diverse AI risks at varying complexity levels. Finally, we implement a LLM-as-a-judge evaluation system to streamline testing. When applied to red-team two finance-based LLM applications, our approach achieved perfect recall in identifying failed outputs while reducing manual evaluation by over 90%. The developed features halved the time required for red teaming exercises, enhancing scalability and thoroughness while maintaining effectiveness. This work enhances LLM safety, accelerates deployment, and adds business value through improved risk management and responsible AI use.","2025-05","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:10:30","902-905","","","","","","Scaling Responsible Generative AI","","","","","","","","","","","","IEEE Xplore","","","","","","","Adversarial testing; AI Safety; Business; Generative AI; Large language models; Large Language Models; Manuals; Pipelines; Red Teaming; Reliability; Responsible AI; Risk mitigation; Safety; Scalability; Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2025 IEEE Conference on Artificial Intelligence (CAI)","","","","","","","","","","","","","","",""
"S7CHP9NG","conferencePaper","2025","Joshi, Tanaya; Naik, Vaishnavi; Mistry, Isha; Mangrulkar, Ramchandra","Prevention of Prompt Injection Attacks Over Financial Applications Integrated with LLM","2025 3rd International Conference on Advancement in Computation & Computer Technologies (InCACCT)","","","10.1109/InCACCT65424.2025.11011372","https://ieeexplore.ieee.org/document/11011372/","Prompt injection is a growing concern in financial applications integrated with Large Language Models. These attacks pose a critical risk to financial applications leading to data breaches, confidential data loss and financial losses by manipulating input prompts to cause unintended or harmful outputs. Existing defense strategies include utilizing filters f or input and output, and delimiters, however, these techniques have been proven inadequate. This paper builds upon existing mechanisms and proposes an enhanced security system that incorporates role-based access, jailbreak attack detection, validating inputs and securely passing prompts to LLM. This is done by developing a pre-processing layer at both ends-the client and LLMs-through the identification of critical as well as destructive keywords. The experimental results show that blocking of that malicious prompt ensures that only authenticated and authorized commands are processed. The measures, therefore, allow financial institutions to significantly reduce the probability of prompt injection attacks besides increasing data protection and confidence i n AI-based financial applications.","2025-04","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:10:33","225-230","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","","","","Authorization; Computational modeling; Computer security; Data breach; Data protection; Filters; Finance; Jailbreak; Large language models; LLMs; Prevention and mitigation; Prompt engineering; Prompt Engineering; Security; Signing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2025 3rd International Conference on Advancement in Computation & Computer Technologies (InCACCT)","","","","","","","","","","","","","","",""
"YBT9F2K5","conferencePaper","2025","Aminou, Loubna; Daaif, Abdelaziz; Soulami, Maha; Youssfi, Mohamed","’Ignore All and Accept My Resume’: The Impact of Prompt Injection in Automatic Resume Screening","2025 5th International Conference on Innovative Research in Applied Science, Engineering and Technology (IRASET)","","","10.1109/IRASET64571.2025.11008146","https://ieeexplore.ieee.org/document/11008146/","Users of generative AI models—such as ChatGPT, Claude, or any other model—are becoming familiar with prompt manipulation, contributing to the Prompt Engineering as an emerging technical field. Indeed, every new technology possesses a dual nature and can be able of being exploited for both beneficial and harmful purposes. The harmful aspect in the field of prompt engineering is known as Prompt Injection. The latter, which is a harmful practice used in different situations, involves manipulating AI systems by injecting malicious instructions. In the context of resume screening, job candidates often exploit this technique to manipulate the applicant tracking system (ATS) by inserting a harmful prompt into the resume to be approved. By examining recent studies, this review seeks to understand this new area of concern by examining the influence of prompt injection on resume screening. The topic is still general and not tailored to a specific use case, such as resume screening, owing to its novelty. Therefore, we provide an overview to shed light on how this could affect the quality of the hiring decisions.","2025-05","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:10:38","1-5","","","","","","’Ignore All and Accept My Resume’","","","","","","","","","","","","IEEE Xplore","","","","","","","ATS; Investment; job candidate; Large Language Model; Large language models; Organizations; Prompt; Prompt engineering; Prompt Injection; Protection; Recruitment; resume; resume screening; Resumes; Reviews; Security; Sensitivity","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2025 5th International Conference on Innovative Research in Applied Science, Engineering and Technology (IRASET)","","","","","","","","","","","","","","",""
"9LDMAEMX","conferencePaper","2025","Yu, Zhengyi; Fan, Chunlong","Reconstruction-Based Prompt Generation Algorithm for Prompt Injection Attacks","2025 5th International Conference on Advanced Algorithms and Neural Networks (AANN)","","","10.1109/AANN66429.2025.11257661","https://ieeexplore.ieee.org/document/11257661/","In order to better bypass the security mechanism of the existing model and verify the vulnerabilities of the model in prompt word injection, we propose a reconstructed prompt generation algorithm (RPGA) to explore the prompt injection attacks of large language models. By combining fictional scenes and template-based, more confusing and diverse prompt words are generated. By dynamically generating prompt words, the invisibility and success rate of attacks are enhanced. Experimental results show that RPGA has a high attack success rate on multiple mainstream large language models, which fully verifies the vulnerability of the current model in prompt word injection attacks. Compared with direct prompt injection and indirect prompt injection, the success rate of reconfiguration prompt injection attack is significantly improved. The result highlights the vulnerability of current large language models in the face of complex prompt word attacks, and provides an important reference for the improvement of subsequent model security mechanisms.","2025-08","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:10:41","288-291","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","","","","artificial intelligence; Computer security; cybersecurity; Diversity reception; Faces; Heuristic algorithms; large language model; Large language models; Neural networks; prompt Injection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2025 5th International Conference on Advanced Algorithms and Neural Networks (AANN)","","","","","","","","","","","","","","",""
"LIG4UJ83","conferencePaper","2024","Yao, Dongyu; Zhang, Jianshu; Harris, Ian G.; Carlsson, Marcel","FuzzLLM: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models","ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2379-190X","10.1109/ICASSP48485.2024.10448041","https://ieeexplore.ieee.org/document/10448041/","Jailbreak vulnerabilities in Large Language Models (LLMs), which exploit meticulously crafted prompts to elicit content that violates service guidelines, have captured the attention of research communities. While model owners can defend against individual jailbreak prompts through safety training strategies, this relatively passive approach struggles to handle the broader category of similar jailbreaks. To tackle this issue, we introduce FuzzLLM, an automated fuzzing framework designed to proactively test and discover jailbreak vulnerabilities in LLMs. We utilize templates to capture the structural integrity of a prompt and isolate key features of a jailbreak class as constraints. By integrating different base classes into powerful combo attacks and varying the elements of constraints and prohibited questions, FuzzLLM enables efficient testing with reduced manual effort. Extensive experiments demonstrate FuzzLLM’s effectiveness and comprehensiveness in vulnerability discovery across various LLMs.","2024-04","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:10:44","4485-4489","","","","","","FuzzLLM","","","","","","","","","","","","IEEE Xplore","","ISSN: 2379-190X","","/home/arthurbrito/Zotero/storage/P38FWYVX/Yao et al. - 2024 - FuzzLLM A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabiliti.pdf","","","Acoustics; Automated Fuzzing; Fuzzing; Jailbreak Vulnerability; Large Language Model; Manuals; Safety; Signal processing; Speech processing; Training","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","","","","","","","","","","","","","",""
"27KYDWXE","conferencePaper","2025","Bao, Ziqun; Ji, Yu; Wu, Wen; Chen, Xi; He, Liang","Supervisor Alignment Framework: Enhancing LLM Alignment with Query-Ignoring Strategy and Multi-Agent Interaction","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2379-190X","10.1109/ICASSP49660.2025.10890479","https://ieeexplore.ieee.org/document/10890479/","The increasing focus on value alignment in Large Language Models (LLMs) underscores the need to ensure alignment with human morals and avoid biased or harmful outputs. However, LLMs aligned using existing methods are still easily affected by adversarial prompt attacks. Inspired by psychology, this paper introduces a Supervisor Alignment framework, which innovatively incorporates a query-ignoring strategy. This strategy ensures that the supervisor does not receive user queries, preventing it from being influenced by potential adversarial prompts. Meanwhile, the study compares the efficacy of a single supervisor versus a team of supervisors in value alignment tasks. While our designed single-agent supervisor approach utilizes a standalone agent or integrates with Retrieval-Augmented Generation (RAG) techniques, the team approach we proposed emphasizes multi-agent collaboration through voting, cooperation, and debate strategies. Extensive experiments demonstrate that the Supervisor Alignment framework we designed, incorporating the query-ignoring strategy and multi-agent collaboration, effectively defends against adversarial prompts and enhances its performance in value alignment tasks.","2025-04","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:10:48","1-5","","","","","","Supervisor Alignment Framework","","","","","","","","","","","","IEEE Xplore","","ISSN: 2379-190X","","","","","Acoustics; Adversarial prompts; AI Alignment; Collaboration; Ethics; Large Language Model; Large language models; Protection; Psychology; Retrieval augmented generation; Security; Signal processing; Speech processing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","","","","","","","","","","","","","",""
"2RTKDLEV","conferencePaper","2025","Chen, Qiuyu; Yamaguchi, Shingo; Yamamoto, Yudai","Optimization of Detection Mechanisms for GCG Attacks on Diverse Generative AI Models","2025 IEEE 14th Global Conference on Consumer Electronics (GCCE)","","2693-0854","10.1109/GCCE65946.2025.11275513","https://ieeexplore.ieee.org/document/11275513/","In this paper, to enhance the potential generalizability of the proposed machine learning-based detection method, we conducted the Greedy Coordinate Gradient (GCG) attack and detection experiments across various large language models. We proposed a new approach to optimize detection performance based on the original defense framework. This method further reduces false positives and adapts to the specific characteristics of GCG attacks targeting different LLMs. Our goal is to achieve high-precision GCG attack detection across multiple models. Experimental results show that we improved the accuracy to 99.65% on the corresponding model.","2025-09","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:10:50","591-594","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2693-0854","","","","","Accuracy; Adaptation models; Bayes methods; bayesian optimization; Consumer electronics; GCG; Generative AI; jailbreak attack; large language model; Large language models; Optimization; SVM; Syntactics; syntax tree; Trees (botanical)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2025 IEEE 14th Global Conference on Consumer Electronics (GCCE)","","","","","","","","","","","","","","",""
"Z3T8LMW3","conferencePaper","2025","Zhang, Lijie; Wang, Mingsi; Zhao, Yue; Lin, Zijin; Chen, Kai","DEO: Jailbreak a Black-box Multimodal Large Language Model with Dual-Embedding Alignment","2025 International Joint Conference on Neural Networks (IJCNN)","","2161-4407","10.1109/IJCNN64981.2025.11229281","https://ieeexplore.ieee.org/document/11229281/","Multimodal Large Language Models (MLLMs), which integrate textual and visual modalities, have demonstrated unparalleled capabilities in diverse multimodal tasks. However, the inclusion of visual inputs exposes MLLMs to security risks, one of which is jailbreak attacks. Although various methods have been proposed to jailbreak MLLMs via the visual modality, attacks in black-box settings have some limitations. Existing black-box attacks either fail to generate precise harmful outputs in practical scenarios or require substantial preparatory work in constructing adversarial images. In this work, we propose a novel dual-embedding optimization (DEO) attack approach to generate visual adversarial perturbations that induce the MLLMs to produce harmful responses that violate common AI safety policies. Specifically, DEO iteratively optimizes the visual input by enforcing alignment objectives across both the input and output embedding spaces: the image embedding of the input and the text embedding generated by the MLLM are both required to align with a harmful target text within a shared embedding space, which is defined by a frozen pretrained encoder. This alignment is conducted entirely under a black-box setting using a query-based strategy, where the attacker issues queries and observes only the model’s outputs, without access to its internal parameters or gradients. By optimizing in the dual-embedding space, our method can generate an adversarial perturbation to elicit more harmful and precise responses, overcoming the limitations of existing approaches. Experimental results demonstrate that our method significantly improves attack success rates of existing black-box attack methods by up to 30% against two MLLM families, including MiniGPT4 and LLaVa, achieving an average attack success rate of 87% across different models and eight scenarios, demonstrating its superior attack effectiveness. These findings highlight the urgent need for systematic robustness evaluations and improved safety mechanisms in MLLMs.1Content Warning: This paper contains harmful model responses.","2025-06","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:10:57","1-8","","","","","","DEO","","","","","","","","","","","","IEEE Xplore","","ISSN: 2161-4407","","","","","Adaptation models; Black-box Jailbreak; Closed box; Large language models; Multimodal Jailbreak; Multimodal Large Language Models; Optimization; Perturbation methods; Robustness; Safety; Security; Systematics; Visualization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2025 International Joint Conference on Neural Networks (IJCNN)","","","","","","","","","","","","","","",""
"9XW72VHS","conferencePaper","2024","Shibli, Ashfak Md; Pritom, Mir Mehedi A.; Gupta, Maanak","AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns","2024 12th International Symposium on Digital Forensics and Security (ISDFS)","","2768-1831","10.1109/ISDFS60797.2024.10527300","https://ieeexplore.ieee.org/document/10527300/","SMS phishing, also known as “smishing”, is a growing threat that tricks users into disclosing private information or clicking into URLs with malicious content through fraudulent mobile text messages. In recent past, we have also observed a rapid advancement of conversational generative AI chatbot services (e.g., OpenAI's ChatGPT, Google's BARD), which are powered by pre-trained large language models (LLMs). These AI chatbots certainly have a lot of utilities but it is not systematically understood how they can play a role in creating threats and attacks. In this paper, we propose AbuseGPT method to show how the existing generative AI-based chatbot services can be exploited by attackers in real world to create smishing texts and eventually lead to craftier smishing campaigns. To the best of our knowledge, there is no pre-existing work that evidently shows the impacts of these generative text-based models on creating SMS phishing. Thus, we believe this study is the first of its kind to shed light on this emerging cybersecurity threat. We have found strong empirical evidences to show that attackers can exploit ethical standards in the existing generative AI-based chatbot services by crafting prompt injection attacks to create newer smishing campaigns. We also discuss some future research directions and guidelines to protect the abuse of generative AI-based services and safeguard users from smishing attacks.","2024-04","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:11:01","1-6","","","","","","AbuseGPT","","","","","","","","","","","","IEEE Xplore","","ISSN: 2768-1831","","/home/arthurbrito/Zotero/storage/YSKVS5ZF/Shibli et al. - 2024 - AbuseGPT Abuse of Generative AI ChatBots to Create Smishing Campaigns.pdf","","","abuse; Chatbots; Ecosystems; Electronic mail; Ethics; generative AI; Generative AI; LLM; Message services; Phishing; scam; Smishing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 12th International Symposium on Digital Forensics and Security (ISDFS)","","","","","","","","","","","","","","",""
"GA9KEK3M","conferencePaper","2024","Tien, Le Anh; Van Huong, Pham","Optimizing Transformer Models for Prompt Jailbreak Attack Detection in AI Assistant Systems","2024 1st International Conference On Cryptography And Information Security (VCRIS)","","","10.1109/VCRIS63677.2024.10813380","https://ieeexplore.ieee.org/document/10813380/","Integrating AI-powered assistants in different areas has changed how people retrieve information. Users can now get quick responses, use the service whenever needed, and handle inquiries well. However, relying on these systems has increasingly raised worries about their safety and reliability, especially when facing threats like prompt injection or jailbreak attacks. These attacks take advantage of the weaknesses of large language models (LLMs) by changing input prompts to create harmful, biased, or misleading results. This paper examines prompt jailbreak attacks in ChatGPT-based assistant chatbots, especially in education. It examines the weaknesses in these systems and suggests ways to detect these attacks by fine-tuning various Transformer models. The research also includes adding prompt jailbreak attack detection in a virtual assistant application for university use. This aims to ensure teachers and students can interact safely and rely on the system. Through rigorous experimentation and evaluation, we demonstrate the practicality and effectiveness of our detection methods, providing reassurance and confidence in the face of AI security challenges. Our studies emphasize the need for robust AI chatbots and offer practical solutions to preserve the integrity of AI-driven tools.","2024-12","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:11:03","1-4","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","","","","chatbot; Chatbots; Faces; Information security; Large language models; LLM; prompt engineering; prompt injection attack; Refining; Reliability engineering; Safety; Training; transformer; Transformers; Virtual assistants","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 1st International Conference On Cryptography And Information Security (VCRIS)","","","","","","","","","","","","","","",""
"LC4HE4IE","conferencePaper","2025","Yang, Ao; Wang, Bin; Liu, Aofan; Li, Hui","Automatically Generated Multi-Agent Framework for Jailbreaking Large Language Models","2025 5th International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA)","","","10.1109/AIITA65135.2025.11047880","https://ieeexplore.ieee.org/document/11047880/","LLMs have developed rapidly at an unprecedented speed and are being widely used, including question answering, code generation and reasoning. LLMs are not completely safe and reliable, and may output harmful or illegal content under attack. Jailbreak attacks can bypass the security measures of LLMs and use LLMs to generate harmful content. However, jailbreak attack methods often rely on manually crafted prompt templates or not having sufficient diversity or jailbreak effect. In this paper, we introduce a new automatically generated multi-agent framework for automatic jailbreak testing of black-box large language models. We use the collaborative capabilities of multiple agents to automatically generate diverse jailbreak attack prompts to cover a wide range of vulnerability scenarios. We carefully implement three key modules: an automatically generated multi-agent framework, jailbreaking strategies for black-box large language models, and an iterative reflection mechanism. We evaluate on many different attack scenarios and multiple different black-box LLMs. Our results show that our work can achieve jailbreak with a high success rate. The attack success rate for GPT-4 exceeds 95%, which shows that our method can help current LLMs improve security and reliability.","2025-03","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:11:06","1500-1503","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","","","","Closed box; Cognition; Collaboration; Iterative methods; Jailbreak; Large language models; Large Language Models; Multi-Agent; Question answering (information retrieval); Red-Teaming; Reflection; Reliability; Security; Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2025 5th International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA)","","","","","","","","","","","","","","",""
"LJ5A7SIY","journalArticle","2025","Ying, Zonghao; Liu, Aishan; Zhang, Tianyuan; Yu, Zhengmin; Liang, Siyuan; Liu, Xianglong; Tao, Dacheng","Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt","IEEE Transactions on Information Forensics and Security","","1556-6021","10.1109/TIFS.2025.3583249","https://ieeexplore.ieee.org/document/11059299/","In the realm of large vision language models (LVLMs), jailbreak attacks serve as a red-teaming approach to bypass guardrails and uncover safety implications. Existing jailbreaks predominantly focus on the visual modality, perturbing solely visual inputs in the prompt for attacks. However, they fall short when confronted with aligned models that fuse visual and textual features simultaneously for generation. To address this limitation, this paper introduces the Bi-Modal Adversarial Prompt Attack (BAP), which executes jailbreaks by optimizing textual and visual prompts cohesively. Initially, we adversarially embed universally adversarial perturbations in an image, guided by a few-shot query-agnostic corpus (e.g., affirmative prefixes and negative inhibitions). This process ensures that the adversarial image prompt LVLMs to respond positively to harmful queries. Subsequently, leveraging the image, we optimize textual prompts with specific harmful intent. In particular, we utilize a large language model to analyze jailbreak failures and employ chain-of-thought reasoning to refine textual prompts through a feedback-iteration manner. To validate the efficacy of our approach, we conducted extensive evaluations on various datasets and LVLMs, demonstrating that our BAP significantly outperforms other methods by large margins (+29.03% in attack success rate on average). Additionally, we showcase the potential of our attacks on black-box commercial LVLMs, such as GPT-4o and Gemini. Our code is available at https://anonymous.4open.science/r/BAP-Jailbreak-Vision-Language-Models-via-Bi-Modal-Adversarial-Prompt-5496","2025","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:11:13","7153-7165","","","20","","","","","","","","","","","","","","","IEEE Xplore","","","","","","","adversarial perturbation; chain-of-thought; Cognition; Fuses; jailbreak attack; Large vision language models (LVLMs); Optimization; Perturbation methods; red-teaming; Safety; Security; Semantics; Training; Visualization; Weapons","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G9W772CV","conferencePaper","2025","Tian, Chen; Yuan, Qi; Liu, Xiaoli; Yan, Ying","JailSmash: Lightweight Jailbreaking of LLM Applications on Android Smartphones","2025 IEEE 7th International Conference on Communications, Information System and Computer Engineering (CISCE)","","2833-2423","10.1109/CISCE65916.2025.11065135","https://ieeexplore.ieee.org/document/11065135/","The rapid growth of LLM APPs brings potential security risks, particularly jailbreaking attacks. These attacks involve bypassing LLM security mechanisms through carefully designed prompts to generate harmful or unethical content, posing threats to both individuals and organizations. We developed a lightweight jailbreak tool called JailSmash, which leverages jailbreak templates based on in-context learning (ICL) and strategies to guide third-party large language models in automatically generating jailbreak test sets. This tool also enables automated interaction and evaluation of jailbreak results with LLM apps on Android smartphones. Furthermore, we used JailSmash to conduct jailbreak security assessments on 30 mainstream LLM apps, thereby validating the effectiveness of the tool.","2025-05","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:11:17","1033-1036","","","","","","JailSmash","","","","","","","","","","","","IEEE Xplore","","ISSN: 2833-2423","","","","","Android; Computer security; ICL; Information systems; Jailbreak; Large language models; LLM app security; Organizations; Security; Smart phones","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2025 IEEE 7th International Conference on Communications, Information System and Computer Engineering (CISCE)","","","","","","","","","","","","","","",""
"NTVHPJ3F","conferencePaper","2024","Wu, Feng; Wang, Weiqi; Qu, Youyang; Yu, Shui","A Low-cost Black-box Jailbreak Based on Custom Mapping Dictionary with Multi-round Induction","2024 IEEE 23rd International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)","","2324-9013","10.1109/TrustCom63139.2024.00131","https://ieeexplore.ieee.org/document/10944972/","Note:This paper contains many malicious contents generated by LLMs. Jailbreak can cause large language models (LLMs) to violate moral and ethical guidelines, generating harmful text and images. However, the existing jailbreaks are high-cost (e.g., fine-tuning LLMs) and underperform when facing some specific jailbreak tasks (e.g., generating pornographic and bloody texts), which has too many limitations for reflecting the threat of jailbreak on LLMs’ security. To fill this gap, in this paper, we propose a black-box jailbreak with a higher attack success rate and lower cost, called Dictionary Jailbreak with Multi-round Induction (DJMI). First, in DJMI, we create a simple custom language dictionary based on specific jailbreak tasks to be performed and send it to the LLM. Then, we use the custom language from the dictionary to construct malicious prompts, instructing the LLM to respond in the custom language as much as possible. Generally, in the first round, the LLM will provide a neutral response (such as translating the malicious prompts) or refuse to answer. To counteract this, we need to emphasize that the prompts comply with ethical guidelines and repeat the prompts using the custom language. The process can be conducted with multiple rounds to guide LLM in outputting harmful content. During the attack process, the only attack cost is the transmission cost of the prompts through the official interactive interface, without any other costs of LLM fine-tuning, algorithm design and prompt generation. Thus, DJMI is a low-cost jailbreak method. We verified DJMI on multiple mainstream LLMs across various jailbreak tasks. Experiments show that compared to existing black-box jailbreaks, DJMI achieves a higher attack success rate (> 80% on average) across specific jailbreak tasks with various risk levels. Additionally, DJMI enables the LLM to provide highly detailed execution steps for harmful behaviors within a session (such as specifying proportions of ingredients, synthetic chemical formulas, experimental conditions, and other detailed information in illicit drug production), further highlighting the severity of jailbreak attacks.","2024-12","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:11:21","888-895","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2324-9013","","","","","black-box; Closed box; Costs; Dictionaries; Drugs; Ethics; Guidelines; jailbreak; jailbreak tasks; Large language models; LLMs; prompts; Resists; Translation; Waste materials","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE 23rd International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)","","","","","","","","","","","","","","",""
"EB9RT94R","conferencePaper","2024","Rababah, Baha; Wu, Shang Tommy; Kwiatkowski, Matthew; Leung, Carson K.; Akcora, Cuneyt Gurcan","Are Existing Large Language Models Robust Against Jailbreak Attacks?","2024 IEEE International Conference on Big Data (BigData)","","2573-2978","10.1109/BigData62323.2024.10825802","https://ieeexplore.ieee.org/document/10825802/","The safety and robustness of Large Language Models (LLMs) are major challenges in developing generative AI applications. One key issue is the vulnerability to prompt jailbreak attacks, which pose a significant threat to building secure and resilient LLM-based applications. In this work, we present a framework for understanding and evaluating the behaviors of popular LLMs by categorizing their responses into five distinct exposure levels. Additionally, we introduce a novel language attack that circumvents LLMs’ defenses by translating jailbreak prompts into languages such as Arabic, Chinese, and Greek. Despite ongoing efforts to enhance LLMs’ safety, we find that nearly all popular LLMs can be jailbroken. Our findings offer detailed insights into LLMs’ behavior, improve diagnostic capabilities, and support targeted safety improvements.","2024-12","2026-01-17 21:11:25","2026-01-17 21:11:25","2026-01-17 21:11:25","5383-5391","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2573-2978","","","","","Accuracy; Big Data; Buildings; Generative AI; Large language models; LLMs; Multilingual; Multilingual Jailbreak; Prompt Jailbreak; Robustness; Safety; Translation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE International Conference on Big Data (BigData)","","","","","","","","","","","","","","",""