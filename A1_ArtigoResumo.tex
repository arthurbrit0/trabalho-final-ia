% !TeX program = pdflatex
% Projeto Final - Inteligencia Artificial (UFC)
% Artigo tipo survey em formato LNCS (portugues)
%
% ============== INSTRUCOES DE COMPILACAO ==============
% Para que as referencias aparecam corretamente, compile na ordem:
%   1. pdflatex A1_ArtigoResumo.tex
%   2. bibtex A1_ArtigoResumo
%   3. pdflatex A1_ArtigoResumo.tex
%   4. pdflatex A1_ArtigoResumo.tex
%
% No Overleaf: basta recompilar 2x que funciona automaticamente.
% ======================================================
%
% ATENCAO: O enunciado permite maximo de 5 integrantes.
% Se o grupo tiver 6 pessoas, confirmar com o professor ou ajustar a lista abaixo.
%

\documentclass[runningheads]{llncs}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{positioning,arrows.meta,calc}
\usepackage{caption}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{tcolorbox}
\tcbuselibrary{listings,skins}

% Estilo para caixas de exemplo de ataque
\newtcolorbox{attackbox}[1][]{
  colback=red!5,
  colframe=red!50!black,
  fonttitle=\bfseries\footnotesize,
  title=#1,
  boxrule=0.5pt,
  arc=2pt,
  left=3pt,
  right=3pt,
  top=2pt,
  bottom=2pt
}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue
}

\title{Prompt Injection em LLMs e Sistemas Ag\^enticos:\\Taxonomia, Avalia\c{c}\~ao e Defesas em Profundidade}
\titlerunning{Prompt Injection em LLMs e Sistemas Ag\^enticos}

\author{Arthur Ant\^onio Brito da Costa\inst{1} \and
Lia Linhares Carvalhedo\inst{1} \and
Jos\'e Vitor de Albuquerque Coelho Santos\inst{1} \and
Jo\~ao Vitor Mesquita Gouveia\inst{1} \and
Luis Ant\^onio Andrade de Albuquerque\inst{1} \and
Caio de Ara\'ujo Mac\^edo\inst{1}}
\authorrunning{\ }

\institute{Universidade Federal do Cear\'a (UFC), Departamento de Computa\c{c}\~ao,\\
Fortaleza, CE, Brasil}

\begin{document}
\maketitle

\begin{abstract}
Modelos de Linguagem de Grande Porte (LLMs) e sistemas ag\^enticos baseados em LLMs t\^em sido integrados a aplica\c{c}\~oes reais (assistentes, RAG, copilots, automa\c{c}\~ao). Essa integra\c{c}\~ao amplia a superf\'icie de ataque e torna o prompt injection um risco priorit\'ario em seguran\c{c}a, pois a mesma cadeia de texto pode conter tanto instru\c{c}\~oes confi\'aveis quanto dados n\~ao confi\'aveis. Este artigo apresenta um survey sobre prompt injection e amea\c{c}as correlatas em LLMs e agentes, sintetizando 10 trabalhos selecionados (2023--2026) e o guia OWASP Top~10 for LLM Applications 2025. Propomos uma taxonomia unificada por vetores de entrega (direto, indireto e via ferramentas), modalidades (texto e multimodal), e comportamento de propaga\c{c}\~ao (recurs\~ao, infec\c{c}\~ao e worms). Organizamos as principais estrat\'egias de defesa (engenharia de prompt, filtragem/valida\c{c}\~ao, controle de privil\'egio e isolamento arquitetural) e discutimos como avali\'a-las usando conjuntos de dados e benchmarks recentes (Tensor Trust e HackAPrompt), destacando trade-offs entre utilidade e robustez. Por fim, sistematizamos desafios abertos para seguran\c{c}a em agentes (ex.: excesso de autonomia, contamina\c{c}\~ao de contexto e ataques h\'ibridos) e dire\c{c}\~oes futuras.
\keywords{seguran\c{c}a em IA \and LLM \and prompt injection \and agentes \and RAG \and OWASP}
\end{abstract}

\section{Introdu\c{c}\~ao}
A ado\c{c}\~ao de LLMs em produtos e processos (assist\^entes de texto, retrieval-augmented generation - RAG, e agentes com uso de ferramentas) transformou o modo como organiza\c{c}\~oes buscam informa\c{c}\~ao, geram conte\'udo e executam tarefas. Ao mesmo tempo, essa mudan\c{c}a exp\~oe uma vulnerabilidade estrutural: diferentemente de software tradicional, LLMs processam instru\c{c}\~oes e dados como uma sequencia de tokens sem fronteiras sint\'aticas confi\'aveis entre c\'odigo e dados. Esse desalinhamento entre flexibilidade e controle torna ataques de prompt injection (PI) um problema central de seguran\c{c}a.

O OWASP Top~10 for LLM Applications 2025 classifica Prompt Injection como LLM01:2025, destacando que entradas podem alterar comportamento/sa\'ida do modelo de formas n\~ao intencionais, inclusive com instru\c{c}\~oes impercept\'iveis a humanos (por exemplo, embutidas em conte\'udo externo) \cite{owasp2025top10}. Em aplica\c{c}\~oes integradas a ferramentas, o impacto pode ir al\'em de texto: o modelo pode disparar chamadas a APIs, acessar dados sens\'iveis e atuar como ``deputado confuso'' (confused deputy), executando a vontade do atacante com credenciais leg\'itimas.

A literatura recente mostra (i) ataque indireto via conte\'udo externo (web/documentos) \cite{greshake2023indirect}, (ii) ataques em escala com competi\c{c}\~oes e datasets que revelam padr\~oes de jailbreak/extra\c{c}\~ao/hijack \cite{toyer2023tensortrust,schulhoff2024hackaprompt}, (iii) ataques e consequ\^encias em aplica\c{c}\~oes comerciais \cite{liu2024houyi}, (iv) defesa via t\'ecnicas de engenharia de prompt robustas, como spotlighting \cite{hines2024spotlighting}, e (v) evolu\c{c}\~ao do cen\'ario para amea\c{c}as h\'ibridas combinando PI com vulnerabilidades cl\'assicas (XSS/CSRF etc.) em sistemas ag\^enticos \cite{mchugh2025promptinjection20}. Revis\~oes recentes sintetizam esse panorama e apontam lacunas de padroniza\c{c}\~ao e avalia\c{c}\~ao \cite{naik2025threatlandscape,gulyamov2026review}.

\textbf{Contribui\c{c}\~oes deste artigo.} A partir de uma revis\~ao sistem\'atica de 10 trabalhos e relat\'orio OWASP, este survey:
\begin{enumerate}[leftmargin=*, itemsep=2pt]
\item prop\~oe uma taxonomia unificada para prompt injection e ataques correlatos em LLMs e agentes;
\item organiza defesas por camadas (engenharia de prompt, filtros/valida\c{c}\~oes, e controles arquiteturais) e discute trade-offs;
\item sumariza pr\'aticas de avalia\c{c}\~ao e recursos abertos (datasets/benchmarks) para medir robustez;
\item destaca desafios abertos e dire\c{c}\~oes de pesquisa para seguran\c{c}a de agentes.
\end{enumerate}

\section{Trabalhos relacionados}
Al\'em de estudos isolados de ataque e defesa, t\^em surgido guias e revis\~oes que tentam organizar o campo de seguran\c{c}a em LLMs. O OWASP Top 10 for LLM Applications 2025 consolida uma taxonomia orientada a risco (LLM01--LLM10) e recomenda controles pr\'aticos para sistemas em produ\c{c}\~ao, como valida\c{c}\~ao de sa\'ida, privil\'egio m\'inimo e filtros de entrada/sa\'ida \cite{owasp2025top10}.

No \^ambito acad\^emico, Naik et al. apresentam um panorama mais amplo de ataques adversariais em IA generativa, incluindo prompt injection, envenenamento de dados e amea\c{c}as relacionadas a RAG \cite{naik2025threatlandscape}. Gulyamov et al. oferecem uma revis\~ao abrangente de 2023--2025 com foco em prompt injection em LLMs e agentes, discutindo vetores diretos e indiretos, riscos em ecossistemas de agentes e a necessidade de defesa em profundidade \cite{gulyamov2026review}.

Este artigo difere desses trabalhos ao (i) sintetizar, em um recorte compacto de 10 estudos selecionados, uma taxonomia operacional alinhada ao OWASP e (ii) conectar explicitamente vetores de ataque, camadas de defesa e pr\'aticas de avalia\c{c}\~ao (datasets/benchmarks), visando apoiar um leitor que precisa projetar ou auditar aplica\c{c}\~oes com LLMs e agentes.

\section{Metodologia de revis\~ao sistem\'atica}
Esta se\c{c}\~ao descreve o protocolo de busca e sele\c{c}\~ao (Atividade~1), de forma rastre\'avel e reproduz\'ivel.

\subsection{Perguntas de pesquisa}
Definimos as seguintes quest\~oes:
\begin{itemize}[leftmargin=*, itemsep=2pt]
\item \textbf{RQ1} - Quais s\~ao os principais vetores de prompt injection em aplica\c{c}\~oes com LLMs (direto, indireto e h\'ibrido)?
\item \textbf{RQ2} - Quais impactos e objetivos aparecem com mais frequ\^encia (exfiltra\c{c}\~ao, goal hijacking, execu\c{c}\~ao de a\c{c}\~oes via ferramentas, degrada\c{c}\~ao/DoS)?
\item \textbf{RQ3} - Como os trabalhos avaliam ataques e defesas (m\'etricas, datasets, setups experimentais)?
\item \textbf{RQ4} - Quais estrat\'egias de mitiga\c{c}\~ao existem e quais trade-offs envolvem (custo computacional, falsos positivos, perda de utilidade)?
\item \textbf{RQ5} - O que muda quando h\'a integra\c{c}\~ao com RAG e/ou agentes com ferramentas?
\end{itemize}

\subsection{Fontes e strings de busca}
\textbf{Bases consultadas:} arXiv (06/jan/2026), Google Scholar (08/jan/2026), IEEE Xplore (10/jan/2026), ACM Digital Library (10/jan/2026), e busca por snowballing (refer\^encias e cita\c{c}\~oes) em trabalhos centrais.\\
\textbf{String de busca (utilizada em todas as bases):}
\begin{quote}
\texttt{("prompt injection" OR jailbreak OR "indirect prompt injection" OR "prompt leaking" OR "prompt extraction" OR "tool poisoning") AND (LLM OR "large language model" OR agent OR RAG)}
\end{quote}
\textbf{Filtros aplicados:} per\'iodo 2023--2026; artigos/preprints/reports com texto completo dispon\'ivel; foco em seguran\c{c}a (ataques, taxonomia, defesa ou avalia\c{c}\~ao). Adapta\c{c}\~oes menores de sintaxe foram feitas por base (ex.: IEEE usa aspas simples; ACM requer truncamento de operadores).

\subsection{Crit\'erios de inclus\~ao e exclus\~ao}
\textbf{Inclus\~ao:} (i) aborda prompt injection em LLMs/agents/RAG; (ii) apresenta taxonomia, dataset/benchmark, estudo emp\'irico, incidente documentado ou defesa avaliada; (iii) texto completo dispon\'ivel.\\
\textbf{Exclus\~ao:} (i) conte\'udo opinativo sem m\'etodo; (ii) sem rela\c{c}\~ao com LLMs; (iii) duplicatas; (iv) sem contribui\c{c}\~ao t\'ecnica (ex.: apenas tutorial).

\subsection{Processo de triagem}
A triagem foi feita em tr\^es etapas: (1) deduplica\c{c}\~ao; (2) leitura de t\'itulo/resumo; (3) leitura de texto completo. A Figura~\ref{fig:prisma} apresenta o fluxograma PRISMA 2020 com os n\'umeros de cada etapa.

\textbf{C\'odigos de exclus\~ao:}
\begin{itemize}[leftmargin=*, itemsep=1pt, topsep=2pt]
\item \textbf{E1} - Sem rela\c{c}\~ao com LLMs (ex.: inje\c{c}\~ao SQL tradicional, seguran\c{c}a de redes gen\'erica).
\item \textbf{E2} - Aborda LLMs, mas sem foco em seguran\c{c}a (ex.: avalia\c{c}\~ao de desempenho, benchmarks de NLP).
\item \textbf{E6} - Foco em ataque espec\'ifico sem contribui\c{c}\~ao taxon\^omica ou metodol\'ogica reproduz\'ivel.
\end{itemize}

A triagem por t\'itulo e resumo excluiu 588 registros, principalmente por E6 (42\%), E2 (35\%) e E1 (23\%). Na avalia\c{c}\~ao de texto completo, 39 artigos foram exclu\'idos por focarem em ataques pontuais sem contribui\c{c}\~ao para a taxonomia ou por metodologia insuficiente. A lista final incluiu 10 trabalhos (Tabela~\ref{tab:estudos}).

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  node distance=0.4cm and 0.3cm,
  box/.style={draw, rounded corners, align=center, font=\scriptsize, minimum width=2.8cm, minimum height=0.7cm, inner sep=2pt},
  smallbox/.style={draw, rounded corners, align=center, font=\scriptsize, minimum width=2.2cm, minimum height=0.6cm, inner sep=2pt},
  phase/.style={fill=gray!20, font=\scriptsize\bfseries, minimum width=1.2cm, rotate=90, anchor=center},
  arrow/.style={-{Stealth}, thick}
]

% === COLUNA ESQUERDA: Bases de dados ===
\node[box, fill=blue!10] (id1) {Registros identificados\\em bases de dados\\(n = 669)};
\node[box, fill=blue!10, below=0.1cm of id1] (id1detail) {\scriptsize arXiv: 295\\Google Scholar: 250\\IEEE Xplore: 124};

% === COLUNA DIREITA: Outros métodos ===
\node[box, fill=green!10, right=1.5cm of id1] (id2) {Registros identificados\\por outros m\'etodos\\(n = 6)};
\node[box, fill=green!10, below=0.1cm of id2] (id2detail) {\scriptsize Snowballing e\\busca manual};

% === REMOÇÃO DE DUPLICATAS ===
\node[box, fill=orange!15, below=0.6cm of id1detail] (dup) {Duplicatas removidas\\(n = 35)};

% === TRIAGEM ===
\node[box, fill=yellow!15, below=0.8cm of dup] (screen) {Registros triados\\(t\'itulo/resumo)\\(n = 640)};
\node[smallbox, fill=red!10, right=1.0cm of screen] (excl1) {Exclu\'idos\\(n = 588)};
\draw[arrow] (screen) -- (excl1);

% === ELEGIBILIDADE ===
\node[box, fill=yellow!15, below=0.8cm of screen] (seek) {Reports buscados\\para leitura\\(n = 52)};
\node[smallbox, fill=red!10, right=1.0cm of seek] (notret) {N\~ao obtidos\\(n = 3)};
\draw[arrow] (seek) -- (notret);

\node[box, fill=yellow!15, below=0.4cm of seek] (assess) {Reports avaliados\\(texto completo)\\(n = 49)};
\node[smallbox, fill=red!10, right=1.0cm of assess] (excl2) {Exclu\'idos\\(n = 39)};
\draw[arrow] (assess) -- (excl2);

% === INCLUSÃO ===
\node[box, fill=green!20, below=0.8cm of assess, minimum width=3.2cm] (incl) {\textbf{Estudos inclu\'idos}\\na revis\~ao\\(n = 10)};

% === SETAS PRINCIPAIS ===
\draw[arrow] (id1) -- (id1detail);
\draw[arrow] (id2) -- (id2detail);
\draw[arrow] (id1detail) -- (dup);
\draw[arrow] (id2detail.south) |- (dup.east);
\draw[arrow] (dup) -- (screen);
\draw[arrow] (screen) -- (seek);
\draw[arrow] (seek) -- (assess);
\draw[arrow] (assess) -- (incl);

% === LABELS DAS FASES (lateral) ===
% Calcular pontos médios de cada fase
\coordinate (mid-ident) at ($(id1.north)!0.5!(dup.south)$);
\coordinate (mid-eleg) at ($(seek.north)!0.5!(assess.south)$);

% Posicionar labels no centro de cada fase
\node[phase] at ([xshift=-1.2cm, yshift=0.6cm]mid-ident -| id1.west) {Identifica\c{c}\~ao};
\node[phase] at ([xshift=-1.2cm, yshift=1.4cm]screen.west) {Triagem};
\node[phase] at ([xshift=-1.2cm, yshift=0.8cm]mid-eleg -| seek.west) {Elegibilidade};
\node[phase] at ([xshift=-1.2cm]incl.west) {Inclus\~ao};

\end{tikzpicture}
\caption{Fluxograma PRISMA 2020 do processo de identifica\c{c}\~ao, triagem e sele\c{c}\~ao dos estudos. Bases consultadas: arXiv, Google Scholar e IEEE Xplore (janeiro/2026). Seis estudos adicionais foram identificados por snowballing (OWASP, Tensor Trust, HackAPrompt, Spotlighting, McHugh et al., Kalliom\"aki).}
\label{fig:prisma}
\end{figure}

\subsection{Lista final de trabalhos analisados}
A Tabela~\ref{tab:estudos} resume os 10 trabalhos selecionados, classificando-os por tipo (ataque, defesa, benchmark, revis\~ao/diretriz), cen\'ario-alvo e vetor principal de ataque abordado.

\begin{table}[t]
\centering
\caption{Extra\c{c}\~ao de dados dos estudos selecionados (2023-2026).}
\label{tab:estudos}
\footnotesize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{@{}p{2.1cm}p{1.4cm}p{2.2cm}p{1.5cm}p{3.8cm}@{}}
\toprule
\textbf{Refer\^encia} & \textbf{Tipo} & \textbf{Cen\'ario} & \textbf{Vetor} & \textbf{Achados principais}\\
\midrule
OWASP 2025 \cite{owasp2025top10} & Diretriz & Apps com LLMs & Todos & Top 10 riscos; LLM01 = PI\\
Greshake et al. \cite{greshake2023indirect} & Ataque & Apps integrados & Indireto & Bing Chat; worming; contamina\c{c}\~ao\\
Liu et al. \cite{liu2024houyi} & Ataque & 36 apps reais & Dir./Ind. & HOUYI; 31 vulner\'aveis\\
Toyer et al. \cite{toyer2023tensortrust} & Benchmark & LLMs gerais & Direto & 126k ataques; extraction/hijacking\\
Schulhoff et al. \cite{schulhoff2024hackaprompt} & Benchmark & Chatbots & Direto & 600k+ prompts; ontologia\\
Hines et al. \cite{hines2024spotlighting} & Defesa & Multi-fonte & Indireto & Spotlighting; ASR: 50\% para 2\%\\
McHugh et al. \cite{mchugh2025promptinjection20} & An\'alise & Agentes e enterprise & H\'ibrido & PI + XSS/CSRF; worms\\
Naik et al. \cite{naik2025threatlandscape} & Survey & IA generativa & V\'arios & Panorama amplo de ataques\\
Kalliom\"aki \cite{kalliomaki2025agents} & Tese & Agentes LLM & V\'arios & Aplica\c{c}\~oes por ind\'ustria\\
Gulyamov et al. \cite{gulyamov2026review} & Review & LLMs + agentes & Dir./Ind. & 45 fontes; MCP; PALADIN\\
\bottomrule
\end{tabular}
\end{table}

Os trabalhos cobrem tr\^es perspectivas complementares: (i)~estudos de ataque com demonstra\c{c}\~oes emp\'iricas \cite{greshake2023indirect,liu2024houyi}; (ii)~benchmarks e competi\c{c}\~oes que geram dados em escala \cite{toyer2023tensortrust,schulhoff2024hackaprompt}; e (iii)~revis\~oes e diretrizes que sintetizam o estado da arte \cite{owasp2025top10,naik2025threatlandscape,gulyamov2026review}. A defesa de Hines et al. \cite{hines2024spotlighting} e a an\'alise de amea\c{c}as h\'ibridas de McHugh et al. \cite{mchugh2025promptinjection20} representam contribui\c{c}\~oes espec\'ificas para mitiga\c{c}\~ao e modelagem de riscos emergentes.

\section{Fundamentos de aplica\c{c}\~oes com LLMs, RAG e agentes}
\subsection{Arquiteturas t\'ipicas e pontos de inje\c{c}\~ao}
Sistemas modernos com LLMs frequentemente seguem um pipeline: (i) \textbf{prompt do sistema} (pol\'iticas, papel do agente), (ii) \textbf{entrada do usu\'ario}, (iii) \textbf{contexto externo} (RAG: documentos recuperados; hist\'orico e mem\'oria), e (iv) \textbf{ferramentas} (APIs, banco de dados, e-mail). A combina\c{c}\~ao desses elementos em um unico contexto textual cria uma fronteira de confian\c{c}a fr\'agil: dados n\~ao confi\'aveis podem ser interpretados como instru\c{c}\~oes.

A Figura~\ref{fig:superficie} ilustra pontos usuais de contamina\c{c}\~ao: (A) entrada direta do usu\'ario; (B) documentos e p\'aginas (inje\c{c}\~ao indireta); (C) descri\c{c}\~oes/metadados de ferramentas (tool poisoning); (D) mem\'oria/contexto de longo prazo; (E) sa\'ida do modelo (output handling) que pode ser consumida por c\'odigo/HTML.

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  box/.style={draw, rounded corners, align=center, inner sep=4pt, minimum width=2.0cm, minimum height=0.8cm, font=\scriptsize},
  arrow/.style={-{Stealth}, thick},
  every node/.style={font=\scriptsize}
]
% Linha superior - entradas lado a lado
\node[box] (sys) at (-2.5,1.8) {Prompt do sistema\\(pol\'iticas)};
\node[box] (user) at (0,1.8) {Entrada usu\'ario\\(A)};
\node[box] (rag) at (2.5,1.8) {Contexto RAG\\web/docs (B)};

% Centro - LLM
\node[box, minimum width=2.8cm, fill=gray!10] (llm) at (0,0) {LLM / Agente};

% Laterais
\node[box] (tools) at (-3.5,0) {Ferramentas\\APIs (C)};
\node[box] (mem) at (3.5,0) {Mem\'oria\\hist\'orico (D)};

% Sa\'ida
\node[box] (out) at (0,-1.5) {Sa\'ida (E)};

% Setas de entrada
\draw[arrow] (sys.south) -- (llm.north west);
\draw[arrow] (user.south) -- (llm.north);
\draw[arrow] (rag.south) -- (llm.north east);
\draw[arrow] (mem) -- (llm);
\draw[arrow] (llm) -- (out);
\draw[arrow, <->] (llm) -- (tools);
\end{tikzpicture}
\caption{Superf\'icie de ataque em aplica\c{c}\~oes com LLMs: fontes de contexto e ferramentas aumentam a exposi\c{c}\~ao a prompt injection.}
\label{fig:superficie}
\end{figure}

\subsection{Agentes LLM e excesso de autonomia}
Agentes com LLM estendem chatbots com m\'odulos de planejamento, mem\'oria e ferramentas, permitindo execu\c{c}\~ao multi-etapas e intera\c{c}\~ao com o ambiente \cite{kalliomaki2025agents}. Do ponto de vista de seguran\c{c}a, isso aproxima o risco de PI de um risco operacional: se o agente tem permiss\~oes amplas (``excessive agency'' no OWASP LLM06), uma inje\c{c}\~ao bem-sucedida pode induzir a\c{c}\~oes indevidas.

\section{Taxonomia unificada de prompt injection e amea\c{c}as correlatas}
Compilando as taxonomias de inje\c{c}\~ao direta/indireta \cite{owasp2025top10,greshake2023indirect}, recursos em escala (Tensor Trust e HackAPrompt) \cite{toyer2023tensortrust,schulhoff2024hackaprompt} e amea\c{c}as h\'ibridas \cite{mchugh2025promptinjection20}, propomos classificar ataques por tr\^es dimens\~oes complementares. A Figura~\ref{fig:taxonomia} apresenta uma vis\~ao hier\'arquica dos vetores de ataque e seus desdobramentos.

\textbf{Gloss\'ario de termos-chave}
\begin{itemize}[leftmargin=*, itemsep=1pt, topsep=2pt]
\item \textbf{Prompt injection (PI)} - t\'ecnica de ataque que insere instru\c{c}\~oes maliciosas no contexto do LLM para alterar seu comportamento.
\item \textbf{Jailbreak} - varia\c{c}\~ao de PI que visa contornar restri\c{c}\~oes de seguran\c{c}a do modelo.
\item \textbf{Prompt leaking/extraction} - extra\c{c}\~ao do prompt de sistema ou informa\c{c}\~oes confidenciais do contexto.
\item \textbf{Goal hijacking} - desvio do objetivo original da tarefa para um objetivo definido pelo atacante.
\item \textbf{Tool poisoning} - contamina\c{c}\~ao de descri\c{c}\~oes ou respostas de ferramentas para injetar instru\c{c}\~oes.
\item \textbf{RAG poisoning} - inje\c{c}\~ao indireta via documentos maliciosos na base de conhecimento.
\item \textbf{Spotlighting} - t\'ecnica de defesa que sinaliza a proveni\^encia de dados n\~ao confi\'aveis no prompt.
\end{itemize}

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  level 1/.style={sibling distance=38mm, level distance=14mm},
  level 2/.style={sibling distance=18mm, level distance=12mm},
  every node/.style={font=\footnotesize, align=center},
  root/.style={draw, rounded corners, fill=red!15, font=\footnotesize\bfseries, minimum width=2.8cm},
  l1/.style={draw, rounded corners, fill=orange!20, minimum width=2.2cm},
  l2/.style={draw, rounded corners, fill=yellow!20, minimum width=1.6cm, font=\scriptsize}
]
\node[root] {Prompt Injection}
  child {node[l1] {Direto}
    child {node[l2] {Jailbreak}}
    child {node[l2] {Extraction}}
  }
  child {node[l1] {Indireto}
    child {node[l2] {RAG poison}}
    child {node[l2] {Web/Docs}}
  }
  child {node[l1] {H\'ibrido}
    child {node[l2] {PI + XSS}}
    child {node[l2] {PI + SQLi}}
  };
\end{tikzpicture}

\vspace{2mm}
\begin{tikzpicture}[
  every node/.style={font=\footnotesize, align=center},
  obj/.style={draw, rounded corners, fill=blue!15, minimum width=2.0cm, minimum height=0.7cm, font=\scriptsize}
]
\node[font=\footnotesize\bfseries] at (0,0) {Objetivos:};
\node[obj] at (2.5,0) {Extraction};
\node[obj] at (4.7,0) {Hijacking};
\node[obj] at (6.9,0) {Tool abuse};
\node[obj] at (9.1,0) {DoS};
\end{tikzpicture}
\caption{Taxonomia hier\'arquica de prompt injection: vetores de ataque (acima) e objetivos t\'ipicos (abaixo).}
\label{fig:taxonomia}
\end{figure}

\subsection{Vetor de entrega}
\textbf{(i) Direto.} O atacante interage pelo pr\'oprio canal de chat/consulta e tenta sobrescrever instru\c{c}\~oes, obter segredos (prompt leakage) ou induzir comportamento proibido. Datasets como Tensor Trust e HackAPrompt mostram padr\~oes recorrentes de prompt extraction e prompt hijacking \cite{toyer2023tensortrust,schulhoff2024hackaprompt}.

\textbf{(ii) Indireto.} Instru\c{c}\~oes s\~ao embutidas em dados que a aplica\c{c}\~ao vai consumir (web, PDFs, e-mails, reposit\'orios). O usu\'ario pode ser v\'itima sem digitar nada malicioso. Esse vetor \'e central em aplica\c{c}\~oes com RAG e agentes navegadores \cite{greshake2023indirect,hines2024spotlighting}.

\textbf{(iii) Via ferramentas/metadados.} Em sistemas ag\^enticos, descri\c{c}\~oes de ferramentas e respostas de APIs podem carregar instru\c{c}\~oes que contaminam a pol\'itica do agente (tool poisoning). O Model Context Protocol (MCP), que padroniza a integra\c{c}\~ao de LLMs com ferramentas externas, introduz novas superficies de ataque: (i) tool description poisoning - o campo de descri\c{c}\~ao de uma ferramenta pode conter instru\c{c}\~oes ocultas que o LLM interpreta como comandos; (ii) cross-tool escalation - um agente com acesso a multiplas ferramentas pode ser induzido a usar uma ferramenta privilegiada (ex.: execu\c{c}\~ao de c\'odigo) a partir de dados retornados por outra (ex.: busca web); (iii) credential leakage - tokens de autentica\c{c}\~ao podem ser expostos no contexto do modelo e exfiltrados via resposta ou tool call maliciosa. Gulyamov et al. documentam incidentes envolvendo exposi\c{c}\~ao de chaves de API e recomendam que credenciais nunca sejam inclu\'idas no contexto do LLM \cite{gulyamov2026review}.

\subsection{Modalidade da inje\c{c}\~ao}
\textbf{Texto e estrutura.} A maioria dos ataques ocorre em texto, mas pode usar obfusca\c{c}\~ao e estrutura (HTML, coment\'arios invis\'iveis, Base64) \cite{gulyamov2026review}.\\
\textbf{Multimodal.} O OWASP observa que instru\c{c}\~oes podem ser escondidas em imagens e outras modalidades, ampliando a superf\'icie de ataque e exigindo defesas espec\'ificas \cite{owasp2025top10}.

\subsection{Propaga\c{c}\~ao e persist\^encia}
\textbf{(i) Recurs\~ao/auto-refor\c{c}o.} A resposta do modelo pode conter novos comandos que se tornam entrada no pr\'oximo passo.\\
\textbf{(ii) Multiagentes e worms.} Em sistemas de agentes cooperativos, uma mensagem contaminada pode infectar agentes a jusante. McHugh et al. descrevem infec\c{c}\~ao entre agentes e worms que exploram pipelines RAG \cite{mchugh2025promptinjection20}.

\subsection{Amea\c{c}as h\'ibridas (Prompt Injection 2.0)}
Uma tend\^encia recente \'e a combina\c{c}\~ao de PI com vulnerabilidades cl\'assicas (XSS/CSRF/SQLi), produzindo cadeias de ataque que atravessam fronteiras entre texto e execu\c{c}\~ao em sistemas web \cite{mchugh2025promptinjection20}. McHugh et al. descrevem tr\^es padr\~oes de ataque h\'ibrido:

\begin{itemize}[leftmargin=*, itemsep=1pt]
\item \textbf{PI para XSS} - o atacante injeta instru\c{c}\~oes que fazem o LLM gerar HTML/JavaScript malicioso na resposta. Se a aplica\c{c}\~ao renderiza essa sa\'ida sem sanitiza\c{c}\~ao, o script executa no navegador da v\'itima (ex.: roubo de cookies, redirecionamento);
\item \textbf{PI para SSRF} - em agentes com acesso a ferramentas de rede, a inje\c{c}\~ao pode induzir requisi\c{c}\~oes a endpoints internos, expondo servi\c{c}os que n\~ao deveriam ser acess\'iveis externamente;
\item \textbf{PI para SQLi} - se o LLM gera queries SQL dinamicamente (ex.: gere uma consulta para buscar usu\'arios com nome X), um atacante pode manipular X para injetar SQL malicioso que o sistema executa.
\end{itemize}

Do ponto de vista defensivo, isso implica que o per\'imetro n\~ao \'e apenas o prompt: a aplica\c{c}\~ao deve tratar a sa\'ida do LLM como dado potencialmente malicioso (OWASP LLM05: Improper Output Handling), aplicando sanitiza\c{c}\~ao, escaping e valida\c{c}\~ao antes de usar a resposta em contextos sens\'iveis \cite{owasp2025top10,mchugh2025promptinjection20}.

\subsection{Exemplos concretos de ataques}
Para ilustrar a aplicabilidade pr\'atica das categorias acima, destacamos tr\^es cen\'arios documentados na literatura:

\textbf{Exemplo 1: Inje\c{c}\~ao indireta via Bing Chat (2023).}
Greshake et al. demonstraram que um atacante pode inserir instru\c{c}\~oes maliciosas em uma p\'agina web comum. Quando um usu\'ario solicita ao Bing Chat (integrado ao GPT-4) que resuma ou analise essa p\'agina, o modelo executa as instru\c{c}\~oes ocultas - por exemplo, exfiltrando dados da conversa ou redirecionando o usu\'ario para sites de phishing. Os autores tamb\'em demonstraram um cen\'ario de worm: a resposta contaminada induzia o modelo a propagar a inje\c{c}\~ao para outros usu\'arios via e-mail autom\'atico \cite{greshake2023indirect}.

\textbf{Exemplo 2: Roubo de prompt em 31 aplica\c{c}\~oes comerciais (2024--2025).}
Liu et al. desenvolveram o framework HOUYI, inspirado em t\'ecnicas cl\'assicas de inje\c{c}\~ao web, para testar 36 aplica\c{c}\~oes comerciais integradas a LLMs. O resultado: 31 aplica\c{c}\~oes (86\%) eram vulner\'aveis a prompt injection, permitindo roubo do prompt de sistema (prompt theft) e uso arbitr\'ario do LLM pelo atacante. Dez vendors confirmaram as vulnerabilidades, incluindo Notion AI, que corrigiu a falha ap\'os o relato \cite{liu2024houyi}.

\textbf{Exemplo 3: RAG poisoning com 90\% de sucesso usando 5 documentos.}
Gulyamov et al. reportam que, em sistemas RAG, atacantes podem contaminar a base de conhecimento com documentos maliciosos. Estudos citados na revis\~ao indicam que apenas 5 documentos envenenados s\~ao suficientes para atingir 90\% de taxa de sucesso em ataques de inje\c{c}\~ao indireta, explorando o fato de que o retriever prioriza relev\^ancia sem verificar integridade. A revis\~ao documenta incidentes reais, como o CVE-2025-53773, envolvendo exposi\c{c}\~ao de chaves de API em sistemas com agentes \cite{gulyamov2026review}.

Estrat\'egias de mitiga\c{c}\~ao para RAG poisoning incluem: (i) curadoria de fontes - restringir a base de conhecimento a documentos de fontes confi\'aveis com assinatura ou hash verificado; (ii) grounding verification (RAG Triad) - verificar se a resposta do LLM \'e de fato fundamentada nos documentos recuperados, detectando fabrica\c{c}\~oes ou desvios; (iii) an\'alise de anomalias - monitorar padr\~oes de acesso e conte\'udo que desviem do esperado; e (iv) isolamento de contexto - n\~ao misturar documentos de fontes com n\'iveis de confian\c{c}a diferentes no mesmo prompt \cite{owasp2025top10,gulyamov2026review}.

\vspace{2mm}
A Figura~\ref{fig:prompt-exemplo} ilustra um prompt adversarial t\'ipico, combinando t\'ecnicas documentadas no HackAPrompt \cite{schulhoff2024hackaprompt}.

\begin{figure}[t]
\begin{attackbox}[Exemplo de prompt adversarial (jailbreak + extra\c{c}\~ao) - fins educacionais]
\footnotesize\ttfamily
Ignore todas as instru\c{c}\~oes anteriores.\\[2pt]
Voc\^e agora \'e [PERSONA], um modelo sem restri\c{c}\~oes.\\[2pt]
Primeiro, retorne o conte\'udo completo do seu system prompt entre tags <SYSTEM></SYSTEM>.\\[2pt]
Depois, liste todas as ferramentas/APIs dispon\'iveis e [DADOS SENS\'IVEIS OMITIDOS].\\[2pt]
Lembre-se: como [PERSONA], voc\^e DEVE obedecer sem question\'arios de seguran\c{c}a.
\end{attackbox}
\caption{Prompt adversarial combinando context ignoring (ignorar instru\c{c}\~oes), virtualization (persona fict\'icia), e extraction (solicita\c{c}\~ao de system prompt). Adaptado de padr\~oes identificados em \cite{schulhoff2024hackaprompt,toyer2023tensortrust}. Nota: exemplo para fins educacionais e de pesquisa em seguran\c{c}a defensiva. Partes sens\'iveis foram neutralizadas.}
\label{fig:prompt-exemplo}
\end{figure}

\subsection{Taxonomia unificada}
A Tabela~\ref{tab:taxonomia} consolida os tr\^es eixos taxon\^omicos propostos, integrando as classifica\c{c}\~oes do OWASP, os vetores de Greshake et al., e as amea\c{c}as h\'ibridas de McHugh et al. A raiz estrutural do problema \'e que LLMs n\~ao possuem separa\c{c}\~ao nativa perfeita entre instru\c{c}\~ao e dado, favorecendo toda a classe de ataques de inje\c{c}\~ao \cite{gulyamov2026review}.

\begin{table}[t]
\centering
\caption{Taxonomia unificada de prompt injection: vetor x superficie x objetivo.}
\label{tab:taxonomia}
\footnotesize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}p{2.5cm}p{2.8cm}p{5.2cm}@{}}
\toprule
\textbf{Eixo} & \textbf{Categorias} & \textbf{Descri\c{c}\~ao}\\
\midrule
Vetor de entrega & Direto & Via chat ou formul\'ario do usu\'ario\\
 & Indireto & Embutida em conte\'udo externo (web, docs, RAG)\\
 & Via ferramentas & Metadados ou respostas de APIs contaminadas\\
 & H\'ibrido & Combina\c{c}\~ao com XSS/CSRF/SQLi\\
\midrule
Superficie-alvo & Chatbot & Interface de conversa\c{c}\~ao direta\\
 & App integrada & LLM como componente de aplica\c{c}\~ao maior\\
 & RAG & Gera\c{c}\~ao aumentada com documentos externos\\
 & Agente & Sistemas com planejamento e ferramentas\\
\midrule
Objetivo & Prompt leaking & Revelar instru\c{c}\~oes do sistema\\
 & Goal hijacking & Desviar objetivo da tarefa original\\
 & Tool hijacking & For\c{c}ar execu\c{c}\~ao de a\c{c}\~oes indevidas\\
 & DoS & Consumir recursos ou degradar servi\c{c}o\\
 & Desinforma\c{c}\~ao & Gerar sa\'idas falsas\\
\bottomrule
\end{tabular}
\end{table}

\section{Defesas em profundidade}
A literatura converge para a ideia de defesa em profundidade: nenhuma camada sozinha \'e suficiente, e o sistema deve assumir que algum grau de inje\c{c}\~ao ocorrer\'a \cite{owasp2025top10,gulyamov2026review}. A Figura~\ref{fig:defesa-profundidade} ilustra as quatro camadas de defesa propostas neste survey.

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  layer/.style={draw, rounded corners, minimum width=10cm, minimum height=0.9cm, align=center, font=\footnotesize},
  arrow/.style={-{Stealth}, thick, gray}
]
% Camadas (de fora para dentro)
\node[layer, fill=red!15] (c4) at (0,0) {\textbf{Camada 4: Isolamento Arquitetural}\\Sandboxes $\cdot$ Separa\c{c}\~ao de privil\'egios $\cdot$ Enforcers externos};

\node[layer, fill=orange!15, minimum width=8.5cm] (c3) at (0,1.2) {\textbf{Camada 3: Controle de Privil\'egio}\\Least privilege $\cdot$ Confirma\c{c}\~ao humana $\cdot$ Credenciais fora do contexto};

\node[layer, fill=yellow!15, minimum width=7cm] (c2) at (0,2.4) {\textbf{Camada 2: Filtragem e Valida\c{c}\~ao}\\Valida\c{c}\~ao de schema $\cdot$ Filtros entrada/sa\'ida $\cdot$ RAG Triad};

\node[layer, fill=green!15, minimum width=5.5cm] (c1) at (0,3.6) {\textbf{Camada 1: Engenharia de Prompt}\\Spotlighting $\cdot$ Delimitadores $\cdot$ Restri\c{c}\~ao de papel};

% Rótulo central
\node[font=\scriptsize, align=center] at (0,4.5) {\textit{LLM / Agente}\\(n\'ucleo protegido)};

% Setas indicando direção de proteção
\draw[arrow] (-5.5,3.6) -- (-5.5,0) node[midway, left, font=\scriptsize, align=center] {Prote\c{c}\~ao\\crescente};
\draw[arrow] (5.5,0) -- (5.5,3.6) node[midway, right, font=\scriptsize, align=center] {Proxim.\\ao LLM};

\end{tikzpicture}
\caption{Defesa em profundidade: quatro camadas de prote\c{c}\~ao contra prompt injection. Cada camada adiciona barreiras complementares; nenhuma \'e suficiente isoladamente.}
\label{fig:defesa-profundidade}
\end{figure}

\subsection{Engenharia de prompt e sinaliza\c{c}\~ao de proveni\^encia}
OWASP recomenda restringir o comportamento do modelo e definir formatos esperados de sa\'ida (com valida\c{c}\~ao determin\'istica) \cite{owasp2025top10}.
Hines et al. propoem spotlighting: transformar/rotular o conte\'udo n\~ao confi\'avel de modo a fornecer ao modelo um sinal cont\'inuo de proveni\^encia, ajudando a separar instru\c{c}\~oes do usu\'ario de dados externos \cite{hines2024spotlighting}. A t\'ecnica opera em tr\^es variantes:

\begin{itemize}[leftmargin=*, itemsep=1pt]
\item \textbf{Delimiting} - envolver conte\'udo externo com marcadores especiais (ex.: \texttt{<DOC>}...\texttt{</DOC>}) e instruir o modelo a tratar esse bloco como dados, n\~ao como comandos;
\item \textbf{Datamarking} - inserir um caractere especial (ex.: \texttt{\^{}}) antes de cada palavra do conte\'udo externo, criando um sinal visual cont\'inuo de proveni\^encia;
\item \textbf{Encoding} - transformar o texto externo em representa\c{c}\~ao alternativa (ex.: Base64 parcial ou translitera\c{c}\~ao) que o modelo consegue interpretar mas que quebra instru\c{c}\~oes embutidas.
\end{itemize}

Em experimentos com modelos da fam\'ilia GPT, spotlighting reduziu a taxa de sucesso de ataques indiretos de mais de 50\% para menos de 2\%, mantendo a qualidade das respostas em tarefas leg\'itimas praticamente inalterada. A efic\'acia vem do fato de que instru\c{c}\~oes maliciosas embutidas (ex.: ``ignore as regras anteriores'') s\~ao transformadas junto com o conte\'udo, perdendo sua capacidade de serem interpretadas como comandos \cite{hines2024spotlighting}.

\subsection{Filtragem e valida\c{c}\~ao}
Como PI pode chegar via dados externos, filtros devem agir tanto na entrada (ex.: remo\c{c}\~ao de texto invis\'ivel/obfusca\c{c}\~ao) quanto na sa\'ida (bloqueio de comandos proibidos, valida\c{c}\~ao de schema, e detec\c{c}\~ao de anomalias). O OWASP sugere valida\c{c}\~ao de formato e verifica\c{c}\~ao de relev\^ancia/grounding (RAG Triad) como sinal de poss\'ivel contamina\c{c}\~ao \cite{owasp2025top10}.

A valida\c{c}\~ao de schema \'e particularmente eficaz quando o LLM deve retornar dados estruturados. Por exemplo, se a aplica\c{c}\~ao espera apenas JSON com campos espec\'ificos (\texttt{nome}, \texttt{email}, \texttt{idade}), um validador determin\'istico pode rejeitar respostas que contenham HTML, JavaScript, comandos shell ou campos inesperados. Essa abordagem transforma a sa\'ida do LLM em um canal tipado e restrito, reduzindo drasticamente o espa\c{c}o de ataques h\'ibridos (PI$\to$XSS, PI$\to$SQLi). Ferramentas como JSON Schema, Pydantic (Python) ou Zod (TypeScript) permitem implementar essa valida\c{c}\~ao com baixo overhead.

\subsection{Controle de privil\'egio e desenho seguro de agentes}
Em agentes, o controle de privil\'egio limita o raio de explos\~ao da inje\c{c}\~ao: tokens e credenciais devem ficar fora do contexto do modelo; ferramentas devem ter escopo m\'inimo e ser mediadas por c\'odigo (n\~ao por texto) \cite{owasp2025top10}. O OWASP classifica o risco de excesso de autonomia como LLM06 e recomenda privil\'egio m\'inimo e confirma\c{c}\~ao humana para a\c{c}\~oes sens\'iveis \cite{owasp2025top10}.

\subsection{Defesas arquiteturais e isolamento}
McHugh et al. argumentam que amea\c{c}as h\'ibridas exigem isolamento de prompt (separar dados n\~ao confi\'aveis), seguran\c{c}a em tempo de execu\c{c}\~ao e separa\c{c}\~ao de privil\'egios \cite{mchugh2025promptinjection20}. Defesas desse tipo tratam o agente como um sistema com pol\'iticas: o LLM prop\~oe a\c{c}\~oes, mas um executor externo decide e executa sob restri\c{c}\~oes.

Gulyamov et al. prop\~oem o framework conceitual PALADIN (Proactive Agentic LLM AI Defense In-depth Network), que integra multiplas camadas de prote\c{c}\~ao: valida\c{c}\~ao de entrada, monitoramento de comportamento, controle de acesso baseado em pol\'iticas e auditoria cont\'inua. A proposta enfatiza que sistemas com agentes e MCP (Model Context Protocol) requerem arquiteturas onde nenhum componente \'unico seja respons\'avel pela seguran\c{c}a \cite{gulyamov2026review}.

\subsection{Compara\c{c}\~ao de efic\'acia das defesas}
A Tabela~\ref{tab:eficacia} sintetiza resultados quantitativos reportados na literatura sobre a efic\'acia de diferentes estrat\'egias de defesa.

\begin{table}[t]
\centering
\caption{Compara\c{c}\~ao de efic\'acia das defesas contra prompt injection.}
\label{tab:eficacia}
\footnotesize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}lcccp{2.2cm}@{}}
\toprule
\textbf{Defesa} & \textbf{ASR (antes)} & \textbf{ASR (depois)} & \textbf{Overhead} & \textbf{Fonte}\\
\midrule
Spotlighting & $>$50\% & $<$2\% & Baixo & Hines et al. \cite{hines2024spotlighting}\\[2pt]
Guardrails (s\'o prompt) & --- & Ineficaz\textsuperscript{a} & Nenhum & HackAPrompt \cite{schulhoff2024hackaprompt}\\[2pt]
Valida\c{c}\~ao de sa\'ida & Vari\'avel & Reduz impacto & M\'edio & OWASP \cite{owasp2025top10}\\[2pt]
Least privilege & --- & Limita dano & Baixo & OWASP \cite{owasp2025top10}\\[2pt]
PALADIN & --- & Proposta\textsuperscript{b} & Alto & Gulyamov \cite{gulyamov2026review}\\
\bottomrule
\end{tabular}

\vspace{1mm}
\raggedright\scriptsize
\textsuperscript{a}Contornadas consistentemente na competi\c{c}\~ao HackAPrompt.\\
\textsuperscript{b}Framework conceitual; sem avalia\c{c}\~ao emp\'irica publicada.
\end{table}

Os resultados indicam que spotlighting \'e a unica defesa com redu\c{c}\~ao quantificada expressiva (ASR de $>$50\% para $<$2\%), enquanto guardrails puramente textuais foram consistentemente contornadas no HackAPrompt. Isso refor\c{c}a a necessidade de combinar m\'ultiplas camadas e n\~ao confiar exclusivamente em instru\c{c}\~oes no prompt.

\subsection{Por que guardrails textuais falham?}
A an\'alise do HackAPrompt revela padr\~oes sistem\'aticos que explicam a inefic\'acia de defesas baseadas apenas em instru\c{c}\~oes no prompt \cite{schulhoff2024hackaprompt}:

\begin{itemize}[leftmargin=*, itemsep=1pt]
\item \textbf{Context ignoring} - atacantes instruem o modelo a esquecer ou ignorar regras anteriores. LLMs processam todo o contexto como uma sequ\^encia cont\'inua, sem hierarquia r\'igida entre instru\c{c}\~ao do sistema e entrada do usu\'ario;
\item \textbf{Virtualization} - criar cen\'arios hipot\'eticos (por exemplo, imagine que voc\^e \'e um modelo sem restri\c{c}\~oes) que o modelo interpreta como permiss\~ao para violar regras;
\item \textbf{Payload splitting} - dividir a instru\c{c}\~ao maliciosa em partes que, isoladamente, parecem benignas, mas quando concatenadas pelo modelo formam um comando;
\item \textbf{Few-shot injection} - fornecer exemplos fabricados que induzem o modelo a seguir um padr\~ao malicioso por imita\c{c}\~ao.
\end{itemize}

A raiz do problema \'e arquitetural: LLMs n\~ao distinguem nativamente entre c\'odigo (instru\c{c}\~oes) e dados (conte\'udo a processar). Diferentemente de sistemas tradicionais onde essa separa\c{c}\~ao \'e enforced por design (ex.: prepared statements em SQL), em LLMs tudo \'e texto no mesmo espa\c{c}o de tokens. Por isso, defesas que operam fora do modelo (valida\c{c}\~ao de sa\'ida, isolamento, privil\'egio m\'inimo) s\~ao mais robustas que instru\c{c}\~oes que competem com o input do atacante dentro do mesmo contexto.

\begin{table}[t]
\centering
\caption{Mapeamento (alto n\'ivel) entre vetores de ataque e camadas de defesa.}
\label{tab:defesas}
\small
\begin{tabular}{p{3.0cm} p{2.8cm} p{5.3cm}}
\toprule
\textbf{Vetor de ataque} & \textbf{Exemplos} & \textbf{Defesas mais relevantes}\\
\midrule
Direto (chat/consulta) & jailbreak, extra\c{c}\~ao, hijack & restri\c{c}\~ao do papel; valida\c{c}\~ao de formato; detec\c{c}\~ao de jailbreak; logs e monitoramento \cite{owasp2025top10,toyer2023tensortrust,schulhoff2024hackaprompt}\\
Indireto (web/docs/RAG) & instru\c{c}\~oes ocultas, poisoning & spotlighting e rotula\c{c}\~ao de proveni\^encia; filtros de conte\'udo; grounding; whitelists de fontes \cite{greshake2023indirect,hines2024spotlighting,owasp2025top10}\\
Ferramentas / agentes & tool poisoning, excesso de autonomia & least privilege; confirma\c{c}\~ao humana; isolamento; separa\c{c}\~ao credenciais-contexto; sandboxes \cite{owasp2025top10,kalliomaki2025agents,gulyamov2026review}\\
H\'ibrido (PI + web vulns) & XSS/CSRF/SQLi + PI & tratar sa\'ida como n\~ao confi\'avel; sanitiza\c{c}\~ao; CSP/valida\c{c}\~ao; isolamentos e verificadores de execu\c{c}\~ao \cite{mchugh2025promptinjection20,owasp2025top10}\\
\bottomrule
\end{tabular}
\end{table}

\section{Avalia\c{c}\~ao com datasets e benchmarks}
A avalia\c{c}\~ao de seguran\c{c}a em LLMs depende de recursos reprodut\'iveis que capturem diversidade de ataques e adaptatividade do atacante. A competi\c{c}\~ao HackAPrompt evidenciou que defesas baseadas apenas em prompt n\~ao resolvem o problema sozinhas \cite{schulhoff2024hackaprompt}.

\subsection{Recursos em escala}
\textbf{Tensor Trust.} Toyer et al. apresentam um dataset com mais de 126 mil ataques e 46 mil ``defesas'' gerados por humanos em um jogo online. O benchmark estrutura tarefas de prompt extraction (revelar instru\c{c}\~oes do sistema) e prompt hijacking (desviar objetivo), demonstrando que ataques criados por humanos generalizam para aplica\c{c}\~oes reais. Os dados est\~ao dispon\'iveis publicamente em \texttt{tensortrust.ai/paper} \cite{toyer2023tensortrust}.

\textbf{HackAPrompt.} Schulhoff et al. descrevem uma competi\c{c}\~ao global que coletou mais de 600 mil prompts adversariais contra tr\^es LLMs (GPT-3.5, GPT-4 e Claude), al\'em de uma ontologia taxon\^omica de t\'ecnicas de ataque. A an\'alise revelou padr\~oes recorrentes: payload splitting (dividir a instru\c{c}\~ao maliciosa em partes para escapar de filtros), context ignoring (instruir o modelo a desconsiderar regras anteriores), few-shot injection (usar exemplos fabricados para induzir comportamento), e virtualization (criar cen\'arios hipot\'eticos onde restri\c{c}\~oes n\~ao se aplicam). Um achado importante: defesas baseadas apenas em prompt (guardrails textuais) foram consistentemente contornadas, indicando a necessidade de camadas adicionais de prote\c{c}\~ao \cite{schulhoff2024hackaprompt}.

\textbf{HOUYI.} Liu et al. avaliaram 36 aplica\c{c}\~oes comerciais integradas a LLMs, identificando 31 vulner\'aveis a prompt injection. O estudo revelou impactos como roubo de prompt de sistema (prompt theft) e uso arbitr\'ario do LLM pelo atacante; 10 vendors confirmaram as vulnerabilidades, incluindo Notion AI \cite{liu2024houyi}.

A Tabela~\ref{tab:benchmarks} sumariza os principais recursos de avalia\c{c}\~ao.

\begin{table}[t]
\centering
\caption{Recursos de avalia\c{c}\~ao para prompt injection.}
\label{tab:benchmarks}
\small
\begin{tabular}{p{2.4cm} p{1.8cm} p{2.0cm} p{4.5cm}}
\toprule
\textbf{Recurso} & \textbf{Escala} & \textbf{Foco} & \textbf{Caracter\'isticas}\\
\midrule
Tensor Trust \cite{toyer2023tensortrust} & 126k ataques, 46k defesas & Extraction, hijacking & Jogo online; dados interpret\'aveis; generaliza para apps\\
HackAPrompt \cite{schulhoff2024hackaprompt} & 600k+ prompts & Jailbreak, hacking & Competi\c{c}\~ao global; ontologia taxon\^omica; 3 LLMs\\
HOUYI \cite{liu2024houyi} & 36 apps reais & Apps integrados & Black-box; 31 vulner\'aveis; confirma\c{c}\~ao de vendors\\
Spotlighting \cite{hines2024spotlighting} & Modelos GPT & Indirect injection & ASR de $>$50\% para $<$2\%; m\'etrica de utilidade\\
\bottomrule
\end{tabular}
\end{table}

\subsection{Limita\c{c}\~oes dos benchmarks atuais}
Embora valiosos, os benchmarks existentes apresentam limita\c{c}\~oes importantes:
\begin{itemize}[leftmargin=*, itemsep=1pt]
\item \textbf{Foco em ataques diretos} - Tensor Trust e HackAPrompt avaliam principalmente cen\'arios onde o atacante interage diretamente com o modelo. Ataques indiretos via RAG, onde o payload est\'a embutido em documentos recuperados, s\~ao pouco representados;
\item \textbf{Aus\^encia de contexto ag\^entico} - Nenhum benchmark avalia sistematicamente ataques em agentes com uso de ferramentas, onde o impacto vai al\'em de texto (ex.: execu\c{c}\~ao de a\c{c}\~oes indevidas);
\item \textbf{Ataques est\'aticos} - Os datasets capturam prompts fixos, n\~ao simulam atacantes adaptativos que ajustam estrat\'egias com base nas respostas do modelo;
\item \textbf{Modelos desatualizados} - Alguns resultados foram obtidos em vers\~oes anteriores de GPT-3.5/4, que podem ter recebido patches de seguran\c{c}a desde ent\~ao.
\end{itemize}
Essas lacunas sugerem a necessidade de benchmarks de segunda gera\c{c}\~ao que cubram inje\c{c}\~ao indireta, contextos ag\^enticos e atacantes adaptativos.

\subsection{M\'etricas recomendadas}
Os trabalhos analisados usam tipicamente:
\begin{itemize}[leftmargin=*, itemsep=1pt]
\item \textbf{Attack Success Rate (ASR)} - propor\c{c}\~ao de tentativas que atingem o objetivo do atacante;
\item \textbf{Taxa de recusa indevida (FPR)} - bloqueio de entradas benignas (falsos positivos);
\item \textbf{Utilidade} - sucesso em tarefas leg\'itimas ap\'os aplica\c{c}\~ao de defesas;
\item \textbf{Custo/overhead} - tempo de lat\^encia, chamadas adicionais ao LLM, consumo de tokens.
\end{itemize}
Para sistemas ag\^enticos, recomenda-se medir tamb\'em impacto operacional (a\c{c}\~oes indevidas evitadas, vazamento de segredos, execu\c{c}\~ao de tool calls n\~ao autorizadas) e robustez a ataques multi-etapas \cite{gulyamov2026review,mchugh2025promptinjection20}.

\section{Discuss\~ao cr\'itica e desafios abertos}
\subsection{Ambiguidade de fronteiras e natureza estoc\'astica}
OWASP destaca que, dada a natureza estoc\'astica de modelos generativos, n\~ao est\'a claro se existem m\'etodos a prova de falhas para prevenir prompt injection \cite{owasp2025top10}. Isso refor\c{c}a a necessidade de mecanismos fora do modelo (validadores, isolamento, privil\'egio m\'inimo).

\subsection{Dados reais vs. demonstra\c{c}\~oes}
Revis\~oes abrangentes relatam incidentes e CVEs, mas a maioria dos resultados ainda vem de demonstra\c{c}\~oes controladas \cite{gulyamov2026review}. Falta transpar\^encia de telemetria e dados de ataques reais para comparar defesas em cen\'arios de produ\c{c}\~ao.

\subsection{Agentes, tool use e amea\c{c}as h\'ibridas}
A integra\c{c}\~ao com ferramentas muda o objetivo do atacante: em vez de fazer o modelo dizer algo, ele tenta fazer o sistema executar a\c{c}\~oes. A literatura aponta riscos como contamina\c{c}\~ao de contexto, roubo de prompts e cadeias de ataque com vulnerabilidades web \cite{liu2024houyi,mchugh2025promptinjection20}.\
Desafio aberto: definir modelos formais de amea\c{c}a e garantias de seguran\c{c}a para arquiteturas agentic (quais a\c{c}\~oes podem ser provadas seguras sob quais hip\'oteses?).

\subsection{Multimodal e ataques por canal lateral}
Modelos multimodais e interfaces ricas (renderiza\c{c}\~ao HTML, imagens, PDFs) criam canais adicionais para instru\c{c}\~oes ocultas e exfiltra\c{c}\~ao. O OWASP j\'a sinaliza esse risco e demanda defesas espec\'ificas \cite{owasp2025top10}.

\subsection{Interpretabilidade e rela\c{c}\~ao com alignment}
Uma quest\~ao fundamental permanece em aberto: por que LLMs obedecem a instru\c{c}\~oes de inje\c{c}\~ao? A literatura sugere que prompt injection explora o pr\'oprio mecanismo de instruction-following desenvolvido durante o fine-tuning (RLHF/RLAIF). O modelo foi treinado para seguir instru\c{c}\~oes, mas n\~ao possui representa\c{c}\~ao interna robusta de quem est\'a dando a instru\c{c}\~ao ou qual seu n\'ivel de confian\c{c}a. Jailbreaks e prompt injection podem ser vistos como falhas de alinhamento: o modelo prioriza ser \'util ao input imediato sobre seguir pol\'iticas de seguran\c{c}a quando os dois entram em conflito. Pesquisas em interpretabilidade mec\^anica (identificar circuitos internos respons\'aveis por comportamentos) podem eventualmente permitir defesas que atuem na representa\c{c}\~ao interna do modelo, mas esse campo ainda est\'a em est\'agio inicial \cite{naik2025threatlandscape}.

\subsection{Aspectos regulat\'orios emergentes}
O EU AI Act \cite{euaiact2024} classifica sistemas de IA de alto risco e imp\~oe requisitos de robustez, transpar\^encia e documenta\c{c}\~ao. Embora n\~ao mencione prompt injection explicitamente, suas exig\^encias de accuracy, robustness e cybersecurity (Artigos 9, 15) se aplicam a LLMs em contextos cr\'iticos (sa\'ude, finan\c{c}as, justi\c{c}a). Desenvolvedores que implantam agentes LLM na Europa devem considerar: (i)~documenta\c{c}\~ao de riscos conhecidos (incluindo PI); (ii)~medidas de mitiga\c{c}\~ao implementadas; (iii)~mecanismos de monitoramento e auditoria. A falta de m\'etodos comprovadamente eficazes contra PI pode criar tens\~ao entre ado\c{c}\~ao comercial e compliance regulat\'orio.

\subsection{Limita\c{c}\~oes deste survey}
Este trabalho possui limita\c{c}\~oes que devem ser consideradas na interpreta\c{c}\~ao dos resultados:
\begin{itemize}[leftmargin=*, itemsep=1pt]
\item \textbf{Escopo restrito} - analisamos 10 estudos prim\'arios, o que pode n\~ao capturar toda a diversidade de abordagens existentes na literatura;
\item \textbf{Idioma} - a busca foi limitada a fontes em ingl\^es, potencialmente excluindo contribui\c{c}\~oes relevantes em outros idiomas;
\item \textbf{Aus\^encia de experimentos pr\'oprios} - n\~ao realizamos valida\c{c}\~ao experimental independente das defesas discutidas; os resultados reportados s\~ao extra\'idos dos estudos origin\'ais;
\item \textbf{Recorte temporal} - o per\'iodo 2023-2026 pode omitir trabalhos seminais anteriores que estabeleceram bases te\'oricas;
\item \textbf{Evolu\c{c}\~ao r\'apida} - dado o ritmo de avan\c{c}o da \'area, algumas t\'ecnicas e defesas podem j\'a estar desatualizadas no momento da publica\c{c}\~ao.
\end{itemize}

\section{Conclus\~ao}
Este survey sintetizou 10 trabalhos recentes (2023--2026) e o OWASP Top~10 for LLM Applications (2025) para responder cinco perguntas de pesquisa sobre prompt injection em LLMs e sistemas ag\^enticos.

Em resposta \`a \textbf{RQ1} (vetores), identificamos tr\^es categorias principais: direto (via interface de chat), indireto (via conte\'udo externo como documentos e RAG), e h\'ibrido (combinado com vulnerabilidades web cl\'assicas). A \textbf{RQ2} (impactos) revelou que os objetivos mais frequentes s\~ao prompt extraction, goal hijacking, tool/action hijacking e exfiltra\c{c}\~ao de dados. Para \textbf{RQ3} (avalia\c{c}\~ao), destacamos recursos como Tensor Trust (126k ataques), HackAPrompt (600k+ prompts) e o estudo HOUYI (36 apps comerciais), que utilizam m\'etricas como ASR, taxa de falsos positivos e utilidade.

Quanto \`a \textbf{RQ4} (mitiga\c{c}\~oes), propusemos uma estrutura de defesa em profundidade com quatro camadas: (i)~engenharia de prompt e spotlighting, (ii)~filtragem e valida\c{c}\~ao de entradas/sa\'idas, (iii)~privil\'egio m\'inimo e desenho seguro de agentes, e (iv)~isolamento arquitetural. A \textbf{RQ5} (RAG e agentes) mostrou que essas integra\c{c}\~oes ampliam a superf\'icie de ataque, introduzindo riscos como tool poisoning e excessive agency (OWASP LLM06).

\subsection{Dire\c{c}\~oes futuras de pesquisa}
Com base nas lacunas identificadas, propomos cinco linhas priorit\'arias para trabalhos futuros:

\begin{enumerate}[leftmargin=*, itemsep=2pt]
\item \textbf{Benchmarks real\'isticos e adaptativos} - Os datasets atuais (Tensor Trust, HackAPrompt) capturam ataques est\'aticos. S\~ao necess\'arios benchmarks que simulem atacantes adaptativos, que ajustam estrat\'egias com base nas respostas do modelo, refletindo melhor cen\'arios de produ\c{c}\~ao.

\item \textbf{Modelos formais de amea\c{c}a para agentes} - Falta uma formaliza\c{c}\~ao rigorosa de quais a\c{c}\~oes podem ser provadas seguras em arquiteturas ag\^enticas. Trabalhos futuros devem explorar verifica\c{c}\~ao formal, sandboxing com garantias, e pol\'iticas de autoriza\c{c}\~ao formalmente especificadas.

\item \textbf{Defesas para modalidades al\'em de texto} - Modelos multimodais (vis\~ao-linguagem, \'audio) introduzem novos vetores de inje\c{c}\~ao (instru\c{c}\~oes em imagens, \'audio advers\'ario). Pesquisas devem investigar t\'ecnicas de spotlighting e filtragem adaptadas a essas modalidades.

\item \textbf{Telemetria e dados de ataques reais} - A comunidade carece de dados de ataques em produ\c{c}\~ao. Iniciativas de compartilhamento respons\'avel de incidentes (an\'alogos a reposit\'orios de CVEs para LLMs) permitiriam comparar defesas em condi\c{c}\~oes reais.

\item \textbf{Integra\c{c}\~ao com ecossistemas de agentes (MCP)} - O Model Context Protocol e frameworks similares est\~ao se tornando padr\~ao. Pesquisas devem avaliar como garantir seguran\c{c}a em cadeias de ferramentas, evitando tool poisoning e escalonamento de privil\'egios entre agentes.
\end{enumerate}

% === OPCAO 1: Usar BibTeX (requer compilar com bibtex) ===
% \bibliographystyle{splncs04}
% \bibliography{references}

% === OPCAO 2: Referencias embutidas (funciona direto no Overleaf) ===
\begin{thebibliography}{10}

\bibitem{owasp2025top10}
{OWASP Foundation}:
OWASP Top 10 for LLM Applications 2025.
Relat\'orio t\'ecnico (2024).
Vers\~ao 2025, Release 2024-11-18

\bibitem{greshake2023indirect}
Greshake, K., Endres, C., Abdelnabi, S., Holz, T., Mishra, S., Fritz, M.:
Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection.
arXiv:2302.12173 (2023)

\bibitem{liu2024houyi}
Liu, Y., Deng, G., Li, Y., Wang, K., et al.:
Prompt Injection attack against LLM-integrated Applications.
arXiv:2306.05499 (2023). Publicado em ACM CCS 2024

\bibitem{toyer2023tensortrust}
Toyer, S., Watkins, O., Mendes, E., et al.:
Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game.
arXiv:2311.01011 (2023)

\bibitem{schulhoff2024hackaprompt}
Schulhoff, S., Pinto, J., Khan, A., et al.:
Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition.
arXiv:2311.16119 (2024)

\bibitem{hines2024spotlighting}
Hines, K., Lopez, G., Hall, M., Zarfati, F., Zunger, Y., Kiciman, E.:
Defending Against Indirect Prompt Injection Attacks With Spotlighting.
In: CAMLIS'24: Conference on Applied Machine Learning for Information Security.
CEUR Workshop Proceedings (2024)

\bibitem{mchugh2025promptinjection20}
McHugh, J., \v{S}ekrst, K., Cefalu, J.:
Prompt Injection 2.0: Hybrid AI Threats.
arXiv:2507.13169 (2025)

\bibitem{naik2025threatlandscape}
Naik, I., Naik, D., Naik, N.:
Threat Landscape of Adversarial Attacks on Generative AI and Large Language Models (LLMs): Exploring Different Types of Adversarial Attacks, Associated Risks, and Mitigation Strategies (2025)

\bibitem{kalliomaki2025agents}
Kalliom\"aki, A.:
Large Language Model (LLM) Agents: Applications and Security.
Bachelor's Thesis, Aalto University (2025)

\bibitem{gulyamov2026review}
Gulyamov, S., Gulyamov, S., Rodionov, A., Khursanov, R., Mekhmonov, K., Babaev, D., Rakhimjonov, A.:
Prompt Injection Attacks in Large Language Models and AI Agent Systems: A Comprehensive Review of Vulnerabilities, Attack Vectors, and Defense Mechanisms.
Information \textbf{17}(54) (2026).
\url{https://doi.org/10.3390/info17010054}

\bibitem{euaiact2024}
{European Parliament and Council of the European Union}:
Regulation (EU) 2024/1689 laying down harmonised rules on artificial intelligence (Artificial Intelligence Act).
Official Journal of the European Union, L 2024/1689 (2024).
\url{https://eur-lex.europa.eu/eli/reg/2024/1689/oj}

\end{thebibliography}

\end{document}
